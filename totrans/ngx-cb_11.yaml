- en: Performance Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Gzipping content in NGINX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancing NGINX with keep alive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning worker processes and connections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine tuning basic Linux system limits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating `ngx_pagespeed`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have your NGINX configuration working, you can turn your focus to fine
    tuning to enhance performance. A few sections of this chapter will focus on delivering
    increased performance for users, while others will focus on delivering performance
    enhancements at a server level to allow greater concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: As with any tuning, you need to ensure that you understand the limits first.
  prefs: []
  type: TYPE_NORMAL
- en: Premature optimization is the root of all evil.
  prefs: []
  type: TYPE_NORMAL
- en: â€“ Donald Knuth, 1974
  prefs: []
  type: TYPE_NORMAL
- en: In the context of NGINX, you need to ensure that you know what the limits are
    before changing them. Not all changes will necessarily result in performance increases
    if they don't suit your system or if they're not a current limitation.
  prefs: []
  type: TYPE_NORMAL
- en: On the flip side, optimization of your NGINX server is also critical to ensure
    that your website or application can handle increased traffic and to ensure fast
    responses. Especially when it comes to e-commerce platforms, keeping user engagement
    through low response times is paramount. Studies conducted by Amazon found that
    an increase in the page load time by one second would result in a loss of over
    $1.6 billion in revenue each year. Even without e-commerce, the last thing that
    you would want a user to be doing is waiting unnecessarily, as they will disengage
    very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Gzipping content in NGINX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gzip is a compression format, which is based on the DEFLATE algorithm and commonly
    found in most Unix environments. Compressing your HTML files is an easy way to
    reduce the amount of data transferred from NGINX to the browser. This in turn
    means that pages also load quicker as the file can be transferred in a shorter
    time due to the reduced size.
  prefs: []
  type: TYPE_NORMAL
- en: While it usually shows the most gain, HTML-based content isn't the only thing,
    which can compress easily. Any text-based file (for example, JavaScript or CSS)
    will generally compress by 70 percent or more, which can be quite significant
    with modern websites.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, enabling compression isn't free. There is a performance hit in server
    load, as the server needs to use CPU cycles to compress the data. While this used
    to be a large consideration, with modern CPU's, the performance hit is far outweighed
    by the benefit of the compression.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NGINX `gzip` module is part of the core modules, so no additional installation
    is required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to enable `gzip` within NGINX, we need to enable the `gzip` module
    and explicitly tell it what files to compress. The easiest way to do this server-wide
    is to create a `gzip.conf` file within the `/etc/nginx/conf.d` directory directly,
    alongside your server directive files. This could also be set per site or even
    per location if required; the `gzip` directives can be nested within an existing
    block directive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s what is required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to measure how much difference *gzipping* your files may make,
    tools such as GTmetrix can outline the reduction in file transmission size. For
    example, if we look at the [https://www.packtpub.com/](https://www.packtpub.com/)
    website, we see the following in the Gzip section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5189f612-1300-42f4-b646-a30c1c192f6f.png)'
  prefs: []
  type: TYPE_IMG
- en: While the savings in this example aren't massive, the 82 percent reduction can
    show you what's possible for other sites. If there were other files, such as JS
    or CSS, which weren't already compressed, the decrease becomes much more significant.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first part of our configuration explicitly turns the `gzip` module `on`.
    Then, to maintain compatibility with really old versions of Internet Explorer
    (which hopefully nobody still uses), we disable `gzip` using the `MSIE [1-6]\.(?!.*SV1)`
    regex.
  prefs: []
  type: TYPE_NORMAL
- en: Then, `gzip_proxied` sets which proxied connections will use `gzip`, which we
    set to `any` to cover all requests. `gzip_types` is then used to set what file
    types are to be compressed. This is matched with the MIME type, for example, `text/plain`.
    We explicitly set types, as not every file type can be compressed further (for
    example, JPEG images).
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we set `gzip_vary` to `on`. This sets the `Vary: Accept-Encoding` header,
    which specifies that both **Content Distribution Networks** (**CDN**) and upstream
    proxies store a copy of the file as both compressed and uncompressed. While every
    modern browser supports Gzip compression, there are still some minor browsers
    and script-based HTTP tools which don''t. Instructing the upstream CDN or proxy
    to store both shows that they''re still able to support these older systems.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the performance hit from gzipping the files on the fly is too much, NGINX
    also allows the ability to precompress the files to serve. While this means that
    there's a bit of extra maintenance work required, this can be incorporated into
    an existing build process (such as **Grunt** or **Gulp**) to reduce the steps
    required.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable in NGINX, we modify our `gzip.conf` file to look like the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: With `gzip_static` set to `on`, NGINX will serve the precompressed files if
    they exist.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Gzip NGINX module can be found at [http://nginx.org/en/docs/http/ngx_http_gzip_module.html](http://nginx.org/en/docs/http/ngx_http_gzip_module.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to GTmetrix's official website at [https://gtmetrix.com/](https://gtmetrix.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancing NGINX with keep alive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using a persistent HTTP connection between the server and the browser speeds
    up additional requests, as there''s no extra handshaking required. Especially
    over more latent connections, this can increase the overall performance. If NGINX
    is being used as a reverse proxy (as detailed in [Chapter 7](bc04362e-995f-4550-92b7-183754306d34.xhtml),
    *Reverse Proxy*), it''s also important to ensure that these connections have `keepalive`
    enabled to ensure high throughput while minimizing latency. The following diagram
    highlights both areas where the `keepalive` packets are important to maintain
    high performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d68c0f3a-c1d6-442f-8d4a-38ff97850155.png)'
  prefs: []
  type: TYPE_IMG
- en: This persistent connection remains established using **Keep Alive** packets,
    so that the connections remain open for minutes rather than closing once they
    are complete. This reuse can be immediate for additional CSS/JS files or as further
    pages and resources are requested.
  prefs: []
  type: TYPE_NORMAL
- en: While some of the client-side gains are negated using HTTP/2 (which multiplexes
    connections as well as uses `keepalive`), it's still necessary for HTTP (non-SSL)
    connections and upstream connections.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NGINX `keepalive` module is part of the core modules, so no additional installation
    is required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By default, NGINX enables keep alive connections for non-proxied connections.
    This means that the connection between NGINX and the browser has already been
    optimized. However, as `keepalive` packets require HTTP/1.1 support, it''s not
    enabled by default for reverse proxy connections. Using our Express example from
    [Chapter 3](db163fa8-2a5d-40bc-b83e-61e72ec67237.xhtml), *Common Frameworks*,
    we can add the additional directives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By adding the `keepalive` directive, we define the maximum number of idle connections
    to keep open using keep alives. In our recipe, we specified a maximum of eight
    idle connections. It's important to note that this isn't the maximum number of
    connections in total, this only defines the number of idle connections to keep
    open.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can confirm that `keepalive` is working by checking the connections from
    the NGINX server. To do this, we use the `ss` command with the `-o` flag to display
    timer information in relation to the socket. For example, we can run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'With our Express-based demo, you should see something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can see that sockets which have a `keepalive` packet have been flagged with
    a timer output to show the expiry.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need to test if the browser is seeing the `keepalive` response from
    the server, you can do this with browser developer tools such as Chrome **Developer
    Tools** (**DevTools**). In your browser of preference, open the developer tools
    and look for the response headers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/801e506e-3bb0-4b83-bf46-7ec05000d7a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding screenshot, we can see that the server responded with `Connection:
    keep-alive`. This means that the keepalive packets are supported and working.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NGINX `keepalive` documentation can be found at [http://nginx.org/en/docs/http/ngx_http_upstream_module.html#keepalive](http://nginx.org/en/docs/http/ngx_http_upstream_module.html#keepalive)
  prefs: []
  type: TYPE_NORMAL
- en: Tuning worker processes and connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the first limits you'll find with running NGINX at scale is the defaults
    for the worker processes and connections. At a low level, an NGINX worker process
    is the dedicated event handler for processing all requests.
  prefs: []
  type: TYPE_NORMAL
- en: The defaults for most NGINX installations are 512 worker connections and 1 worker
    process. While these defaults work in most scenarios, a very busy server can benefit
    from adjusting these levels to suit your environment. There is no one-size-fits-all
    scenario when it comes to the correct values, so it's important to know where
    you're hitting limits and therefore, how to adjust to overcome them.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the limits too high can result in increased memory and CPU overhead,
    which would have the overall effect of reduced performance rather than increasing
    it. Thankfully, NGINX will log when it hits certain limits, which is why the logging
    (as covered in [Chapter 5](3aa7298c-9fc0-4f41-9dfa-6db2e4e5e345.xhtml), *Logging*)
    and metrics of your systems are paramount to maintaining high performance.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No special requirements exist for modifying worker process or connection directives.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the worker processes and worker connections can be independently adjusted,
    we can adjust either or both of them; depending on the limits which you're hitting.
  prefs: []
  type: TYPE_NORMAL
- en: Worker processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To adjust the number of worker processes, we need to edit the main NGINX configuration
    file. For most installations, this will be located at `/etc/nginx/nginx.conf`,
    and the directive is generally the first line in the file. Here''s what you may
    see for a default value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If the server is dedicated to just running NGINX (for example, it doesn''t
    have the database and other services also running on it), a good rule of thumb
    is to set it to the number of CPU''s available. Consider this example when you
    have four CPU''s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you have a high number of connections and they're not CPU bound (for example,
    heavy disk I/O), having more processes than CPU's may assist with increasing the
    overall throughput of the server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, NGINX can attempt to autodetect the number of CPU''s in your system
    by setting the value to auto:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If you're not sure what value to use, auto is the best option to use.
  prefs: []
  type: TYPE_NORMAL
- en: Worker connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Each process can handle up to a maximum number of connections, as specified
    by the `worker_connections` directive. This defaults to 512, but for systems with
    high numbers of connections, we can increase this further. To do this, we need
    to edit the main nginx configuration file (`/etc/nginx/nginx.conf`) and adjust
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As increasing the maximum number of connections means that additional system
    resources are required, caution should be exercised when making a change. If the
    server hits the limit for `worker_connections`, this will be logged in the NGINX
    error log. By ensuring that this is monitored alongside server resources, you
    can ensure that it has been set to the correct limit.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The worker connections can be further enhanced on modern systems by a few extra
    directives. Here''s what our updated block directive looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We have added the additional `multi_accept` directive and set it to `on`. This
    tells the NGINX worker to accept more than one connection at once when there are
    a high number of new, incoming connections.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we set the `use` directive to `epoll`. This is the method NGINX uses to
    process the connections. While on every modern system this should be automatically
    set to `epoll` by default, we can explicitly set this to ensure that it's used.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For more information on worker processes, refer to [http://nginx.org/en/docs/ngx_core_module.html#worker_processes](http://nginx.org/en/docs/ngx_core_module.html#worker_processes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on worker connections, refer to [http://nginx.org/en/docs/ngx_core_module.html#worker_connections](http://nginx.org/en/docs/ngx_core_module.html#worker_connections)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine tuning basic Linux system limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most popular hosting **Operating System** (**OS**) for NGINX is Linux, which
    is why we have focused on it in this book. Like NGINX, the out-of-the-box parameters
    are a good balance between resource usage and performance.
  prefs: []
  type: TYPE_NORMAL
- en: One of the neat features of Linux is the fact that most of the kernel (the *engine*
    of the OS) can be tweaked and tuned as required. In fact, nearly every underlying
    aspect can be adjusted quite easily to ensure that you can perfectly tune it to
    your needs. With over 1,000 configurable parameters in the kernel, there's virtually
    infinite tuning available.
  prefs: []
  type: TYPE_NORMAL
- en: Like our warning at the beginning of this chapter however, larger numbers don't
    necessarily reflect greater performance. It's important to ensure that you understand
    what parameters you're changing to understand the impact they'll have.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Make sure that you have a backup of your existing server before making any changes.
    As the changing of the kernel parameters could result in adverse performance or
    even an unworkable system, it's advisable to perform this on a development or
    staging system first.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Each kernel parameter can be set in real time, which allows you to test and
    refine the changes on the fly. To do this, you can use the `sysctl` program to
    change these parameters. For starters, we can ensure that TCP syncookies (a method
    of resisting low level denial of service attacks) are enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want the change to be persistent between boots, we can add this to the
    `sysctl` configuration file, generally located at `/etc/sysctl.conf`. Here''s
    what the configuration line should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You can test and retrieve what value any of the kernel parameters are set to
    using `sysctl` again in read-only mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If you'' set it correctly, you should see the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: When you're trying to determine if limits have been hit, ensure that you check
    the kernel ring buffer for errors by running the `dmesg` utility. This logs the
    output from any kernel module, which generally occurs when they either encounter
    a limit or error and usually the first port of call to determine what limits you've
    hit.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have a busy server, one of the first Linux kernel limits you may find
    yourself hitting is when there are some delays in processing and you still have
    a large number of incoming connections which haven''t yet been accepted. While
    there is a buffer, once you hit this buffer, the server will simply drop any further
    incoming connections which will cause disruption. To increase this limit, we can
    adjust the limit by increasing the value for `net.core.somaxconn`. On many systems,
    this defaults to 128 connections, but we can increase this by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'With a large amount of incoming connections, you may also find yourself running
    out of ephemeral ports for newer connections. As one of the final stages of a
    TCP connection is the `TIME_WAIT` stage, here the connection has been requested
    to be closed but i''s held open just in case there are any further packets. On
    a busy server, this can result in thousands of connections being held in a `TIME_WAIT`
    state and, by default, these need to be completely closed before they can be reused.
    We can see the state of the TCP ports on a server by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the output from a moderately-low used server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'If the server becomes very busy, it''s possible that all the ports available
    will be locked in the `TIME_WAIT` state. There are two approaches to overcoming
    this limitation. The first is to reduce the time we hold a connection in the `TIME_WAIT`
    stage. This can be done by lowering the default of `60` seconds to `10` seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Secondly, we could simply tell the Linux kernel to reuse ports still in the
    `TIME_WAIT` stage for new connections, if required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This generally isn't enabled by default due to possible conflicts and issues
    with old, legacy applications, but should be safe to enable for an NGINX server.
  prefs: []
  type: TYPE_NORMAL
- en: You may also find many blogs and articles advising you to increase the buffer
    sizes for TCP, but these generally focus on increasing the buffer sizes for file
    serving. Unless you're using NGINX to serve large files, the default values are
    generally high enough for low latency connections.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NGINX tuning blog can be found at [https://www.nginx.com/blog/tuning-nginx/](https://www.nginx.com/blog/tuning-nginx/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on kernel tuning, refer to [https://www.linux.com/news/kernel-tuning-sysctl](https://www.linux.com/news/kernel-tuning-sysctl)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating ngx_pagespeed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the kings of high performance, Google has given us many tools and enhancements
    that have benefited the web world enormously. Google Chrome (as of 2017) has over
    60 percent of the browser market share and its drive for performance has forced
    other browsers to play catch-up.
  prefs: []
  type: TYPE_NORMAL
- en: Not to be outdone at the server level, Google also has vested interest in ensuring
    that websites are highly performant as well. This is because faster sites offer
    a better user experience, which is important when you're trying to offer highly
    relevant search results. To expedite this, Google released `ngx_pagespeed`, which
    is a module for NGINX that tries to apply Google's best practices to reduce both
    latency and bandwidth for websites.
  prefs: []
  type: TYPE_NORMAL
- en: While many (if not all) of these optimizations can be manually applied, or should
    be part of any highly-performant development workflow, not everyone has the time
    to put the same amount of focus on overall website performance. This is especially
    common with smaller business websites, where the development has been outsourced
    to a third party, but don't have the budget to fully optimize.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `ngx_pagespeed` module requires you to compile the module and NGINX from
    source, which can be achieved in two ways. The first is to use the automated script
    provided by Google:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Manually inspecting any script before running it is advisable, especially when
    downloading from new or unknown sources.
  prefs: []
  type: TYPE_NORMAL
- en: This script will install any required dependencies, then download the latest
    mainline NGINX edition, and then add the `ngx_pagespeed` module. This script isn't
    completely headless and may ask you to confirm some basic parameters such as additional
    modules to compile (if required). If you intend to use this on a production server,
    you will need to adjust some of the installation paths to suit your environment.
  prefs: []
  type: TYPE_NORMAL
- en: The second method is via a manual installation, which can also be used if you
    need to modify the standard build. Details of the manual installation steps are
    located on the `ngx_pagespeed` website. With the advent of dynamic modules within
    NGINX, there should hopefully be a compiled binary version available very soon
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To test the differences with and without the module enabled, I''ve used a typical
    Bootstrap-based website which incorporates several JavaScript and **Cascading
    Style Sheets** (**CSS**) scripts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To serve these files, we have a basic NGINX configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'With a basic site set up, we can now enable the `nginx_pagespeed` module. Before
    we enable the module, we first need to create a directory for cache file storage.
    We can do this using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to ensure that it''s writable by NGINX, so we simply set the ownership
    to be the `nginx` user, using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'With the cache directory ready, we can now load the module by adding the following
    lines into the server block directive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'While it may seem overly simple, there''s an enormous amount of complexity
    and work which goes on in the module. Since some of the options may cause issues
    with a small number of sites, there''s also the ability to disable certain sub-modules.
    For instance, if we wanted to disable combining CSS files, we can disable the
    filter by adding the following directive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using Google''s **PageSpeed Insights**, we can see the out-of-the-box score
    for the website without any optimization enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/584442ec-2238-448f-8e4f-85756a1df79c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Obviously, 51/100 isn''t a great score; this is due to multiple CSS and JS
    files that haven''t been minified, are not compressed, and use no explicit browser
    caching. With `ngx_pagespeed` enabled, we get a much better result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01f109cb-7dd0-4578-bbc6-eaf724ea22d7.png)'
  prefs: []
  type: TYPE_IMG
- en: This has instantly given the website a boost in performance, but the simple
    score doesn't tell the whole story. When comparing the differences in the number
    of requests, the total has nearly halved.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Without ngx_pagespeed** | **With ngx_pagespeed** |'
  prefs: []
  type: TYPE_TB
- en: '| **Files Loaded** | 18 | 11 |'
  prefs: []
  type: TYPE_TB
- en: '| **Total Transferred** | 540 kB | 214 kB |'
  prefs: []
  type: TYPE_TB
- en: While this is only for a very basic site, as the improvements show, there are
    significant performance gains for nearly zero effort.
  prefs: []
  type: TYPE_NORMAL
- en: Like many of the systems that work, Google's optimizations have quite a number
    of neat features. As compressing and minifying CSS/JS can be CPU intensive, on
    the first page load (without the cache being warmed), NGINX will simply serve
    the site in the original format. In the background, the module queues these tasks
    and, once available, they will be served directly from the cache.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to see what''s going on, we can enable the admin area for `modpagespeed`.
    To do this, we need to add configuration items outside of the main server directive.
    Here''s the code to add:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This allows us to see what''s going on within the module, including details
    such as hits and misses from the cache, image compression, CSS, and JavaScript
    minification, and more:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19685a0a-8b8a-4240-9d73-61f111a2b769.png)'
  prefs: []
  type: TYPE_IMG
- en: If this is a production environment, ensure that you restrict access so that
    it can't be used for malicious reasons.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more about NGINX PageSpeed, refer to [http://ngxpagespeed.com/](http://ngxpagespeed.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More information about installation reference is available at [https://modpagespeed.com/doc/build_ngx_pagespeed_from_source](https://modpagespeed.com/doc/build_ngx_pagespeed_from_source)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "More information about PageSpeed Insights can be found at [https://developers.google.com/speed/pagespeed/insights/\uFEFF\
    ](https://developers.google.com/speed/pagespeed/insights/)"
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
