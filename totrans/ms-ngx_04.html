<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;NGINX as a Reverse Proxy"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. NGINX as a Reverse Proxy</h1></div></div></div><p>A <span class="strong"><strong>reverse proxy</strong></span>
<a id="id425" class="indexterm"/> is a web server that terminates connections with clients and makes new ones to upstream servers on their behalf. An <span class="strong"><strong>upstream server</strong></span>
<a id="id426" class="indexterm"/> is defined as a server that NGINX makes a connection with in order to fulfill the client's request. These upstream servers can take various forms, and NGINX can be configured differently to handle each of them.</p><p>NGINX <a id="id427" class="indexterm"/>configuration, which you have been learning about in detail, can be difficult to understand at times. There are different directives that may be used to fulfill similar configuration needs. Some of these options should not really be used, as they can lead to unexpected results.</p><p>At times, an upstream server may not be able to fulfill a request. NGINX has the capability to deliver an error message to the client, either directly from this upstream server, from its local disk, or as a redirect to a page on a completely different server.</p><p>Due to the nature of a reverse proxy, the upstream server doesn't obtain information directly from the client. Some of this information, such as the client's real IP address, is important for debugging purposes, as well as tracking requests. This information may be passed to the upstream server in the form of headers.</p><p>We will cover these topics, as well as an overview of some proxy module directives, in the following sections:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Introduction to reverse proxying</li><li class="listitem" style="list-style-type: disc">Types of upstream servers</li><li class="listitem" style="list-style-type: disc">Converting an "if"-fy configuration to a more modern interpretation</li><li class="listitem" style="list-style-type: disc">Using error documents to handle upstream problems</li><li class="listitem" style="list-style-type: disc">Determining the client's real IP address</li></ul></div><div class="section" title="Introduction to reverse proxying"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec30"/>Introduction to reverse proxying</h1></div></div></div><p>NGINX can serve as a reverse proxy by terminating requests from clients and opening new ones to its upstream servers. On the way, the request can be split up according to its URI, client parameters, or some other logic, in order to best respond to the request from the client. Any part of the request's original URL can be transformed on its way through the reverse proxy.</p><p>The most important directive<a id="id428" class="indexterm"/> when proxying to an upstream server is the<a id="id429" class="indexterm"/> <code class="literal">proxy_pass</code> directive. This directive takes one parameter—the URL to which the request should be transferred. Using <code class="literal">proxy_pass</code> with a URI part will replace the <code class="literal">request_uri</code> with this part. For example, <code class="literal">/uri</code> in the following example will be transformed to <code class="literal">/newuri</code> when the request is passed on to the upstream:</p><div class="informalexample"><pre class="programlisting">location /uri {

  proxy_pass http://localhost:8080/newuri;
}</pre></div><p>There are two exceptions to this rule, however. First, if the location is defined with a regular expression, no transformation of the URI occurs. In this example, the URI <code class="literal">/local</code> will be passed directly to the upstream, and not be transformed to <code class="literal">/foreign</code> as intended:</p><div class="informalexample"><pre class="programlisting">location ~ ^/local {

  proxy_pass http://localhost:8080/foreign;
}</pre></div><p>The second exception is that if within the location a rewrite rule changes the URI, and then NGINX uses this URI to process the request, no transformation occurs. In this example, the URI passed to the upstream will be <code class="literal">/index.php?page=&lt;match&gt;</code>, with <code class="literal">&lt;match&gt;</code> being whatever was captured in the parentheses, and not <code class="literal">/index</code>, as indicated by the URI part of the <code class="literal">proxy_pass</code> directive:</p><div class="informalexample"><pre class="programlisting">location / {

  rewrite /(.*)$ /index.php?page=$1 break;

  proxy_pass http://localhost:8080/index;
}</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip08"/>Tip</h3><p>The <code class="literal">break</code> flag <a id="id430" class="indexterm"/>to the rewrite directive is used here to immediately stop all processing of rewrite module directives.</p></div></div><p>In both of these cases, the URI part of the <code class="literal">proxy_pass</code> directive is not relevant, so the configuration would be <a id="id431" class="indexterm"/>complete without it:</p><div class="informalexample"><pre class="programlisting">location ~ ^/local {

  proxy_pass http://localhost:8080;
}

location / {

  rewrite /(.*)$ /index.php?page=$1 break;

  proxy_pass http://localhost:8080;
}</pre></div><div class="section" title="The proxy module"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec29"/>The proxy module</h2></div></div></div><p>The following <a id="id432" class="indexterm"/>table summarizes some<a id="id433" class="indexterm"/> of the commonly used directives in the proxy module:</p><div class="section" title="Table: Proxy module directives"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl6sec17"/>Table: Proxy module directives</h3></div></div></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Directive</p>
</th><th style="text-align: left" valign="bottom">
<p>Explanation</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_connect_timeout</code>
<a id="id434" class="indexterm"/>
<a id="id435" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The maximum amount of time NGINX will wait for its connection to be accepted when making a request to an upstream server.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cookie_domain</code>
<a id="id436" class="indexterm"/>
<a id="id437" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Replaces the domain attribute of the <code class="literal">Set-Cookie</code> header from the upstream server; the domain to be replaced can either be a string or a regular expression, or reference a variable.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cookie_path</code>
<a id="id438" class="indexterm"/>
<a id="id439" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Replaces the <code class="literal">path</code> attribute of the <code class="literal">Set-Cookie</code> header from the upstream server; the path to be replaced can either be a string or a regular expression, or reference a variable.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_headers_hash_bucket_size</code>
<a id="id440" class="indexterm"/>
<a id="id441" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The maximum size of header names.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_headers_hash_max_size</code>
<a id="id442" class="indexterm"/>
<a id="id443" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The total size of headers received from the upstream server.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_hide_header</code>
<a id="id444" class="indexterm"/>
<a id="id445" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>A list of header fields that should not be passed on to the client.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_http_version</code>
<a id="id446" class="indexterm"/>
<a id="id447" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The HTTP protocol version used to communicate with upstream servers (use <code class="literal">1.1</code> for <code class="literal">keepalive</code> connections).</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_ignore_client_abort</code>
<a id="id448" class="indexterm"/>
<a id="id449" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>If set to <code class="literal">on</code>, NGINX will not abort the connection to an upstream server if the client aborts the connection.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_ignore_headers</code>
<a id="id450" class="indexterm"/>
<a id="id451" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Sets which headers can be disregarded when processing the response from the upstream server.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_intercept_errors</code>
<a id="id452" class="indexterm"/>
<a id="id453" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>If enabled, NGINX will display a configured <code class="literal">error_page</code> error instead of the response directly from the upstream server.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_max_temp_file_size</code>
<a id="id454" class="indexterm"/>
<a id="id455" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The<a id="id456" class="indexterm"/> maximum size of the overflow file, written when the response doesn't fit into memory buffers.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_pass</code>
<a id="id457" class="indexterm"/>
<a id="id458" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Specifies the upstream server to which the request is passed, in the form of a URL.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_pass_header</code>
<a id="id459" class="indexterm"/>
<a id="id460" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Overrides the disabled headers set in <code class="literal">proxy_hide_header</code>, allowing them to be sent to the client.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_pass_request_body</code>
<a id="id461" class="indexterm"/>
<a id="id462" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Prevents<a id="id463" class="indexterm"/> sending the body of the request to the upstream server if set to <code class="literal">off</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_pass_request_headers</code>
<a id="id464" class="indexterm"/>
<a id="id465" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Prevents sending the headers of the request to the upstream server if set to <code class="literal">off</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_read_timeout</code>
<a id="id466" class="indexterm"/>
<a id="id467" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Specifies the length of time that needs to elapse between two successive read operations from an upstream server, before the connection is closed. Should be set to a higher value if the upstream server processes requests slowly.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_redirect</code>
<a id="id468" class="indexterm"/>
<a id="id469" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Rewrites the <code class="literal">Location</code> and <code class="literal">Refresh</code> headers received from the upstream servers; useful for working around assumptions made by an application framework.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_send_timeout</code>
<a id="id470" class="indexterm"/>
<a id="id471" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The length of time that needs to elapse between two successive write operations to an upstream server, before the connection is closed.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_set_body</code>
<a id="id472" class="indexterm"/>
<a id="id473" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The body of a request sent to an upstream server may be altered by setting this directive.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_set_header</code>
<a id="id474" class="indexterm"/>
<a id="id475" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Rewrites the contents of headers sent to an upstream server; may also be used to not send certain headers by setting its value to the empty string.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_temp_file_write_size</code>
<a id="id476" class="indexterm"/>
<a id="id477" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Limits the amount of data buffered to a temporary file at one time, so that NGINX will not block too long on a single request.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_temp_path</code>
<a id="id478" class="indexterm"/>
<a id="id479" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>A<a id="id480" class="indexterm"/> directory where temporary files may be buffered as they are proxied from the upstream server, optionally multi-level deep.</p>
</td></tr></tbody></table></div><p>The following listing brings many of these <a id="id481" class="indexterm"/>directives together in a file that can be included in the configuration within the same location as the <code class="literal">proxy_pass</code> directive.</p><p>Contents of <code class="literal">proxy.conf</code>:</p><div class="informalexample"><pre class="programlisting">proxy_redirect  off;

proxy_set_header Host      $host;

proxy_set_header X-Real-IP    $remote_addr;

proxy_set_header X-Forwarded-For  $proxy_add_x_forwarded_for;

client_max_body_size   10m;

client_body_buffer_size  128k;
proxy_connect_timeout   30;

proxy_send_timeout    	15;

proxy_read_timeout    	15;

proxy_send_lowat     12000;

proxy_buffer_size     	4k;

proxy_buffers       4 32k;

proxy_busy_buffers_size  	64k;

proxy_temp_file_write_size    64k;</pre></div><p>We are setting a number<a id="id482" class="indexterm"/> of common directives to values that we<a id="id483" class="indexterm"/> think would be useful for reverse-proxying scenarios:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">proxy_redirect</code> directive<a id="id484" class="indexterm"/> has been set to <code class="literal">off</code> because there is no need to rewrite the <code class="literal">Location</code> header in most situations.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">Host</code> header<a id="id485" class="indexterm"/> is set so the upstream server can map the request to a virtual server or otherwise make use of the host portion of the URL the user entered.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">X-Real-IP</code> and <code class="literal">X-Forwarded-For</code> headers serve similar purposes—to relay the information about the connecting client's IP address to the upstream server.<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">$remote_addr</code> variable used in the <code class="literal">X-Real-IP</code> header is the IP address of the client as NGINX perceives it.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">$proxy_add_x_forwarded_for</code> variable contains the contents of the <code class="literal">X-Forwarded-For</code> header field from the client's request, followed by the <code class="literal">$remote_addr</code> variable.</li></ul></div></li><li class="listitem" style="list-style-type: disc">The<a id="id486" class="indexterm"/> <code class="literal">client_max_body_size</code> directive, while not strictly a proxy module directive, is mentioned here because of its relevance to proxy configurations. If this value is set too low, uploaded files will not make it to the upstream server. When setting this directive, keep in mind that files uploaded via a web form will usually have a larger file size than that shown in the filesystem.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">proxy_connect_timeout</code> directive<a id="id487" class="indexterm"/> indicates how long NGINX will wait when establishing initial contact with the upstream server.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">proxy_read_timeout</code> <a id="id488" class="indexterm"/>and <code class="literal">proxy_send_timeout</code> directives <a id="id489" class="indexterm"/>define how long NGINX will wait between successive operations with the upstream server.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">proxy_send_lowat</code> directive<a id="id490" class="indexterm"/> is only effective on FreeBSD systems and specifies the number of bytes the socket send buffer should hold before passing the data on to the protocol.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">proxy_buffer_size</code>, <code class="literal">proxy_buffers</code>, and <code class="literal">proxy_busy_buffers_size</code> directives <a id="id491" class="indexterm"/>will be discussed in <a id="id492" class="indexterm"/>detail in the next chapter. Suffice it <a id="id493" class="indexterm"/>to say that these buffers control how quickly NGINX appears to respond to user requests.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">proxy_temp_file_write_size</code> directive<a id="id494" class="indexterm"/> controls how long a worker process blocks while spooling data: the higher the value, the longer the process blocks.</li></ul></div><p>These <a id="id495" class="indexterm"/>directives are <a id="id496" class="indexterm"/>included in a file as follows, and may be used multiple times in the same configuration:</p><div class="informalexample"><pre class="programlisting">location / {

  include proxy.conf;

  proxy_pass http://localhost:8080;
}</pre></div><p>If one of these directives should have a different value than what's in the include file, then override it in that particular location.</p><div class="informalexample"><pre class="programlisting">location /uploads {

  include proxy.conf;

  client_max_body_size   	500m;

  proxy_connect_timeout  	75;

  proxy_send_timeout    	90;

  proxy_read_timeout    	90;

  proxy_pass http://localhost:8080;
}</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip09"/>Tip</h3><p>The order is important here. If there is more than one occurrence of a directive in a configuration file (or include), NGINX will take the value of the directive defined last.</p></div></div></div><div class="section" title="Legacy servers with cookies"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec15"/>Legacy servers with cookies</h3></div></div></div><p>You may find yourself in a situation where you will need to place multiple legacy applications<a id="id497" class="indexterm"/> behind one common endpoint. The legacy<a id="id498" class="indexterm"/> applications were written for a case where they were the only servers talking directly with the client. They set cookies from their own domain, and assumed that they would always be reachable via the <code class="literal">/</code> URI. In placing a new endpoint in front of these servers, these assumptions no longer hold true. The following configuration will rewrite the cookie domain and path to match that of the new application endpoint:</p><div class="informalexample"><pre class="programlisting">server {

  server_name app.example.com;

  location /legacy1 {

    proxy_cookie_domain legacy1.example.com app.example.com;

    proxy_cookie_path $uri /legacy1$uri;

    proxy_redirect default;

    proxy_pass http://legacy1.example.com/;
  }
  </pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip10"/>Tip</h3><p>The value of the <code class="literal">$uri</code> variable already includes the beginning slash (<code class="literal">/</code>), so it is not necessary to duplicate it here.</p></div></div><div class="informalexample"><pre class="programlisting"> location /legacy2 {

    proxy_cookie_domain legacy2.example.org app.example.com;

    proxy_cookie_path $uri /legacy2$uri;

    proxy_redirect default;

    proxy_pass http://legacy2.example.org/;

  }

  location / {

    proxy_pass http://localhost:8080;

  }
}</pre></div></div></div><div class="section" title="The upstream module"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec30"/>The upstream module</h2></div></div></div><p>Closely paired with the <code class="literal">proxy</code> module is the <a id="id499" class="indexterm"/>
<code class="literal">upstream</code> module. The <code class="literal">upstream</code> directive starts a new context, in which a group of upstream servers is defined. These servers<a id="id500" class="indexterm"/> may be given different weights (the higher the weight, the greater the number of connections NGINX will pass to that particular upstream server), may be of different types (TCP versus UNIX domain), and may even be marked as <code class="literal">down</code> for maintenance reasons.</p><p>The following table summarizes the <a id="id501" class="indexterm"/>directives valid within the upstream context:</p><div class="section" title="Table: Upstream module directives"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl6sec18"/>Table: Upstream module directives</h3></div></div></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Directive</p>
</th><th style="text-align: left" valign="bottom">
<p>Explanation</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">ip_hash</code>
<a id="id502" class="indexterm"/>
<a id="id503" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Ensures the distribution of connecting clients evenly over all servers by hashing the IP address, keying on its class-C network.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">keepalive</code>
<a id="id504" class="indexterm"/>
<a id="id505" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The number of connections to upstream servers that are cached per worker process. When used with HTTP connections, <code class="literal">proxy_http_version</code> should be set to <code class="literal">1.1</code> and <code class="literal">proxy_set_header</code> to <code class="literal">Connection "".</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">least_conn</code>
<a id="id506" class="indexterm"/>
<a id="id507" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Activates the load-balancing algorithm where the server with the least number of active connections is chosen for the next new connection.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">server</code>
<a id="id508" class="indexterm"/>
<a id="id509" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Defines an address (domain name or IP address with an optional TCP port, or path to a UNIX-domain socket) and optional parameters for an upstream server. The parameters are:</p>
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">weight</code>: It sets the preference for one server over another</li><li class="listitem" style="list-style-type: disc"><code class="literal">max_fails</code>: It is the maximum number of unsuccessful communication attempts to a server within <code class="literal">fail_timeout</code> before the server is marked as <code class="literal">down</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">fail_timeout</code>: It is the length of time a server has to respond to a request and the length of time a server will be marked as down</li><li class="listitem" style="list-style-type: disc"><code class="literal">backup</code>: It will only receive requests once the other servers are down</li><li class="listitem" style="list-style-type: disc"><code class="literal">down</code>: It marks a server as not able to process requests</li></ul></div>
</td></tr></tbody></table></div></div><div class="section" title="Keepalive connections"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec17"/>Keepalive connections</h3></div></div></div><p>The <code class="literal">keepalive</code> directive<a id="id510" class="indexterm"/> deserves special mention. NGINX will keep this <a id="id511" class="indexterm"/>number of connections per worker open to<a id="id512" class="indexterm"/> an upstream server. This connection cache is useful in situations where NGINX has to constantly maintain a certain number of open connections to an upstream server. If the upstream server speaks HTTP, NGINX can use the HTTP/1.1 Persistent Connections mechanism for maintaining these open connections.</p><div class="informalexample"><pre class="programlisting">upstream apache {

  server 127.0.0.1:8080;

  keepalive 32;

}

location / {

  proxy_http_version	1.1;

  proxy_set_header		Connection "";

  proxy_pass http://apache;

}</pre></div><p>Here, we've indicated that we'd like to hold open 32 connections to Apache running on port <code class="literal">8080</code> of the localhost. NGINX need only negotiate the TCP handshake for the initial 32 connections per worker, and will then keep these connections open by not sending a Connection header with the <code class="literal">close</code> token. With <code class="literal">proxy_http_version</code>, we specify that we'd like to speak HTTP/1.1 with the upstream server. We also clear the contents of the <code class="literal">Connection</code> header with <code class="literal">proxy_set_header</code>, so that we are not proxying the client connection properties directly.</p><p>If more than 32 <a id="id513" class="indexterm"/>connections are needed, NGINX will, of course, open<a id="id514" class="indexterm"/> them to satisfy requests. After this peak has passed, NGINX will close the least recently used connections, to bring the number back down to 32, as we indicated in the <code class="literal">keepalive</code> directive.</p><p>This mechanism can also be used to proxy non-HTTP connections, as well. In the following example, we show that NGINX maintains 64 connections to two instances of <code class="literal">memcached</code>:</p><div class="informalexample"><pre class="programlisting">upstream memcaches {

  server 10.0.100.10:11211;

  server 10.0.100.20:11211;

  keepalive 64;

}</pre></div><p>If we were to switch load-balancing algorithms from the default round-robin to either <code class="literal">ip_hash</code> or <code class="literal">least_conn</code>, we would need to specify this before using the <code class="literal">keepalive</code> directive:</p><div class="informalexample"><pre class="programlisting">upstream apaches {

  least_conn;

  server 10.0.200.10:80;

  server 10.0.200.20:80;

  keepalive 32;

}</pre></div></div><div class="section" title="Load-balancing algorithms"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec18"/>Load-balancing algorithms</h3></div></div></div><p>The <code class="literal">upstream</code> module<a id="id515" class="indexterm"/> can select which upstream server to connect to in the <a id="id516" class="indexterm"/>next<a id="id517" class="indexterm"/> step by using one of three load-balancing algorithms—round-robin, IP hash, or least connections. The <span class="strong"><strong>round-robin </strong></span>algorithm<a id="id518" class="indexterm"/> is selected by default, and doesn't need a configuration directive to activate it. This algorithm selects the next server, based on which server was selected previously, which server is next in the configuration block, and what weight each server carries. The round-robin algorithm tries to ensure a fair distribution of traffic, based on a concept of who's turn it is next.</p><p>The <a id="id519" class="indexterm"/>
<span class="strong"><strong>IP hash</strong></span> algorithm, activated by the<a id="id520" class="indexterm"/> <code class="literal">ip_hash</code> directive, instead takes the view that certain IP addresses should always be mapped to the same upstream server. NGINX does this by using the first three octets of an IPv4 address or the entire IPv6 address, as a hashing key. The same pool of IP addresses are therefore always mapped to the same upstream server. So, this mechanism isn't designed to ensure a fair distribution, but rather a consistent mapping between the client and upstream server.</p><p>The third load-balancing algorithm supported by the default upstream module<span class="strong"><strong>, least connections</strong></span>
<a id="id521" class="indexterm"/>, is activated by the<a id="id522" class="indexterm"/> <code class="literal">least_conn</code> directive. This algorithm is designed to distribute the load evenly among upstream servers, by selecting the one with the fewest number of active connections. If the upstream servers do not all have the same processing power, this can be indicated using the <code class="literal">weight</code> parameter to the <code class="literal">server</code> directive. The algorithm will take into account the differently-weighted servers when calculating the number of least connections.</p></div></div></div></div>
<div class="section" title="Types of upstream servers"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec31"/>Types of upstream servers</h1></div></div></div><p>An upstream server <a id="id523" class="indexterm"/>is a server to which NGINX proxies a connection. This can be on a different physical or virtual machine, but doesn't have to be. The upstream server may be a daemon listening on a UNIX domain socket for connections on the local machine or could be one of <a id="id524" class="indexterm"/>many on a different machine listening over TCP. It may be an Apache server, with multiple modules to handle different kinds of requests, or a Rack middleware server, providing an HTTP interface to Ruby applications. NGINX can be configured to proxy to each of them.</p><div class="section" title="Single upstream server"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec31"/>Single upstream server</h2></div></div></div><p>The Apache <a id="id525" class="indexterm"/>web server is used in common hosting scenarios to serve static files as well as<a id="id526" class="indexterm"/> multiple types of interpreted files. The extensive documentation and how-to's (found online) help users to get up-and-running quickly with their favorite CMS. Unfortunately, the typical Apache configuration, due to resource limits, is not able to handle many simultaneous requests. NGINX, though, is designed to handle this kind of traffic and performs very well with little resource consumption. Since most CMSs come pre-configured for Apache, integrating the use of <code class="literal">.htaccess</code> files for extended configuration, the easiest way to take advantage of NGINX's strengths is for NGINX to simply proxy connections to an Apache instance:</p><div class="informalexample"><pre class="programlisting">server {
 
 location / {

    proxy_pass http://localhost:8080;

  }

}</pre></div><p>This is the <a id="id527" class="indexterm"/>most basic proxy configuration possible. NGINX will terminate<a id="id528" class="indexterm"/> all client connections, and then proxy all requests to the local host on TCP port 8080. We assume here that Apache has been configured to listen on <code class="literal">localhost:8080</code>.</p><p>A configuration such as this is typically extended so that NGINX will serve any static files directly, and then proxy the remaining requests to Apache:</p><div class="informalexample"><pre class="programlisting">server {

  location / {

    try_files $uri @apache;

  }

  location @apache {

    proxy_pass http://127.0.0.1:8080;

  }

}</pre></div><p>The <code class="literal">try_files</code> directive (included in the <code class="literal">http</code> core module) <a id="id529" class="indexterm"/>does just what its name implies—it tries files, in order, until it finds a match. So, in the preceding example, NGINX will deliver any files it finds in its root that match the URI given by the client. If it doesn't find any files, it will proxy the request to Apache for further processing. We use a named location here to proxy the request after an unsuccessful try to locate the file locally.</p></div><div class="section" title="Multiple upstream servers"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec32"/>Multiple upstream servers</h2></div></div></div><p>It is also <a id="id530" class="indexterm"/>possible to configure NGINX to pass the request to more <a id="id531" class="indexterm"/>than one upstream server. This is done by declaring an upstream context, defining multiple servers, and referencing the upstream in a <code class="literal">proxy_pass</code> directive:</p><div class="informalexample"><pre class="programlisting">upstream app {

  server 127.0.0.1:9000;

  server 127.0.0.1:9001;

  server 127.0.0.1:9002;

}
server {

  location / {

    proxy_pass http://app;

  }

}</pre></div><p>Using this configuration, NGINX will pass consecutive requests in a round-robin fashion to the three upstream servers. This is useful when an application can handle only one request at a time, and you'd like NGINX to handle the client communication so that none of the application servers get overloaded. The configuration is illustrated in the following diagram:</p><div class="mediaobject"><img src="graphics/7447OS_04_01.jpg" alt="Multiple upstream servers"/></div><p>Other load-balancing <a id="id532" class="indexterm"/>algorithms are available, as detailed in the <span class="emphasis"><em>Load-balancing algorithms</em></span> section earlier in this chapter. Which one should be used in a particular<a id="id533" class="indexterm"/> configuration depends on the situation. </p><p>If a client should always get the same upstream server, to effect a poor-man's session-stickiness, the <code class="literal">ip_hash</code> directive should be used. When the distribution of requests leads to widely varying response times per request, the <code class="literal">least_conn</code> algorithm should be selected. The default round-robin algorithm is good for a general case where no special consideration of either the client or upstream server is required.</p></div><div class="section" title="Non-HTTP upstream servers"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec33"/>Non-HTTP upstream servers</h2></div></div></div><p>So far, we've focused<a id="id534" class="indexterm"/> on communicating with upstream servers over <a id="id535" class="indexterm"/>HTTP. For this, we use the <code class="literal">proxy_pass</code> directive. As hinted at earlier in this chapter, in the <span class="emphasis"><em>Keepalive connections</em></span> section, NGINX can proxy requests to a number of different kinds of upstream servers. Each has its corresponding *<code class="literal">_pass</code> directive.</p><div class="section" title="Memcached upstream servers"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec19"/>Memcached upstream servers</h3></div></div></div><p>The <code class="literal">memcached</code> NGINX<a id="id536" class="indexterm"/> module (enabled by default) is responsible for <a id="id537" class="indexterm"/>communicating with a <code class="literal">memcached</code> daemon. As such, there is no direct communication between the client and the <code class="literal">memcached</code> daemon; that is, NGINX does not act as a reverse-proxy in this sense. The <code class="literal">memcached</code> module enables NGINX to speak the <code class="literal">memcached</code> protocol, so that a key lookup can be done before a request is passed to an application server:</p><div class="informalexample"><pre class="programlisting">upstream memcaches {

  server 10.0.100.10:11211;

  server 10.0.100.20:11211;

}

server {

  location / {

    set	$memcached_key "$uri?$args";

    memcached_pass	memcaches;

    error_page 404 = @appserver;

  }
  location @appserver {

    proxy_pass http://127.0.0.1:8080;

  }

}</pre></div><p>The <code class="literal">memcached_pass</code> directive<a id="id538" class="indexterm"/> uses the <code class="literal">$memcached_key</code> variable to <a id="id539" class="indexterm"/>make the key lookup. If there is no corresponding <a id="id540" class="indexterm"/>value (<code class="literal">error_page 404</code>), we pass the request on to <code class="literal">localhost</code>, where there is presumably a server running that will handle this request and insert a key/value pair into the <code class="literal">memcached</code> instance.</p></div><div class="section" title="FastCGI upstream servers"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec20"/>FastCGI upstream servers</h3></div></div></div><p>Using a FastCGI<a id="id541" class="indexterm"/> server is a popular way to run PHP applications behind an <a id="id542" class="indexterm"/>NGINX server. The <code class="literal">fastcgi</code> module<a id="id543" class="indexterm"/> is compiled in by default, and is activated with the<a id="id544" class="indexterm"/> <code class="literal">fastcgi_pass</code> directive. This enables NGINX to speak the FastCGI protocol with one or more upstream servers. We define a set of FastCGI upstream servers as follows:</p><div class="informalexample"><pre class="programlisting">upstream fastcgis {

  server 10.0.200.10:9000;

  server 10.0.200.20:9000;

  server 10.0.200.30:9000;
}</pre></div><p>And pass connections to them from the root location:</p><div class="informalexample"><pre class="programlisting">location / {

  fastcgi_pass fastcgis;
}</pre></div><p>This is a very minimalist configuration to illustrate the basics of using FastCGI. The <code class="literal">fastcgi</code> module contains a number of directives and configuration possibilities, which we will discuss in <a class="link" href="ch06.html" title="Chapter 6. The NGINX HTTP Server">Chapter 6</a>, <span class="emphasis"><em>The NGINX HTTP Server</em></span>.</p></div><div class="section" title="SCGI upstream servers"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec21"/>SCGI upstream servers</h3></div></div></div><p>NGINX can<a id="id545" class="indexterm"/> also <a id="id546" class="indexterm"/>speak the SCGI protocol by using its built-in <code class="literal">scgi</code> module. The principle is the same as for the <code class="literal">fastcgi</code> module. NGINX communicates with an upstream server indicated with the <code class="literal">scgi_pass</code> directive.</p></div><div class="section" title="uWSGI upstream servers"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec22"/>uWSGI upstream servers</h3></div></div></div><p>The <code class="literal">uWSGI</code> protocol <a id="id547" class="indexterm"/>has been very popular with Python developers. NGINX <a id="id548" class="indexterm"/>provides support for connecting to a Python-based upstream server through its <code class="literal">uwsgi</code> module. The configuration is similar to the <code class="literal">fastcgi</code> module, using the <code class="literal">uwsgi_pass</code> directive instead to indicate an upstream server. An example configuration will be shown in <a class="link" href="ch06.html" title="Chapter 6. The NGINX HTTP Server">Chapter 6</a>, <span class="emphasis"><em>The NGINX HTTP Server</em></span>.</p></div></div></div>
<div class="section" title="Converting an &quot;if&quot;-fy configuration to a more modern interpretation"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec32"/>Converting an "if"-fy configuration to a more modern interpretation</h1></div></div></div><p>Using the <code class="literal">if</code> directive<a id="id549" class="indexterm"/> within a location is really only considered valid for certain cases. It may be used in combination with a return and with a rewrite with a <code class="literal">last</code> or <code class="literal">break</code> flag, but should generally be avoided in other situations. This is due in part to the fact that it can produce some very unexpected results. Consider the following example:</p><div class="informalexample"><pre class="programlisting">location / {

 try_files /img /static @imageserver;

 if ($request_uri ~ "/blog") {

  proxy_pass http://127.0.0.1:9000;

  break;

 }

 if ($request_uri ~ "/tickets") {

  proxy_pass http://tickets.example.com;

  break;
 }

}

location @imageserver {

  proxy_pass http://127.0.0.1:8080;
}</pre></div><p>Here, we're trying to determine which upstream to pass the request to, based on the value of the <code class="literal">$request_uri</code> variable. This seems like a very reasonable configuration at first glance, because it works for our simple test cases. But the images will neither be served from the <code class="literal">/img</code> filesystem location, the <code class="literal">/static</code> filesystem location, nor from the <code class="literal">@imageserver</code> named location. <code class="literal">try_files</code> simply doesn't work when an <code class="literal">if</code> directive is present in the same location. <code class="literal">if</code> creates an implicit location with its own content handler; in this case, the <code class="literal">proxy</code> module. So the outer content handler, where <code class="literal">try_files</code> is registered, won't ever get invoked. There is a way to write this configuration differently to make it do what we want.</p><p>Let's think about our request as NGINX processes it. After having found a matching IP and port, it first selects a virtual host (server) based on the <code class="literal">Host</code> header. Then, it scans all locations under this server, looking for a matching URI. So, we see that the better way to configure a selector based on the URI is in fact by defining multiple locations, as shown in the following example:</p><div class="informalexample"><pre class="programlisting">location /blog {

  proxy_pass http://127.0.0.1:9000;

}

location /tickets {

  proxy_pass http://tickets.example.com;

}

location /img {

  try_files /static @imageserver;

}

location / {

  root /static;

}

location @imageserver {
  proxy_pass http://127.0.0.1:8080;
}</pre></div><p>This configuration can be illustrated by the following diagram:</p><div class="mediaobject"><img src="graphics/7447OS_04_02.jpg" alt="Converting an &quot;if&quot;-fy configuration to a more modern interpretation"/></div><p>Another example of an <code class="literal">"if"-fy</code> configuration is the following:</p><div class="informalexample"><pre class="programlisting">server {

  server_name marketing.example.com communication.example.com marketing.example.org communication.example.org marketing.example.net communication.example.net;

  if ($host ~* (marketing\.example\.com|marketing\.example\.org|marketing\.example\.net)) {
 
   rewrite ^/$ http://www.example.com/marketing/application.do redirect;

  }

  if ($host ~* (communication\.example\.com|communication\.example\.org|communication\.example\.net)) {

    rewrite ^/$ http://www.example.com/comms/index.cgi redirect;

  }

  if ($host ~* (www\.example\.org|www\.example\.net)) {

    rewrite ^/(.*)$ http://www.example.com/$1 redirect;

  }

}</pre></div><p>Here, we have a number of <code class="literal">if</code> directives matching the Host header (or, if not present, <code class="literal">server_name</code>). After each <code class="literal">if</code>, the URI is rewritten to lead directly to the correct application component. Besides being terribly inefficient due to the processing required to match each regular expression for every URI, it breaks our "no ifs within a location" rule.</p><p>This type of configuration is better rewritten as a series of separate server contexts, in which the URL is rewritten to the application component:</p><div class="informalexample"><pre class="programlisting">server {

  server_name marketing.example.com marketing.example.org marketing.example.net;

  rewrite ^ http://www.example.com/marketing/application.do permanent;

}

server {

  server_name communication.example.com communication.example.org communication.example.net;

  rewrite ^ http://www.example.com/comms/index.cgi permanent;

}

server {

  server_name www.example.org www.example.net;

  rewrite ^ http://www.example.com$request_uri permanent;
}</pre></div><p>In each block, we have placed only those <code class="literal">server_name</code> that are relevant to the respective rewrite, so that no <code class="literal">if</code> is needed. In each <code class="literal">rewrite</code> rule, we have replaced the <code class="literal">redirect</code> flag with the <code class="literal">permanent</code> flag to indicate that this is a full URL that the browser should remember and automatically use the next time the domain is requested. In the last rewrite rule, we have also replaced the match (<code class="literal">^/(.*)$</code>) with a readily-available variable, <code class="literal">$request_uri</code>, which contains the same information but saves the trouble of matching the regular expression and saving the capture variable.</p></div>
<div class="section" title="Using error documents to handle upstream problems"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec33"/>Using error documents to handle upstream problems</h1></div></div></div><p>There are <a id="id550" class="indexterm"/>situations in which the upstream server <a id="id551" class="indexterm"/>cannot respond to a request. In these cases, NGINX can be configured to supply a document from its local disk:</p><div class="informalexample"><pre class="programlisting">server {

  error_page 500 502 503 504 /50x.html;

  location = /50x.html {

    root share/examples/nginx/html;

  }

}</pre></div><p>Or from an external site:</p><div class="informalexample"><pre class="programlisting">server {

  error_page 500 http://www.example.com/maintenance.html;

}</pre></div><p>When proxying to a set of upstream servers, you may want to define an extra upstream as being a "fallback" server, to handle requests when the others cannot. This is useful in scenarios when<a id="id552" class="indexterm"/> the fallback server is able to deliver a<a id="id553" class="indexterm"/> customized response based on the requested URI:</p><div class="informalexample"><pre class="programlisting">upstream app {

  server 127.0.0.1:9000;

  server 127.0.0.1:9001;

  server 127.0.0.1:9002;

}

server {

  location / {

    error_page 500 502 503 504 = @fallback;

    proxy_pass http://app;
  }

  location @fallback {

    proxy_pass http://127.0.0.1:8080;

  }
}</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip11"/>Tip</h3><p>The "<code class="literal">=</code>" notation shown in the preceding <code class="literal">error_page</code> line is used to indicate that we want to return the status code resulting from the last parameter; in this case, the <code class="literal">@fallback</code> location.</p></div></div><p>These examples cover cases in which the error code was 500 or greater. NGINX can also supply an <code class="literal">error_page</code> for error codes 400 or greater, when the <code class="literal">proxy_intercept_errors</code> directive is set to <code class="literal">on</code>, as in the following example:</p><div class="informalexample"><pre class="programlisting">server {

  proxy_intercept_errors on;

  error_page 400 403 404 /40x.html;

  location = /40x.html {

    root share/examples/nginx/html;

  }
}</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note08"/>Note</h3><p>When HTTP error code 401 is configured to be served from an <code class="literal">error_page</code>, the authentication will not complete. You may want to do this in situations when the authentication backend is offline, for maintenance or other reasons, but you should otherwise avoid them.</p></div></div></div>
<div class="section" title="Determining the client's real IP address"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec34"/>Determining the client's real IP address</h1></div></div></div><p>When using a proxy server, <a id="id554" class="indexterm"/>the clients don't have a direct connection to the upstream servers. The upstream servers, therefore, aren't able to get information directly from those clients. Any information, such as the client's IP address, would need to be passed via headers. NGINX <a id="id555" class="indexterm"/>provides this with the <code class="literal">proxy_set_header</code> directive:</p><div class="informalexample"><pre class="programlisting">proxy_set_header X-Real-IP $remote_addr;
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</pre></div><p>The client's IP address will then be available in both the <code class="literal">X-Real-IP</code> and <code class="literal">X-Forwarded-For</code> headers. The second form takes a client request header into account. If present, the IP address of the request will be added to the <code class="literal">X-Forwarded-For</code> header from the client, separated by a comma. Depending on your upstream server configuration, you will need one or the other of these. Configuring Apache, for example, to use the <code class="literal">X-Forwarded-For</code> header for the client's IP address in its logs is done using the <code class="literal">%{&lt;header-name&gt;}i</code> formatting option.</p><p>The following example shows how to change the default 'combined' Apache log format:</p><div class="informalexample"><pre class="programlisting">LogFormat "%{X-Forwarded-For}i %l %u %t \"%r\" %&gt;s %b \"%{Referer}i\" \"%{User-Agent}i\"" combined</pre></div><p>If your upstream server, on the other hand, requires a non-standard header such as <code class="literal">Client-IP</code>, then this can easily be configured with the following:</p><div class="informalexample"><pre class="programlisting">proxy_set_header Client-IP $remote_addr;</pre></div><p>Other information, such as the <code class="literal">Host</code> header, can be passed to the upstream servers in the same manner:</p><div class="informalexample"><pre class="programlisting">proxy_set_header Host $host;</pre></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec35"/>Summary</h1></div></div></div><p>We have seen how NGINX can be used as a reverse proxy. Its efficient connection-handling model is ideal for interfacing directly with clients. After having terminated requests, NGINX can then open new ones to upstream servers, taking into account the strengths and weaknesses of each upstream server. Using <code class="literal">if</code> inside a location is only considered valid under certain situations. By thinking about how NGINX actually handles a request, we can develop a configuration that is more suited to what we want to achieve. If NGINX cannot reach an upstream server for any reason, it can serve another page instead. As NGINX terminates the clients' requests, the upstream servers can obtain information about the client only via headers passed in NGINX's proxied request. These concepts will help you design an ideal NGINX configuration to match your needs.</p><p>Coming up in the next chapter, we will explore more advanced reverse-proxy techniques.</p></div></body></html>