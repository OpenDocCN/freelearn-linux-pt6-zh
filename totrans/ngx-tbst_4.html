<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Optimizing Website Performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Optimizing Website Performance</h1></div></div></div><p>One of the most popular reasons to migrate to Nginx is striving for better performance. Over the years, Nginx has acquired a certain reputation of being a silver bullet, a speed beast. Sometimes, this reputation may harm the project, but it is definitely earned. In many situations, that is exactly what happens: you <span class="emphasis"><em>add</em></span> Nginx to a website setup as if it is a concoction ingredient and the website magically becomes faster. We will not explain the basics of how to set up Nginx because you probably know it all pretty well. In this chapter, we are going to delve a little into why this happens and what are the less-known options that will help you squeeze more out of your website.</p><p>We will cover these topics in the chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How Nginx processes the requests</li><li class="listitem" style="list-style-type: disc">Nginx caching subsystems</li><li class="listitem" style="list-style-type: disc">Optimizing the upstreams</li><li class="listitem" style="list-style-type: disc">Some new Nginx features such as thread pools</li><li class="listitem" style="list-style-type: disc">Other performance issues</li></ul></div><p>The overwhelming majority of all performance problems people have with Nginx-powered websites are actually on the upstreams. We will try to at least mention some of the methods you may use to tackle the challenge of optimizing your upstream application servers, but we will concentrate on the Nginx itself mostly. You will have to understand the inner workings of Nginx and reverse proxying in general, and we are devoting a good part of the chapter to explain the principles implemented in Nginx that let it run around other older web servers in terms of performance.</p><p>The bad news is that you probably won't be able to optimize Nginx very much. If you embarked on a project of making your website sufficiently, significantly faster, and started with inserting Nginx between the application and the users, then you have probably already done the most important steps in moving towards your goal. Nginx is extremely optimal in the sense of avoiding doing extra, unneeded work, and that is the core of any optimization.</p><p>Still, some of the configuration defaults may be too conservative for the sake of compatibility, and we will try to talk about this.</p><div class="section" title="Why Nginx is so fast?"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec17"/>Why Nginx is so fast?</h1></div></div></div><p>The <a class="indexterm" id="id132"/>question is intentionally formulated in an oversimplified way. This is what you might hear from your boss or client—let us migrate from old technologies to Nginx because it will make our website faster and users happier. The migration process is described in thousands of online articles and even some books, and we will not write about it here. Many of our readers have probably gone down that path several times and know the facts: first, it is usually true that websites get faster and second, no, it is not usually a full migration. You will rarely dispose of Apache completely and plug Nginx in its place. Although this "total conversion" also happens, most of the time you start with inserting Nginx between Apache and the Internet. To understand why this is okay, why this helps at all, and how to move forward from there, read on.</p><p>To describe the main conceptual change that is implemented by using Nginx as a reverse proxy we will use, for simplicity, the processing model of Apache 1.x, that is, a very old piece of software written in premultithreading traditions. The latest Apache version, which is 2.x, may use another, slightly more efficient model, which is based on threads instead of processes. But in comparison to Nginx, those two models look very similar, and the older one is easier to understand.</p><p>This is a simple diagram of how one HTTP request-response pair is processed:</p><div class="mediaobject"><img alt="Why Nginx is so fast?" src="graphics/B04329_04_01.jpg"/></div><p>Here is<a class="indexterm" id="id133"/> an explanation of the diagram:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">A user's browser opens a connection to your server using TCP.</li><li class="listitem">A web server software that runs on your server and listens to a particular set of TCP ports, accepts the connection, dedicates a part of itself to processing this connection, separates this part, and returns to listening and accepting other incoming connections. In the case of the Apache 1.x model, the separated part is a child process that has been forked beforehand and is waiting in the pool.</li><li class="listitem">There are usually some limits in place on how many concurrent connections may be processed and they are enforced on this step. It is very important to understand that this is the part where scaling happens.</li><li class="listitem">This dedicated part of the web server software reads the actual request URI, interprets it, and finds the relevant file or any other way to generate the response. Maybe that would even be an error message; it doesn't matter. It starts sending this response into the connection.</li><li class="listitem">The user's browser receives the bytes of the response one by one and generates pixels on the user's screen. This is actually a real job and a long one. Data is sent over hundreds of kilometers of wires and optical fiber, emitted into the air as electromagnetic waves and then "condensed" by induction into current again. From the viewpoint of your server, most of your users are on excruciatingly slow networks. The web server is literally feeding those browsers large amounts of data through a straw.</li></ol></div><p>There is<a class="indexterm" id="id134"/> nothing that could be done to solve the fifth point. The last mile will always be the slowest link in the chain between your server and the user. Nginx makes a conceptual optimization on step 2 and scales much better this way. Let us explain that at a greater length.</p><p>Due to slow client connections, a snapshot of any popular website server software at any particular moment in time looks like this: a couple of requests that are actually processed in the sense that there is some important work being done by the CPU, memory, and disks and then a couple of thousands of requests for which all processing is done, responses are already generated and are very slowly, piece by piece inserted into the narrow connections to the users' browsers. Again, this is a simplified model, but still very adequate to explain what actually happens.</p><p>To implement scaling on step 2, the original Apache 1.x uses a mechanism that is very natural for all UNIX-based systems—it forks. There are some optimizations, for example, in the form of having a pool of processes forked beforehand (hence, the "prefork" model), and Apache 2.x may use threads instead of processes (also with pregenerated pools and all), but the idea is the same: scaling is achieved by handling individual requests to a group of some OS-level entities, each of which is able to work on a request and then send the data to the client. The problem is that those entities are rather big; you don't just need a group, but more like a horde of them, and most of the time, they do a very simple thing: they send bytes from a buffer into a TCP connection.</p><p>Nginx and other state machine-based servers significantly optimize step 2 by not making big, complex OS-level processes or threads do a simple job while hogging the memory at the same time. This is the essence of why Nginx suddenly makes your website faster—it manages to slowly feed all those thousands of very bandwidth-limited client connections using very little memory, saving on RAM.</p><p>An inquisitive reader may ask the question here about why adding Nginx as a reverse proxy without removing Apache still saves memory and speeds up websites. We believe that you already should have all the knowledge to come up with the correct answer for that. We will mention the most important part as a hint: the horde of Apaches is not needed anymore because Apache only does the response generation—the smartest and hardest thing—while offloading the dumb job of pushing bytes to thousands of slow connections. The reverse proxy is acting as a proxy client on behalf of all the users' browsers with the very important distinction: this client is sitting very close to the server and is capable of receiving the bytes of the response lightning fast.</p><p>So, the secret sauce to Nginx's performance is not its magical code quality (although it is written very well), but the fact that it saves up on system resources, mostly memory, by not making<a class="indexterm" id="id135"/> huge copies of data for each individual request it is processing. Interestingly enough, modern operating systems all have different low-level mechanisms to avoid excessive copying of data. Long gone are times when <code class="literal">fork()</code> literally created a whole copy of all code and data. As virtual memory and network subsystems get more and more sophisticated, we may end up with a system where the state machine as a model to code tight event-processing loops won't be needed any more. As of now, they still bring noticeable improvements.</p></div></div>
<div class="section" title="Optimizing individual upstreams"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec18"/>Optimizing individual upstreams</h1></div></div></div><p>You <a class="indexterm" id="id136"/>may remember from previous chapters that Nginx has two main methods of generating a response to a request, one being very specific—reading a static file from the filesystem, and the other including a whole family of the so-called upstream modules. An upstream is an external server to which Nginx proxies the request. The most popular upstream is <code class="literal">ngx_proxy</code>, others are <code class="literal">ngx_fastcgi</code>, <code class="literal">ngx_memcached</code>, <code class="literal">ngx_scgi</code>, and so on. Because serving only static files is not usually enough for a modern website, upstreams are an essential part of any comprehensive setup. As we mentioned in the beginning of this chapter, upstreams themselves are usually the reason why your website has performance troubles. Your developers are responsible for this part because this is where all the web application processing happens. In the following sections, we are going to briefly describe the major stacks or platforms used to implement business logic on the upstream behind Nginx and the directions you should at least look in for clues about what to optimize.</p><div class="section" title="Optimizing static files"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec31"/>Optimizing static files</h2></div></div></div><p>Any <a class="indexterm" id="id137"/>web application will contain static resources that do not change and do not depend on the user currently using the application. Those are usually known as static files in webmaster parlance and consist of all the static images, CSS, JavaScript, and some other extra data, for example, <code class="literal">cross-domain.xml</code> files that are used by access control policies of the browsers. Serving the data directly from the application is usually supported to facilitate simple setups without any frontend, intermediate, accelerating server such as Nginx. Nginx's built-in HTTP proxy will happily serve them, and in the case of local caching, may even do that without any noticeable performance loss. However, such a setup is not recommended as a long-term solution if you strive for maximum performance.</p><p>One universal step that we feel the need to recommend (or remind of) is moving as much of the static data from the upstream under the control of Nginx. It will make your application more fragmented, but it will also be a very good performance optimization method trumping many of other potential and much harder to implement methods. If your upstreams <a class="indexterm" id="id138"/>serve static files, then you need to make them available as files to Nginx and serve them directly. This might be the first thing you do when you receive a new legacy upstream to optimize. It is also a very easy task to accomplish yourself or implement as a part of the whole deployment process.</p></div><div class="section" title="Optimizing PHP backends"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec32"/>Optimizing PHP backends</h2></div></div></div><p>For<a class="indexterm" id="id139"/> several years, the modern way to run PHP applications behind Nginx front is the PHP-FPM or FastCGI Process Manager. As you may guess, it uses the FastCGI protocol and will require FastCGI upstream module in Nginx. However, when dealing with inherited legacy PHP websites, you may still meet the older ways of running the code, which will be your first candidates for optimizations.</p><p>There is the official Apache way using the <code class="literal">mod_php</code> Apache module. This module embeds PHP interpreter directly into (each and every!) Apache process. Most of the time, you will inherit Apache websites configured to run in this way. The main good side of embedded interpreters is well known—the code may be saved in some intermediate form between requests and not reinterpreted every time. The <code class="literal">mod_php</code> Apache module does that wonderfully and some people call it the single reason why PHP gained popularity on the Web in the first place. Well, the way to deal with <code class="literal">mod_php</code> in 2016 is getting rid of it, together with Apache.</p><p>Many PHP codebases can be moved from <code class="literal">mod_php</code> to PHP-FPM almost effortlessly. After this, you will change your main Nginx upstream from HTTP proxying to directly speaking FastCGI protocol with your PHP scripts that are kept running and ready by the FPM.</p><p>Sometimes, your developers will need to invest some resources into mostly restructuring and refactoring code to be runnable in a separate process without any help from Apache. One particularly difficult case is a code that relies heavily on calling into the Apache internals. Fortunately, this is not nearly as common in PHP codebases as it was in the <code class="literal">mod_perl</code> codebases. I will mention dealing with Perl-based websites later.</p><p>Another really old (and odd) way to run PHP is CGI scripts. Each and every web administrator did or will write a fair amount of temporary CGI scripts. You know, the kind of temporary scripts that live on and on through generations of hardware, managers, and business models. They rarely power parts of production that are user-oriented. Anyway, CGI was not popular with PHP at all because of the ubiquity and rather good quality of <code class="literal">mod_php</code> and Apache. Nevertheless, you may have some in your legacy, especially if that code had or has some chances to run on Windows.</p><p>CGI scripts are executed as separate processes for each request/response pair and therefore are prohibitively expensive. The only upsides of using CGI are increased compatibility with other Apache modules and another degree of privilege separation. Those are trumped by the performance compromises in all but the most exotic scenarios. By the way, Nginx <a class="indexterm" id="id140"/>will make a CGI-powered portion of your website significantly better by buffering the output and releasing the resources on the backend. You still have to plan the rewrite of those parts to be run as FastCGI under FPM as soon as possible.</p><p>PHP-FPM uses the same prefork model as does Apache 1.x and that renders some of the familiar knobs under your control. For example, you may configure the number of working processes FPM starts, the upper limit of the requests that may be processed by one child, and also the size of the available child processes pool. All those parameters may be set via the <code class="literal">php-fpm.conf</code> file, which is usually installed directly in <code class="literal">/etc</code> and following a good convention includes <code class="literal">/etc/php-fpm.d/*.conf</code>.</p></div><div class="section" title="Java backends"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec33"/>Java backends</h2></div></div></div><p>The Java<a class="indexterm" id="id141"/> ecosystem is so huge that there is a whole bookshelf devoted solely to different Java web servers. We cannot delve deeper into such a topic. If you as an administrator have never had any experience with Java web applications, you will be happy to know that most of the time, those apps run their own web servers that do not depend on Apache. This is a list of popular Java web servers that you may encounter inside your upstreams: Apache Tomcat, Jetty, and Jboss/WildFly. Java applications are usually built on top of huge and comprehensive frameworks that employ a web server as one of the components. Your Nginx web accelerator will talk to the Java upstream via normal HTTP protocol using the <code class="literal">ngx_proxy</code> module. All the usual <code class="literal">ngx_proxy</code> optimizations apply, therefore. See a note on caching later in this chapter for examples.</p><p>There is little you can do to make a Java application perform better without getting your hands dirty deep inside the code. Some of the steps available from the level of system administration are:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Choosing the right JVM. Many Java web servers support several different Java Virtual Machine implementations. The HotSpot JVM from Oracle (Sun) is considered one of the best, and you will probably start with that. But there are others; some of them are commercially available, for example, Azul Zing VM. They might provide you with a little performance boost. Unfortunately, changing JVM vendor is a huge step prone to incompatibility errors.</li><li class="listitem">Tuning threading parameters. Java code is traditionally written using threads that are a native and natural feature of the language. JVMs are free to implement threads using whatever resources they have. Usually, you will have a <a class="indexterm" id="id142"/>choice of using either operating system-level threads or the so-called "green threads," which are implemented in userland. Both approaches have advantages and disadvantages. Threads are usually grouped into pools, which are preforked in a fashion that is very similar to what Apache 1.x does with processes. There are a number of models that thread pools use to optimize both memory and performance, and you, as administrator, will be able to tune them up.</li></ol></div></div><div class="section" title="Optimizing Python and Ruby backends"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec34"/>Optimizing Python and Ruby backends</h2></div></div></div><p>Python<a class="indexterm" id="id143"/> and Ruby both built their strength as more open and clear alternatives to Perl and PHP in the age when web applications were already one of the dominant way to deploy business logic. They both started late and with a clear goal of being very web-friendly. There were both the <code class="literal">mod_python</code> and <code class="literal">mod_ruby</code> Apache modules that embedded interpreters into the Apache web server processes, but they quickly went out of fashion. The Python community developed <a class="indexterm" id="id144"/>the <span class="strong"><strong>Web Server Gateway Interface</strong></span> (<span class="strong"><strong>WSGI</strong></span>) protocol as a generic interface to write web applications regardless of deployment options. This allowed free innovation in the actual web server space that mostly converged on a couple of standalone WSGI servers or containers (such as gunicorn and uWSGI) and <code class="literal">mod_wsgi</code> Apache module. They all may be used to run a Python web application without changing any code.</p><p>So, it was very natural that Nginx developed its own WSGI upstream module, <code class="literal">ngx_wsgi</code>, which you should use to replace any other WSGI implementation. The actual migration path may be a little bit more complex. If the backend application used to run under Apache + <code class="literal">mod_wsgi</code>, then, by all means, switch to <code class="literal">ngx_wsgi</code> immediately and ditch Apache. Otherwise, for the sake of smoothness and stability, you may start with a simpler <code class="literal">ngx_proxy</code> configuration and then move to <code class="literal">ngx_wsgi</code>.</p><p>You may also encounter an application that uses long-polling (named Comet sometimes) and WebSockets, and runs on a special web server, for example, Tornado (of the FriendFeed fame). These are problems mostly because synchronous communication between the web server and the clients defeats the main advantage of Nginx as an accelerating reverse proxy—the part of the server that processes a request won't be made available quickly for another request by handling the byte pushing to the Nginx frontend. Modern Nginx supports proxying both Comet requests and Web Sockets, of course, but without any acceleration that you may have gotten used to.</p><p>The Ruby ecosystem went a slightly different way because there was (and still is) a so-called killer app for Ruby, that is, the Ruby on Rails web application framework. Most of the Ruby web applications are built on Ruby on Rails, and there was even a joke that it is high time to rename the whole language Ruby on Rails because nobody uses Ruby without those Rails. It is a wonderfully designed and executed web framework with many revolutionary ideas that inspired a whole wave of rapid application development techniques throughout the industry. It also decoupled the application developers from the problems of deploying their work by providing the web server that could be shared on the Internet right away.</p><p>The current <a class="indexterm" id="id145"/>Ruby on Rails preferred deployment options are either using Phusion Passenger or running a cluster of Unicorn web servers. Both options are fine for your task of migrating to Nginx. Phusion Passenger is a mature example of providing its own in-process code as it contains modules for both Apache and Nginx web servers. So, if you are lucky, you will switch from one to the other effortlessly. Passenger will still run worker processes outside of your main Nginx workers, but the module allows Nginx to communicate freely. It is a good example<a class="indexterm" id="id146"/> of a custom upstream module. See <a class="ulink" href="https://www.phusionpassenger.com/library/deploy/nginx/deploy/ruby/">https://www.phusionpassenger.com/library/deploy/nginx/deploy/ruby/</a> Passenger guide for the actual instructions. Passenger may also run in the standalone mode exposing HTTP to the world. That is also the way Unicorn deploys Ruby applications. You know the way to deal with that—the universal helper <code class="literal">ngx_proxy</code>.</p></div><div class="section" title="Optimizing Perl backends"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec35"/>Optimizing Perl backends</h2></div></div></div><p>Perl<a class="indexterm" id="id147"/> was the first widely used server-side programming language for the Web. We may say that it is Perl that brought the notion of dynamically generated web pages to popularity and paved the way for the web applications galore we experience today. There are still plenty of Perl-powered web businesses of various sizes, from the behemoths such as <a class="ulink" href="https://www.booking.com">https://www.booking.com</a> to smaller, feisty, ambitious startups such as DuckDuckGo. You might also have seen a couple of MovableType-powered blogs. This is a professional blogging platform developed by SixApart and then resold several times.</p><p>Perl is <a class="indexterm" id="id148"/>also the most popular language to write CGI scripts, and that is also the single <a class="indexterm" id="id149"/>reason why it is considered slow. CGI is a simple interface to run external programs from inside a web server. It is rather inefficient because it usually involves forking an operating system-level process and then shutting it down after a single request. This model plus the interpreting nature of Perl means that Perl CGI scripts are so suboptimal that they are used as a model of inefficient web development platforms.</p><p>If you have a user-facing, dynamic web page generated by a CGI script run from Apache, you have to get rid of it. See below for details.</p><p>There are a number of more advanced ways to run Perl code in production. Partly inspired by the <code class="literal">mod_php</code> success, there is a long-running project named <code class="literal">mod_perl</code>, which is an Apache module embedding the Perl interpreter into Apache processes. It is also highly successful because it is stable and robust, and powers a lot of heavily loaded websites. Alas, it is also rather complex, both for the developer and the administrator. Another difference from the <code class="literal">mod_php</code> Apache module is that <code class="literal">mod_perl</code> failed to provide strong separation of environments, which is vital for the virtual hosting businesses.</p><p>Anyway, if you have inherited a website based on <code class="literal">mod_perl</code>, you have several options. First, there might be a cheap way to move to the PSGI or FastCGI models that will allow you to get rid of Apache. The module Apache::Registry,which emulates a CGI environment inside <code class="literal">mod_perl,</code> may be a great sign of such situation. Second, the code may be written in a way<a class="indexterm" id="id150"/> that couples it tightly with Apache. The <code class="literal">mod_perl</code> module provides an interface to hook deeply into Apache's internals, which while providing several interesting capabilities for the developer, also makes it much harder to migrate. The developers will have to investigate the methods used in the software and make a final decision. They may decide to leave Apache + <code class="literal">mod_perl</code> alone and continue to use it as a heavy and over-capable process manager.</p><p>Moving CGI to <code class="literal">mod_perl</code> nowadays is never a good way forward, we do not recommend it.</p><p>There are a number of FastCGI managers for Perl that are similar to PHP-FPM described earlier. They all are very lucky options for you as the Nginx administrator because most of the time the migration will be smooth and easy.</p><p>One <a class="indexterm" id="id151"/>of the interesting recent modes to run Perl code in web servers is the so-called <a class="indexterm" id="id152"/>
<span class="strong"><strong>Perl Server Gateway Interface</strong></span> (<span class="strong"><strong>PSGI</strong></span>). It is more or less a direct port of Rack architecture from the Ruby stack to Perl. It is interesting that PSGI was invented and implemented in the world where Nginx was already popular. Therefore, if you have a web application that uses PSGI, it was most probably tested and run behind Nginx. No need to port anything. PSGI might be the most important target architecture to upgrade CGI or the <code class="literal">mod_perl</code> applications.</p><p>Bigger Perl web frameworks usually have a number of ways to run the applications. For example, both Dancer and the older Catalyst provide the glue scripts to run the same application as a separate web server (which you might expose to the world with the help of the Nginx <code class="literal">ngx_proxy</code> upstream), as a <code class="literal">mod_perl</code> application or even as a CGI script. Not all of those methods are suitable for production, but they will definitely help in migration. Never accept "we should rewrite everything from scratch" as a recommendation from the developers before weighing other options. If the application was written during the last 3–4 years, it should definitely have PSGI implemented directly or via its framework.</p><p>PSGI applications are run with the help of special PSGI servers, such as Starman or Starlet, that speak simple HTTP to the outside world. Nginx will use the <code class="literal">ngx_proxy</code> upstream for such applications.</p></div></div>
<div class="section" title="Using thread pools in Nginx"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec19"/>Using thread pools in Nginx</h1></div></div></div><p>Using<a class="indexterm" id="id153"/> asynchronous, event-driven architecture serves Nginx <a class="indexterm" id="id154"/>well as it allows to save up on the precious RAM and CPU context switches while processing thousands and millions of slow clients in separate connections. Unfortunately, event loops, such as the one that power Nginx, easily fail when facing blocking operations. Nginx was born on FreeBSD, which has several advantages over Linux, and one of the relevant ones is a robust, asynchronous input/output implementation. Basically, the OS kernel is able to not block on traditionally blocking operations like reading data from disks by having its own kernel-level background threads. Linux, on the other hand, requires more work from the application side, and very recently, in version 1.7.11, the Nginx team released its own thread pools feature to work better on Linux. You may find a good introduction into the problem and the solution <a class="indexterm" id="id155"/>in this official Nginx blog post at <a class="ulink" href="https://www.nginx.com/blog/thread-pools-boost-performance-9x/">https://www.nginx.com/blog/thread-pools-boost-performance-9x/</a>. We will provide an example of the configuration you may use to turn on thread pools on your web server. Remember that you will only need this on Linux.</p><p>To turn on background threads that will perform blocking input/output operations without stalling the main loop you use the directive <code class="literal">aio</code> in this way:</p><div class="informalexample"><pre class="programlisting">server {
    location /file {
        root /mnt/huge-static-storage;
        aio threads;
    }
}</pre></div><p>You may know the <code class="literal">aio</code> directive that is used to turn on the Async IO interface, so it is a natural fit for its use to be extended this way.</p><p>The implementation is rather simple to explain from a very high level. Transparently to you, Nginx will run a number (pool) of background, userland-level threads that fulfill the input/output tasks. Nginx will continue to run the main event loop in parallel to waiting for the slow disk or the network.</p></div>
<div class="section" title="The caching layer of Nginx"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec20"/>The caching layer of Nginx</h1></div></div></div><p>If there is<a class="indexterm" id="id156"/> one universally known and acclaimed algorithm to speed things up, it is caching. Pragmatically speaking, caching is a process of not doing the same work many times. Ideally, each distinct computational unit should be executed once. This, of course, never happens in the real world. Still, techniques to minimize repetitions by rearranging work or using saved results are very popular. They form a huge discipline named "dynamic programming."</p><p>In the context of a web server, caching usually means saving the generated response in a file so that the next time when the same request is received; it could be processed by reading this file and not computing the response again. Now please refer to the steps outlined in the first section of this chapter. For many of the real-world websites, the actual computing <a class="indexterm" id="id157"/>of the responses is not the bottleneck; transferring those responses to the slow clients is. That's why the most efficient caching happens right in the browser, or as developers prefer to say, on the client side.</p><div class="section" title="Emitting caching headers"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec36"/>Emitting caching headers</h2></div></div></div><p>All <a class="indexterm" id="id158"/>browsers (and even many non-browser HTTP clients) support client-side caching. It is a part of the HTTP standards, albeit one of the most complex to understand. Web servers do not control client-side caching to full extent, obviously, but they may issue recommendations about what to cache and how, in the form of special HTTP response headers. This is a topic thoroughly discussed in many great articles and guides, so we will mention it shortly, and with a lean towards problems you may face and how to troubleshoot them.</p><p>In spite of the fact that browsers have been supporting caching on their side for at least 20 years, configuring cache headers was always a little confusing, mostly due to the fact that there are two sets of headers designed for the same purpose, but having different scopes and totally different formats.</p><p>There is the <code class="literal">Expires:</code> header, which was designed as a quick and dirty solution and also the new (relatively) almost omnipotent <code class="literal">Cache-Control:</code> header, which tries to support all the different ways an HTTP cache could work.</p><p>This is an example of a modern HTTP request-response pair containing the caching headers. These are the request headers sent from the browser (here Firefox 41, but it does not matter):</p><div class="informalexample"><pre class="programlisting">User-Agent:"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:41.0) Gecko/20100101 Firefox/41.0"
Accept:"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
Accept-Encoding:"gzip, deflate"
Connection:"keep-alive"
<span class="strong"><strong>Cache-Control:"max-age=0"</strong></span>
</pre></div><p>Then, the response headers are:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>Cache-Control:"max-age=1800"</strong></span>
Content-Encoding:"gzip"
Content-Type:"text/html; charset=UTF-8"
Date:"Sun, 10 Oct 2015 13:42:34 GMT"
<span class="strong"><strong>Expires:"Sun, 10 Oct 2015 14:12:34 GMT"</strong></span>
Server:"nginx"
X-Cache:"EXPIRED"</pre></div><p>We highlighted the parts that are relevant. Note that some directives may be sent by both sides of the conversation. The browser sent the <code class="literal">Cache-Control: max-age=0</code> header because the user pressed the <span class="emphasis"><em>F5</em></span> key. This is an indication that the user wants to receive a response that is fresh. Normally, the request will not contain this header and will allow any intermediate cache to respond with a stale but still nonexpired response.</p><p>In this<a class="indexterm" id="id159"/> case, the server we talked to responded with a gzipped HTML page encoded in UTF-8 and indicated that the response is okay to use for half an hour. It used both mechanisms available, the modern <code class="literal">Cache-Control:max-age=1800</code> header and the very old <code class="literal">Expires:Sun, 10 Oct 2015 14:12:34 GMT</code> header.</p><p>The <code class="literal">X-Cache: "EXPIRED"</code> header is not a standard HTTP header, but was also probably (there is no way to know for sure from the outside) emitted by Nginx. It may be an indication that there are, indeed, intermediate caching proxies between the client and the server, and one of them added this header for debugging purposes. The header may also show that the backend software uses some internal caching.</p><p>Another possible source of this header is a debugging technique used to find problems in the Nginx cache configuration. The idea is to use the cache hit or miss status, which is available in one of the handy internal Nginx variables as a value for an extra header, and then you are able to monitor the status from the client side. This is the code that will add such a header:</p><div class="informalexample"><pre class="programlisting">add_header X-Cache $upstream_cache_status;</pre></div><p>Nginx has a special directive that transparently sets up both standard cache control headers, and it is named <code class="literal">expires</code>. This is a piece of the <code class="literal">nginx.conf</code> file using the <code class="literal">expires</code> directive:</p><div class="informalexample"><pre class="programlisting">location ~* \.(?:css|js)$ {
  expires 1y;
  add_header Cache-Control "public";
}</pre></div><p>The pattern uses the so-called noncapturing parentheses, which is a feature first appeared in Perl regular expressions. The effect of this regexp is the same as that of a simpler <code class="literal">\.(css|js)$</code> pattern, but the regular expression engine is specifically instructed not to create a variable containing the actual string from inside the parentheses. This is a simple optimization.</p><p>Then, the <code class="literal">expires</code> directive declares that the content of the <code class="literal">css</code> and <code class="literal">js</code> files will expire after a year of storage. The actual headers as received by the client will look like this:</p><div class="informalexample"><pre class="programlisting">Server: nginx/1.9.8 (Ubuntu)
Date: Fri, 11 Mar 2016 22:01:04 GMT
Content-Type: text/css
Last-Modified: Thu, 10 Mar 2016 05:45:39 GMT
Expires: Sat, 11 Mar 2017 22:01:04 GMT
Cache-Control: max-age=31536000</pre></div><p>The last two<a class="indexterm" id="id160"/> lines contain the same information in wildly different forms. The <code class="literal">Expires:</code> header is exactly one year after the date in the <code class="literal">Date:</code> header, whereas <code class="literal">Cache-Control:</code> specifies the age in seconds so that the client can do the date arithmetics itself.</p><p>The last directive in the provided configuration extract adds another <code class="literal">Cache-Control:</code> header with a value of <code class="literal">public</code>. What this means is that the content of the HTTP resource is not access-controlled and therefore may be cached not only for one particular user but also anywhere else. A simple and effective strategy that was used in offices to minimize consumed bandwidth was to have an office-wide caching proxy server. When one user requested a resource from a website on the Internet and that resource had a <code class="literal">Cache-Control: public</code> designation, the company cache server would store that to serve to other users on the office network.</p><p>This may not be as popular today due to cheap bandwidth, but because history has a tendency to repeat itself, you need to know how and why <code class="literal">Cache-Control: public</code> works.</p><p>The<a class="indexterm" id="id161"/> Nginx <code class="literal">expires</code> directive is surprisingly expressive. It may take a number of different values. See this table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">off</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This value turns off the Nginx cache headers logic. Nothing will be added, and more importantly, the existing headers received from upstreams will not be modified.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">epoch</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is an artificial value used to purge a stored resource from all caches by setting the <code class="literal">Expires</code> header to <span class="strong"><strong>"1 January, 1970 00:00:01 GMT"</strong></span>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">max</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the opposite of the "epoch" value. The <span class="strong"><strong>Expires</strong></span> header will be equal to <span class="strong"><strong>"31 December 2037 23:59:59 GMT"</strong></span>, and the <span class="strong"><strong>Cache-Control max-age</strong></span> set to 10 years. This basically means that the HTTP responses are guaranteed to never change, so clients are free to never request the same thing twice and may use their own stored values.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Specific duration</p>
</td><td style="text-align: left" valign="top">
<p>An actual specific duration value means an expiry deadline from the time of the respective request. For example, <code class="literal">expires 10w</code>. A negative value for this directive will emit a special header <code class="literal">Cache-Control: no-cache</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">"modified" specific time</code>
</p>
</td><td style="text-align: left" valign="top">
<p>If you add the<a class="indexterm" id="id162"/> keyword "modified" before the time value, then the expiration moment will be computed relatively to the modification time of the file that is served.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">"@" specific time</code>
</p>
</td><td style="text-align: left" valign="top">
<p>A time with an @ prefix specifies an absolute time-of-day expiry. This should be less than 24 hours. For example, Expires @17h;.</p>
</td></tr></tbody></table></div><p>Many web <a class="indexterm" id="id163"/>applications choose to emit the caching headers themselves, and this is a good thing. They have more information about which resources change often and which never change. Tampering with the headers that you receive from the upstream may or may not be a thing you want to do. Sometimes, adding headers to a response while proxying it may produce a conflicting set of headers and therefore create unpredictable behavior.</p><p>The static files that you serve with Nginx should have the <code class="literal">expires</code> directive in place. However, the general advice about upstreams is to always examine the caching headers you get and refrain from overoptimizing by setting up a more aggressive caching policy.</p><p>The corporate caching proxy configuration that we described earlier in this chapter together with an erroneous <code class="literal">public</code> caching policy on nonpublic resources may result in a situation where some users will see pages that were generated for other users behind the same caching proxy. The way to make that happen is surprisingly easy. Imagine that your client is a book shop. Their web application serves both public pages with book details, cover images, and so on and private resources with recommendation pages and the shopping cart. Those will probably have the same URL for all users and once, by mistake, declared as <code class="literal">public</code> with the expiration date in the distant future, they may freely be cached by intermediate proxies. Some more intelligent proxies will automatically notice cookies and either add them to the cache key or refrain from caching. But then again, less sophisticated proxies do exist, and there are a number of reports when they do show pages that belong to other people.</p><p>There are even techniques such as adding a random number to all URLs to defeat such caching configurations by making all URLs unique.</p><p>We would also like to describe a combination of unique URLs and long expiration dates, which are widely used today. Modern websites are very dynamic, both in the sense of what happens to the document after it is loaded and how often the client-side code changes. It is not unusual to have not only daily but even hourly releases. This is a luxury of the web as an application delivery mechanism, and people seize the opportunity. How to combine rapid releases with caching? The first idea was to code the version into the URLs. It works surprisingly well. After each release, all the URLs change; the old ones start to slowly expire in the cache stores of different levels, whereas the new ones are requested directly from the origin server.</p><p>One clever <a class="indexterm" id="id164"/>trick was developed upon this scheme, and it uses a hash of the content of the resource instead of the version number as a unique element of the URL. This reduces extra cache misses when a new release does not change all the files.</p><p>Implementing this trick is done on the application side. Nginx administrator is only responsible for setting up long expiration date by using, for example, the <code class="literal">expires max</code> directive.</p><p>The one obvious thing that limits the effect of the client-side caching is that many different users may issue the same or similar requests, and those will all reach the web server. The next step to never doing the same work many times is caching on the server.</p></div><div class="section" title="Caching in Nginx upstream modules"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec37"/>Caching in Nginx upstream modules</h2></div></div></div><p>Caching<a class="indexterm" id="id165"/> infrastructure is implemented as a part of the upstream interface if you excuse us to use object-oriented programming terminology. Each of those upstream modules has a group of very similar directives, which allow you to configure the local caching of responses from that particular upstream.</p><p>The basic scheme is very simple—once a request is determined as an upstream material, it is rerouted to the relevant module. If there's caching configured for that upstream, the cache is first searched for an existing response to this request. Only when a cached response is not found, the actual proxying is performed. After this, the newly generated response is saved into the cache while being sent to the client.</p><p>It is interesting that while caching on the reverse proxy is known for a while, Nginx gained its fame as a magical accelerator without implementing it. The reason should be evident from the first section—radical changes in RAM consumption alone brought a lot of performance gains. Until the introduction of version 0.7.44, Nginx did not have any caching facilities built in. At that time, web administrators used either the famous squid HTTP proxy for caching or the <code class="literal">mod_accel</code> module for Apache. By the way, <code class="literal">mod_accel</code> module was created by Nginx's author Igor Sysoev and turned out to be the testbed for all the ideas about proper reverse proxying that were later implemented in Nginx.</p><p>Let us examine the caching directives of the most popular upstream module, <code class="literal">ngx_proxy</code>. Just to remind, this module hands over the request to another HTTP server. This is exactly how Nginx is run as a reverse proxy in front of Apache, for example. The full description is <a class="indexterm" id="id166"/>available in the great Nginx <a class="indexterm" id="id167"/>documentation at <a class="ulink" href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache">http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache</a>. We won't repeat the documentation, but we will provide additional facts and ideas instead.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Directive</p>
</th><th style="text-align: left" valign="bottom">
<p>Additional information</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_path</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This <a class="indexterm" id="id168"/>directive is clearly the main one of the whole caching family. It specifies the storage parameters of the cache store starting with the path on the filesystem. You should definitely familiarize yourself with all the options. The most important are the <code class="literal">inactive</code> and <code class="literal">max_size</code> options, which control how the Nginx cache manager removes unused data from the cache store. One required parameter in this directive is the <code class="literal">keys_zone</code>, which links the cache store to the "zone". See in the later text.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the main switch directive. It is required if you want any caching. It has a single somewhat cryptic parameter named "zone," which will be explained in detail further on. The value "off" will switch the caching off. It may be needed in cases when there is a <code class="literal">proxy_cache</code> directive further up the scope stack.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_bypass</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This directive allows you to easily specify conditions on which some responses will never be cached.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_key</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This directive creates a key that is used to identify objects in the cache. By default, the URL is used, but people add things to it quite commonly. Different responses should never have equal keys. Anything that may change the content of the page should be in the key. Besides obvious cookie values, you may want to add the client IP address if your pages depend on it (for example, use some form of geotargeting via the GeoIP database).</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_lock</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is a binary on/off switch defaulting to off. If you turn it on, then simultaneous requests for the same ("same" here means "having the same cache key") resource will not be run in parallel. Only the first request will be executed while the rest are blocked waiting.</p>
<p>The <code class="literal">proxy_cache_lock_*</code> family of directives might be interesting <a class="indexterm" id="id169"/>when you have some very expensive responses to generate.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_lock_age</code>
</p>
<p>
<code class="literal">proxy_cache_lock_timeout</code>
</p>
</td><td style="text-align: left" valign="top">
<p>These two specify additional lock parameters. Refer to the documentation for details.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_methods</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is a list of HTTP<a class="indexterm" id="id170"/> methods that are cacheable. Besides the obvious "GET" and "HEAD" methods, you might want to sometimes cache less popular methods such as "OPTIONS" or "PROPFIND" from WebDAV. There might be cases when you want to cache responses even to "POST", "PUT," and "DELETE" although that would be a very serious bending of the rules and you should really know what you are doing.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_min_uses</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This numeric parameter with a default value of "1" may be useful to optimize huge cache stores by not caching responses to rare requests. Remember that the effective cache is not the one that stores more but the one that stores useful things that get requested again and again.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_purge</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This directive specifies the additional conditions on which objects are deleted from the cache store before expiration. It may be used as a way to forcefully invalidate a cache entry. A good cache key design should not require invalidation, but we all know how often good designs of anything happen in real life.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_revalidate</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is also a Boolean directive. HTTP conditional requests with headers "If-None-Match" or "If-Modified-Since" may update the validity of objects in the cache even if they do not return any new content to the requesting client. For this, specify "on".</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_use_stale</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is an interesting directive that sometimes allows responding with an expired response from the cache. The main case to do this is an upstream failure. Sometimes, responding with a stale content is better than rejecting the request on the basis of the famous "Internal server error". From the user's point of view, this is very often the case.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_valid</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is a very rough cache expiration specification. Usually, you should control the validity of the cached data via response<a class="indexterm" id="id171"/> headers. However, if you need something quick or something broad, this directive will help you.</p>
</td></tr></tbody></table></div><p>One very important concept that is used in caching subsystems throughout all the upstream modules is that of the cache zone. A zone is a named memory region, which is accessible by its name from all Nginx processes. Readers familiar with the concept of System<a class="indexterm" id="id172"/> V-shared memory or IPC via mmap-ed regions will instantly see the similarity. Zones were chosen as an abstraction for the cache state storage, which should be shared between all the worker processes. You may configure many caches inside your Nginx instance, but you will always specify a zone for each cache. You may link different caches to the same zone, and the information about the cached objects will be shared. Zones also act as objects encapsulating the actual cache storage configuration such as where on the filesystem the cached objects will persist, how the storage hierarchy will be organized, when to purge the expired objects, and how to load the objects from disk into memory on restart.</p><p>To summarize, an administrator first sets up at least one zone with all the relevant storage parameters with the directive <code class="literal">*_cache_path</code> and then plugs subtrees of the whole URL space into those zones with the directive <code class="literal">*_cache</code>.</p><p>Zones are set up globally, usually in the <code class="literal">http</code> scope while individual caches are linked to zones with the simple <code class="literal">*_cache</code> directive in the relevant contexts, for example, locations down the path tree or the whole server blocks.</p><p>We should remind you that the described caching subsystem directives' family exists for all the upstream modules of Nginx. You will substitute <code class="literal">proxy_</code> for the other upstream moniker to end up with a whole other family of directives that do exactly the same, maybe with some slight variations for responses generated by upstreams of another type. For example, <a class="indexterm" id="id173"/>here for the information on how to cache FastCGI responses at <a class="ulink" href="http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html#fastcgi_cache">http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html#fastcgi_cache</a>.</p><p>Let us provide some real-world caching configuration examples that will help you grasp the idea better:</p><div class="informalexample"><pre class="programlisting">http {
    proxy_cache_path /var/db/cache/nginx levels=1:2 keys_zone=cache1:1m max_size=1000m inactive=600m;
    proxy_temp_path /var/db/cache/tmp;

    server {
        listen 80;
        server_name example.com;

        location / {
            proxy_pass http://localhost:8080/;
            proxy_cache cache1;
            proxy_cache_valid 200 302 24h;
            proxy_cache_valid 404 5m;
        }
        }
}</pre></div><p>This is a canonically simple cache configuration with one zone named <code class="literal">cache1</code> and one cache configured under location <code class="literal">/</code> in one server. Several important details are worth mentioning. The <a class="indexterm" id="id174"/>temporary files directory configured with the <code class="literal">proxy_temp_path</code> directive is highly recommended to be on the same filesystem as the main cache storage because otherwise, Nginx will not be able to quickly move files between the temporary and permanent storage and will instead perform an expensive file copy operation.</p><p>The <code class="literal">key_zone</code> size specifies the amount of memory dedicated to the zone. This memory is used to store the keys and metainformation about the objects in the cache and not the actual cached responses (objects). The limit on the object storage is specified in the <code class="literal">max_size</code> parameter. Nginx spawns a separate process named <code class="literal">cache manager</code>, which will constantly scan all the cache zones and remove the least used objects when the <code class="literal">max_size</code> is exceeded.</p><p>The <code class="literal">proxy_cache_valid</code> directive combination specifies a much shorter period of validity for the negative 404 results. The idea behind it is that 404 might actually be fixed, at least some of them may appear due to some misconfiguration. It makes sense to retry such requests more frequently. You should also consider the load on the upstream when making decisions about validity periods. Many computationally heavy search algorithms require much more resources to give a negative answer. It is quite understandable that to make sure that a looked for entity is absent may require checking everywhere instead of stopping after the first found instance. This is a very simplified description of a search algorithm, but it is short enough so that you will remember to always check the request processing time in the logs for negative responses and their relative amount before shortening the cache validity interval.</p><p>Two important parameters of the cache are left out in the above configuration, and this means that you will fly with default values. The <code class="literal">proxy_cache_methods</code> defaults to only caching GET and HEAD requests, which may not be optimal for your web application. And <code class="literal">proxy_cache_key</code> defaults to <code class="literal">$scheme$proxy_host$request_uri,</code> which may be dangerous if your web application make similar requests for different users. Read about these directives and either add uniqueness to the key or fall back to uncached behavior via <code class="literal">proxy_cache_bypass</code>.</p><p>Another <a class="indexterm" id="id175"/>example that we would like to present is much more complex. Let us devote a separate section to it.</p></div><div class="section" title="Caching static files"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec38"/>Caching static files</h2></div></div></div><p>When<a class="indexterm" id="id176"/> scaling a website horizontally, you will inevitably find yourself in the situation of having many identical Nginx-powered servers behind a low-level balancer. All of them will proxy the requests to the same upstream server farm, and there will be no problems with synchronizing the active, dynamic content served by your website. But if you follow the advice about having all the static content present locally to allow Nginx to serve it in the most native and efficient way possible, you will end up with a task of having many identical copies of the same files everywhere.</p><p>The other way to do the same task is a setup where a farm of Nginx instances is used to serve a huge library of static files, for example, video or music. Having a copy of that library on each Nginx node is out of the question because it is too big.</p><p>As usual, there are many possible solutions for these two cases. One choice is having a secondary smaller farm of Nginx servers serving the files to the main farm, which will employ caching inside the <code class="literal">ngx_proxy</code> upstream.</p><p>Another interesting solution uses a network filesystem mounted on the nodes. The traditional Unix NFS has a bad reputation, but in reality, on current Linux kernels, it is stable enough to be used in production. Two of the alternatives are AFS and SMBFS. The files under the mount point will look local to Nginx, but they will still be downloaded over the network, which is much slower than reading a good, local SSD. Luckily, modern Linux kernels have the ability to locally cache files from the NFS and AFS. It is named FS-Cache and uses a separate userland daemon, <code class="literal">cachefilesd</code>, to store local copies of files from a <a class="indexterm" id="id177"/>network filesystem. You may read about FS-Cache at <a class="ulink" href="https://people.redhat.com/dhowells/fscache/FS-Cache.pdf">https://people.redhat.com/dhowells/fscache/FS-Cache.pdf</a>.</p><p>FS-Cache configuration is rather straightforward, and we will not focus on it. There is another way to do it, which follows the philosophy of Nginx much more closely. SlowFS is a third-party, upstream-like module for Nginx, which provides a simple interface to a filesystem subtree. The interface includes caching capabilities, which are standard to all other Nginx upstreams.</p><p>
<span class="strong"><strong>SlowFS</strong></span> is <a class="indexterm" id="id178"/>open source under a very permissive license and is available either from the author's <a class="indexterm" id="id179"/>website or directly from GitHub as a repository. Refer to <a class="ulink" href="http://labs.frickle.com/nginx_ngx_slowfs_cache">http://labs.frickle.com/nginx_ngx_slowfs_cache</a>.</p><p>Here is an example SlowFS configuration:</p><div class="informalexample"><pre class="programlisting">http {
    slowfs_cache_path /var/db/cache/nginx levels=1:2 keys_zone=cache2:20m;
  slowfs_temp_path /var/db/cache/tmp 1 2;
    location / {
        root /var/www/nfs;
        slowfs_cache cache2;
        slowfs_cache_key $uri;
        slowfs_cache_valid 5d;
    }
}</pre></div><p>This configuration installs a transparent caching layer over files available locally in <code class="literal">/var/www/nfs</code>. It does not matter how these files are actually stored, they still will be cached <a class="indexterm" id="id180"/>according to the parameters specified with the <code class="literal">slowfs_*</code> family of directives. But obviously, you will only note any speed-up if <code class="literal">/var/db/cache</code> is much faster than <code class="literal">/var/www/nfs</code>.</p></div></div>
<div class="section" title="Replacing external redirects with internal ones"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec21"/>Replacing external redirects with internal ones</h1></div></div></div><p>As <a class="indexterm" id="id181"/>modern frontend frameworks grow more and more complex, there is an alarming rise in the number of the so-called client-side redirects. Nginx has a great facility that will allow you to save some traffic and precious client waiting time on client redirects. First, let us briefly refresh your knowledge of those redirects.</p><p>All the HTTP responses are documents consisting of three principal parts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">There's the HTTP code (200: Ok, 404: Not found, and so on)</li><li class="listitem" style="list-style-type: disc">There are a number of loosely structured key-value pairs in the form of headers</li><li class="listitem" style="list-style-type: disc">There is a relatively large, opaque, optional body</li></ul></div><p>There is a lot of good HTTP response codes documentation on the Internet (and also some hilarious pieces given at <a class="ulink" href="http://httpstatusdogs.com/">http://httpstatusdogs.com/</a>)—the ones that are relevant to our discussion are in the fourth hundred, that is, between 300 and 399.</p><p>Responses with those codes are indications that a browser should immediately make another request instead of the original one. This is why they are called redirects. The semantic differences between various 3xx codes are less important here.</p><p>What is important is that many redirects are superfluous. HTTP clients (for example, browsers) spend time on redirects that serve no particular reason besides cleaning up the URL in the address<a class="indexterm" id="id182"/> bar. Does Yahoo really need to redirect me from <code class="literal">yahoo.de</code> to <code class="literal">ru.yahoo.com</code>, <code class="literal">www.yahoo.com</code>, and <a class="ulink" href="https://www.yahoo.com">https://www.yahoo.com</a> by making my browser issue three additional requests that could easily be avoided? If a website under your control does such things, you may address the question to the respective developers. You may also suggest an easy fix; see later in the text.</p><p>There is a cool, little web service that allows you to see the redirects chain as well as some other<a class="indexterm" id="id183"/> metainformation that may be useful for debugging. It may be referred to at <a class="ulink" href="https://httpstatus.io/">https://httpstatus.io/</a>.</p><p>You may <a class="indexterm" id="id184"/>go and check whether some of your websites make unneeded redirects, which may cost your slow mobile users' precious seconds before they actually get to the content of your site.</p><div class="mediaobject"><img alt="Replacing external redirects with internal ones" src="graphics/B04329_04_02.jpg"/></div><p>Nginx has a feature named "internal redirects". The idea is that all the intermediate HTTP request-response pairs are processed right inside the server. The client gets the content from the end of the chain in response to the original request. There are a number of methods to enable internal redirects in Nginx, but probably the most flexible is the <code class="literal">X-Accel-Redirect</code> response header that an upstream behind Nginx may generate.</p><p>For the internal redirects to work with this method, you will have to change the configuration of your upstream software. Instead of generating true redirects via HTTP 3xx response codes coupled with the <code class="literal">Location:</code> response header, you will have to generate the earlier-mentioned <code class="literal">X-Accel-Redirect:</code> header. This is literally the only change you will have to make. There are a number of places where you need to be careful; all of them concerning the security model of the browsers. The geographic redirects as shown with the Yahoo! example are actually quite rare nowadays, so optimizing them may not be worth the troubles you will get by issuing cookies on the wrong domain. But the <code class="literal">example.com</code> to <code class="literal">www.example.com</code> redirects are still very popular and look like perfect candidates for internal redirects.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec22"/>Summary</h1></div></div></div><p>In this chapter, we discussed several approaches to finding performance problems in your Nginx installation. We mostly focused on working with legacy websites that you might have inherited and are optimizing. The reason for this is that Nginx in itself rarely has any specific problems with being fast enough.</p><p>As an operations specialist, you increased your value for the business by gaining knowledge on how to speed up existing working websites having load and customers but based on some pre-Nginx technologies that were a limiting factor.</p></div></body></html>