<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">NGINX Plus â€“ The Commercial Offering</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following:</p>
<ul>
<li>Installing NGINX Plus</li>
<li>Real-time server activity monitoring</li>
<li>Dynamic config reloading</li>
<li>Session persistence</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>While the open source version of NGINX is the one most people are familiar with, Nginx Inc also produces a paid, commercial variant with a number of additional features aimed at enterprise and large tier deployments. With features such as detailed live monitoring, application health checking for load balancers, and dynamic configuration reloading, there are compelling reasons to consider the Plus version.</p>
<p>While some may find the US $2,500 starting point a steep jump over the open source version, these additional features pit it against commercial systems at over ten times the price. Suffice to say, once you get to the point where these features become paramount to your business, the price is well worth it. You'll also be supporting the continual development of NGINX, which most organizations with the Plus version still run.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing NGINX Plus</h1>
                </header>
            
            <article>
                
<p>Like its open source counterpart, NGINX Plus can be easily installed using the official repositories provided by NGINX.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>If you have NGINX installed already, you'll need to uninstall it first to prevent conflict. As NGINX Plus is a paid product, you'll also require a license key to complete the installation as well. This can be purchased from the NGINX store or you can request a trial license so that you can evaluate the features before buying.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Official repositories exist for most major Linux installations, but we'll focus on just CentOS 7 and Ubuntu 16.04 LTS to cover the two most common scenarios.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CentOS</h1>
                </header>
            
            <article>
                
<ol>
<li>Firstly, we need to create a directory to store the repository certificates in:</li>
</ol>
<pre>      <strong>mkdir -p /etc/ssl/nginx</strong></pre>
<ol start="2">
<li>Next, you will need to download both the private and public certificates from the NGINX customer portal (<a href="https://cs.nginx.com" target="_blank"><span class="URLPACKT">https://cs.nginx.com</span></a>), which should be named:</li>
</ol>
<pre>      <strong>nginx-repo.key</strong>
      <strong>nginx-repo.crt</strong></pre>
<ol start="3">
<li>Copy these files into the directory created. We can then ensure that the root CA certificate bundle has been installed:</li>
</ol>
<pre>      <strong>yum install -y ca-certificates</strong></pre>
<ol start="4">
<li>We can now install the NGINX Plus repository:</li>
</ol>
<pre>      <strong>yum-config-manager --add-repo \<br/>      https://cs.nginx.com/static/files/nginx-plus-7.repo</strong></pre>
<ol start="5">
<li>With the repository keys ready and the repository installed, we can now install NGINX Plus:</li>
</ol>
<pre>      <strong>yum install -y nginx-plus</strong></pre>
<ol start="6">
<li>If you have the correct keys, this will install NGINX Plus. You can confirm the installation is correct by inspecting the version:</li>
</ol>
<pre>      <strong>nginx -v</strong></pre>
<p style="padding-left: 60px">This should give you an output similar to the following:</p>
<pre>      <strong>nginx version: nginx/1.11.10 (nginx-plus-r12-p3)</strong></pre>
<ol start="7">
<li>Lastly, we can start and ensure that NGINX Plus automatically starts after boot:</li>
</ol>
<pre>      <strong>systemctl enable nginx</strong>
      <strong>systemctl start nginx</strong></pre>
<p>The installation of NGINX Plus is now complete.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ubuntu</h1>
                </header>
            
            <article>
                
<ol>
<li>Firstly, we need to create a directory to store the repository certificates in:</li>
</ol>
<pre>      <strong>mkdir -p /etc/ssl/nginx</strong></pre>
<ol start="2">
<li>Next, you will need to download both the private and public certificates from the NGINX customer portal <span class="URLPACKT">(<a href="https://cs.nginx.com" target="_blank">https://cs.nginx.com</a></span>), which should be named:</li>
</ol>
<pre>      <strong>nginx-repo.key</strong>
      <strong>nginx-repo.crt</strong></pre>
<ol start="3">
<li>Copy these files into the directory created and then add the NGINX package signing key in using <kbd>apt-key</kbd>:</li>
</ol>
<pre>      <strong>wget http://nginx.org/keys/nginx_signing.key &amp;&amp; apt-key add \  <br/>      nginx_signing.key</strong></pre>
<ol start="4">
<li>To ensure we have the required prerequisites, we can install these packages with the following command:</li>
</ol>
<pre>      <strong>apt install -y software-properties-common apt-transport-https \<br/>      lsb-release ca-certificates</strong></pre>
<ol start="5">
<li>We can then add the NGINX Plus repository via the <kbd>add-apt-repository</kbd> command:</li>
</ol>
<pre>      <strong>add-apt-repository -y \<br/>      "deb https://plus-pkgs.nginx.com/ubuntu $(lsb_release -sc) main"</strong></pre>
<ol start="6">
<li>Before we can install the NGINX Plus packages, we first need to link the secured repositories to the key we previously downloaded. NGINX provides a preconfigured file to do this and we can download it to the correct location:</li>
</ol>
<pre>      <strong>wget -q -O /etc/apt/apt.conf.d/90nginx \<br/>      https://cs.nginx.com/static/files/90nginx</strong></pre>
<ol start="7">
<li>With the repository set up and prepared, we can now update the package information and then install NGINX Plus:</li>
</ol>
<pre>      <strong>apt update</strong>
      <strong>apt install nginx-plus</strong></pre>
<ol start="8">
<li>If you have the correct keys, this will install NGINX Plus. You can confirm the installation is correct by inspecting the version:</li>
</ol>
<pre>      <strong>nginx -v</strong></pre>
<p style="padding-left: 60px">This should give you an output similar to the following:</p>
<pre>      <strong>nginx version: nginx/1.11.10 (nginx-plus-r12-p3)</strong></pre>
<ol start="9">
<li>Lastly, we can start and ensure NGINX Plus automatically starts after boot:</li>
</ol>
<pre>      <strong>systemctl enable nginx</strong>
      <strong>systemctl start nginx</strong></pre>
<p>The installation of NGINX Plus is now complete.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The NGINX Plus product page: <a href="https://www.nginx.com/products/" target="_blank"><span class="URLPACKT">https://www.nginx.com/products/</span></a></li>
<li>The NGINX Plus installation guide: <a href="https://www.nginx.com/resources/admin-guide/installing-nginx-plus/" target="_blank"><span class="URLPACKT">https://www.nginx.com/resources/admin-guide/installing-nginx-plus/</span></a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Real-time server activity monitoring</h1>
                </header>
            
            <article>
                
<p>We covered some of the basics of monitoring your server all the way back in <a href="69685f00-24c3-428c-b607-01a4e9a2784d.xhtml">Chapter 1</a>, <em>Let's Get Started</em>, which used the <kbd>ngxtop</kbd> utility to provide basic command line driven monitoring and statistics. Included in NGINX Plus is a powerful and comprehensive metrics system to show you real-time activity for your NGINX server.</p>
<p>As you'll see in the upcoming screenshots, this is quite a comprehensive system. Not only does it include a web interface so that you can see the stats, but this data is also available as a JSON feed so that it can be directly imported by external monitoring tools.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>As this is part of the NGINX Plus packages, you'll need to ensure you have NGINX Plus installed and configured correctly. You can test to ensure you have NGINX Plus by running <kbd>nginx -v</kbd>. This should give you an output similar to the following:</p>
<pre><strong>nginx version: nginx/1.11.10 (nginx-plus-r12-p3)</strong>  </pre>
<p>The important part to check is the <em>plus</em> in the naming for <kbd>nginx-plus-r12-p3</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>In order to use the status module, we need to include it as part of the <kbd>server</kbd> block directive. You can choose to run in on a different port from your main web server and also restrict via IP, which is what we'll do in our recipe. Here's our configuration:</li>
</ol>
<pre>      server { 
          listen 8188; 
          status_zone status-page; 
 
          allow 192.168.0.0/24; 
          deny all; 
 
          root /usr/share/nginx/html; 
          location = /status.html { } 
          location = / { 
              return 301 /status.html; 
          } 
          location /status { 
              status; 
              status_format json; 
          } 
      } </pre>
<p style="padding-left: 60px">This can be placed in the <kbd>/etc/nginx/conf.d/</kbd> directory as a separate file (for example, <kbd>status.conf</kbd>), to keep it clean and simple.</p>
<ol start="2">
<li>If you open up a browser and go to <kbd>http://&lt;serveripaddress&gt;:8188/status.html</kbd>, you should see a page similar to the following screenshot:</li>
</ol>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="383" src="assets/abd0b9e7-1040-4b75-a169-e0a85a26a3a0.png" width="659"/></div>
<p style="padding-left: 60px">This page is just an HTML page, but uses the JSON data to update the status (every second by default). We can see all of the vital server statistics, such as the connections, requests, uptime, and traffic.</p>
<p style="padding-left: 60px">If you have multiple server zones configured, you can view the stats individually to see the various statistics of your subsystems:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="223" src="assets/a6bdcb96-2842-4ae9-9ae6-6d785fbcff99.png" width="710"/></div>
<p style="padding-left: 60px">Likewise, we can see the stats associated with upstream servers if you're using NGINX Plus in a load balancer configuration:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="294" src="assets/8770adbd-37ec-4774-9e5d-47e3bc2e38f0.png" width="685"/></div>
<p style="padding-left: 60px">These insights allow you to precisely see what's going on with all aspects of your server in real-time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>We use the <kbd>listen</kbd> directive to tell NGINX Plus to listen on a different port for this <kbd>server</kbd> block, so that we can isolate it from the main connections. In this instance, we have it listening on port <kbd>8188</kbd>.</p>
<p>Next, we set <kbd>status_zone</kbd>, which needs to be defined within each <kbd>server</kbd> or <kbd>http</kbd> block directive in order to collect statistics. For this recipe, we simply have one zone within our statistics block directive called <kbd>status-page</kbd>. For other <kbd>server</kbd> block directives, you can either combine into one (for example, <kbd>backend-zone</kbd>) or track individually if you require unique statistics per directive.</p>
<p>In order to serve the static <kbd>status.html</kbd> file, we define the root path where the files are located. Then, we ensure any root calls (for example, without a trailing <kbd>/status.html</kbd>) are redirected.</p>
<p>Lastly, we set the <kbd>/status</kbd> location to serve the actual statistical data. We set this to be JSON format, which means it can be easily ingested into many other systems. It's what powers the HTML-based dashboard as well. This is required in order to display the statistics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>The HTML dashboard is, of course, only one way to use the stats. Because we have direct access to the JSON data, we can pull this data from a third-party monitoring system. For instance, we can view what information is available by calling:</p>
<pre><strong>curl http://202.74.71.216:8188/status</strong>  </pre>
<p>This will produce a JSON based output, similar to the following:</p>
<pre>{"version":8,"nginx_version":"1.11.10","nginx_build":"nginx-plus-r12-p3", "address":"202.74.71.216", "generation":1,"load_timestamp":1503127483754, "timestamp":1503149700611, "pid":10008,"ppid":10007, "processes":{"respawned":0}, "connections":{"accepted":945523, "dropped":0,"active":1, "idle":0}, "ssl":{"handshakes":0, "handshakes_failed":0, "session_reuses":0}, "requests":{"total":5249054, "current":1}, "server_zones":{"status-page":{"processing":1, "requests":5249049, "responses":{"1xx":0, "2xx":4903722, "3xx":345306, "4xx":20, "5xx":0,"total":5249048}, "discarded":0, "received":566205691, "sent":16475272258}}, "slabs":{}, "upstreams":{}, "caches":{}} 
d </pre>
<p>You can also reduce the output to the specific component you're after, for example, we can call <kbd>/status/connections</kbd> to retrieve just the statistics about the connections:</p>
<pre>{"accepted":945526,"dropped":0,"active":1,"idle":0} </pre>
<div class="packt_tip">Don't forget to adjust your accept/allow and/or authentication for third-party monitoring systems.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>The NGINX Plus status module documentation is available at: <a href="https://nginx.org/en/docs/http/ngx_http_status_module.html" target="_blank"><span class="URLPACKT">http</span><span class="URLPACKT">s://nginx.org/en/docs/http/ngx_http_status_module.html</span></a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dynamic config reloading</h1>
                </header>
            
            <article>
                
<p>In large, high-scale systems where it's critical to minimize downtime, changes requiring a restart of services require careful planning. If you have a <strong>Software as a Service</strong> (<strong>SaaS</strong>) style platform, having a delay before deploying changes or new customer signups could be quite detrimental to your operations.</p>
<p>System downtime can also be required in load balancing situations, where you need to add and remove application backends on the fly. Thankfully, NGINX Plus allows for the configuration to be reloaded without having to restart the services. In this recipe, we'll go through how to update your configuration with a dynamic reload.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We're using a basic recipe based on our load balancing recipe back in <a href="4709da7c-9dbd-49d8-8fa7-c3fb6a9cdd6a.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 8</span>,</a> <em>Load Balancing</em>. With the open source version of NGINX, adding or removing backend servers from an NGINX configuration required a full reload in order to use the new changes. While this only has a minimal impact on lightly loaded systems, this can be a significant problem when it comes to highly loaded systems. As it has to wait for all NGINX worker processes to finish before reloading, there's a period where the system won't be processing at the same capacity as normal.</p>
<p>Here's how our scenario will look:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="431" src="assets/08676d57-deaa-476d-be03-557852dca58c.png" width="440"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>In order to allow the dynamic reload, we'll configure our <kbd>upstream</kbd> block directive, as well as two <kbd>server</kbd> block directives. Like most of the NGINX configuration, we can place this in a standalone file (for example, <kbd>loadbalancer.conf</kbd>) within the <kbd>/etc/nginx/conf.d</kbd> directory. Here's the configuration:</li>
</ol>
<pre>      upstream localapp { 
          zone backend 64k; 
          server 127.0.0.1:8080; 
          server 127.0.0.1:8081; 
          server 127.0.0.1:8082; 
      } 
 
      server { 
          listen       80; 
          server_name  dynamicload.nginxcookbook.com; 
          access_log  /var/log/nginx/dynamicload-access.log  combined; 
          location / { 
              proxy_pass http://localapp; 
          } 
      } 
 
      server { 
          listen 127.0.0.1:8189; 
 
          location /upstream_conf { 
              upstream_conf; 
          } 
      } </pre>
<p style="padding-left: 60px">You can also use the HTest tool for ease of testing if this is outside of a production environment to simulate the backend servers.</p>
<ol start="2">
<li>To make changes to this configuration on the fly, we can invoke an API call from the command line. Firstly, let's get a copy of the existing configuration:</li>
</ol>
<pre>      <strong>curl 'http://localhost:8189/upstream_conf?upstream=localapp'</strong></pre>
<p style="padding-left: 60px">This should print a list of servers out, which should match the configuration we've already loaded:</p>
<pre>      <strong>server 127.0.0.1:8080; # id=0</strong>
      <strong>server 127.0.0.1:8081; # id=1</strong>
      <strong>server 127.0.0.1:8082; # id=2</strong></pre>
<ol start="3">
<li>If we want to add another backend, we can send an add command via <kbd>curl</kbd>:</li>
</ol>
<pre>      <strong>curl 'http://localhost:8189/upstream_conf?add=&amp;upstream=localapp&amp;server=127.0.0.1:8083'</strong></pre>
<p style="padding-left: 60px">This should echo out the new server added as confirmation:</p>
<pre>      <strong>server 127.0.0.1:8083; # id=3</strong></pre>
<p style="padding-left: 60px">The new server should be available immediately and will now be part of the round-robin assignment.</p>
<ol start="4">
<li>In a similar fashion, we can also remove backend servers on the fly. To remove a server, we use the assigned ID to remove it:</li>
</ol>
<pre>      <strong>curl 'http://localhost:8189/upstream_conf?remove=&amp;upstream=localapp&amp;id=1'</strong></pre>
<p style="padding-left: 60px">This will echo back the remaining servers configured, which for our example will be:</p>
<pre>      <strong>server 127.0.0.1:8080; # id=0</strong>
      <strong>server 127.0.0.1:8082; # id=2</strong>
      <strong>server 127.0.0.1:8083; # id=3</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Similar to a standard NGINX load balancer configuration, we first define an <kbd>upstream</kbd> block directive. This defines a memory allocation to store the configuration so that it can be updated on the fly.</p>
<p>Next, we define our standard <kbd>server</kbd> block directive. This is as per a standard configuration, which simply proxies the connections to the upstream servers.</p>
<p>Lastly, we then define a separate block directive to handle the <kbd>upstream_conf</kbd> module. We use the separate <kbd>server</kbd> block directive so that we can bind it to a specific port on the localhost and prevent accidental exposure to the internet.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>While our example recipe allows for changes to be made on the fly, they aren't persistent if we reboot NGINX Plus. To ensure these changes are maintained between reboots, we can add a <kbd>state</kbd> file to manage the current configuration. This is done via updating the <kbd>upstream</kbd> block directive. Here's what our updated configuration looks like:</p>
<pre>upstream localapp { 
    zone backend 64k; 
    state /var/lib/nginx/state/servers.state; 
} </pre>
<p>We add the <kbd>state</kbd> directive, which is updated when there are any changes to the upstream configuration as well as being reread when NGINX restarts. All other server directives within that upstream must also be removed, as you can't combine static server configuration with a dynamic, stateful configuration. The state file itself is plain text (and uses the standard NGINX server directive format) but any direct manipulation of the file is strongly discouraged.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>The NGINX Plus <kbd>http_upstream_conf</kbd> module documentation is available at: <a href="https://nginx.org/en/docs/http/ngx_http_upstream_conf_module.html" target="_blank"><span class="URLPACKT">https://nginx.org/en/docs/http/ngx_http_upstream_conf_module.html</span></a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Session persistence</h1>
                </header>
            
            <article>
                
<p>If you have a scenario where you have a load balancer with multiple backend servers (as we covered back in <a href="4709da7c-9dbd-49d8-8fa7-c3fb6a9cdd6a.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Load Balancing</em>), there can be some tricky scenarios where session tracking would be difficult to implement. While using the hash-based algorithm can ensure requests from the same IP is routed to the same backend, this doesn't always ensure a balanced distribution of requests.</p>
<p>One of the key features for NGINX Plus is session persistence, where requests from the same client need to be sent to the same server for the life of that session. Also known as "sticky" sessions, this can be especially important when it comes to payment systems, where the sharing of information between backend servers can be quite restrictive.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We'll reuse our simple, round-robin load balancing scenario and incorporate the various session persistence options available in NGINX Plus.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>There are three different methods to ensuring sticky sessions, each with their various pros and cons. Here's a quick summary:</p>
<ul>
<li><kbd>cookie</kbd>: This method uses a cookie on the first request to store the tracking information in order to ensure subsequent requests are routed to the same backend server. As it means modifying the headers, it may not be compatible with all systems.</li>
<li><kbd>learn</kbd>: This is a stateful method, which relies on existing data within existing response headers to determine a unique identifier in which to track the requests. For example, most web frameworks have their own session ID, which can be leveraged for tracking. This means that no data modification is required.</li>
<li><kbd>route</kbd>: Lastly, we can route the request based on variables to explicitly choose the upstream server to use. While similar to the cookie method (route also uses a cookie to help track), the explicit choice of server can be beneficial when you have reasons to push to different clients to different servers. This could be used a "feature flag" method of routing clients to newer servers with differing features if they match the specific variables.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cookie-based tracking</h1>
                </header>
            
            <article>
                
<ol>
<li>To configure using cookie-based sticky sessions, we can use the following configuration:</li>
</ol>
<pre>      upstream stickyapp { 
          server 127.0.0.1:8080; 
          server 127.0.0.1:8081; 
          server 127.0.0.1:8082; 
          sticky cookie cookbook expires=1h; 
      } 
 
      server { 
          listen       80; 
          server_name  session.nginxcookbook.com; 
          access_log  /var/log/nginx/sessiontest-access.log  combined; 
          location / { 
              proxy_pass http://stickyapp; 
          } 
      }</pre>
<ol start="2">
<li>Upon the first request, NGINX will set a cookie to track all subsequent sessions. You can confirm the cookie by viewing received HTTP headers using a tool such as HTTPie:</li>
</ol>
<pre>      <strong>http http://session.nginxcookbook.com/</strong></pre>
<p style="padding-left: 60px">This output should show the server headers, which will contain a <kbd>Set-Cookie</kbd> header:</p>
<pre>      <strong>HTTP/1.1 200 OK</strong>
      <strong>Connection: keep-alive</strong>
      <strong>Content-Length: 613</strong>
      <strong>Content-Type: text/html; charset=utf-8</strong>
      <strong>Date: Mon, 18 Aug 2017 15:39:37 GMT</strong>
      <strong>Server: nginx/1.11.10</strong>
      <strong>Set-Cookie: cookbook=df2b80dc43705d28db9be6f29fe58da3; expires=Mon,<br/>      18-Aug-17 16:39:37 GMT</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learn-based tracking</h1>
                </header>
            
            <article>
                
<ol>
<li>If you can leverage an existing, unique identifier from your application to use for session tracking, you can avoid having to create any further cookies. Many common web frameworks set the session as a cookie for tracking, so as long as you know the name of the cookie then it can be used. Here's what our recipe looks like:</li>
</ol>
<pre>      upstream stickyapp { 
          server 127.0.0.1:8080; 
          server 127.0.0.1:8081; 
          server 127.0.0.1:8082; 
          sticky learn create=$upstream_cookie_sessionid <br/>          lookup=$cookie_sessionid zone=client_sessions:1m; 
      } 
 
      server { 
          listen       80; 
          server_name  session.nginxcookbook.com; 
          access_log  /var/log/nginx/sessiontest-access.log  combined; 
          location / { 
              proxy_pass http://stickyapp; 
          } 
      }</pre>
<ol start="2">
<li>To test, you'll need to ensure that your application is sending a cookie set as <kbd>sessionid</kbd>.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Route-based tracking</h1>
                </header>
            
            <article>
                
<ol>
<li>The last method of creating and ensuring consistent sessions is via routes. In order to do this, we also need to tell NGINX Plus how to interpret the routes. Here's our configuration:</li>
</ol>
<pre>      map $cookie_route $route_cookie { 
          ~(?P&lt;route&gt;\w+)$ $route; 
      } 
 
      map $arg_route $route_uri { 
          ~ (?P&lt;route&gt;\w+)$ $route; 
      } 
 
      upstream stickyapp { 
          server 127.0.0.1:8080 route=server1; 
          server 127.0.0.1:8081 route=server2; 
          server 127.0.0.1:8082 route=server3; 
          sticky route $route_cookie $route_uri; 
      } 
 
      server { 
          listen       80; 
          server_name  session.nginxcookbook.com; 
          access_log  /var/log/nginx/sessiontest-access.log  combined; 
          location / { 
              proxy_pass http://stickyapp; 
          } 
          status_zone sticky-app; 
      } </pre>
<ol start="2">
<li>For this recipe, we're reading a cookie named <kbd>route</kbd> and then using this to determine which server to persistently send the request to. To test, we can send a test route cookie to ensure it's working:</li>
</ol>
<pre>      <strong>curl --cookie 'route=server2' http://session.nginxcookbook.com/</strong></pre>
<p style="padding-left: 60px">Each request should be served from the upstream server tagged as <kbd>server2</kbd>, which you can verify with the HTest utility or the logs of your backend server.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>For our cookie-based session tracking, we set a cookie named <kbd>cookbook</kbd> to use as the tracking for session persistence. This is also set with an expiry of one hour. It's also possible to explicitly set the domain and path as well if greater restrictions are required.</p>
<p>Our learn-based tracking has three variables set. The first, create is used to track from the <kbd>upsteam</kbd> server, which we look for the cookie set header (<kbd>Set-Cookie</kbd>) using a naming pattern <kbd>$upstream_cookie_&lt;cookiename&gt;</kbd>. For our recipe, <kbd>$upstream_cookie_sessionid</kbd> means we match the <kbd>sessionid</kbd>.</p>
<p>Next, we use the <kbd>lookup</kbd> variable to designate what to track from the client. This uses a similar tracking method to the create command. This recipe uses the <kbd>$cookie_sessionid</kbd> pattern, which means it will match the contents of a cookie named <kbd>sessionid</kbd>.</p>
<p>Lastly, as this is stateful, we need to allocate memory in which to store the lookup table. This is done via the <kbd>zone</kbd> variable. For this recipe we have named the zone <kbd>client_sessions</kbd> and allocated 1 megabyte of memory. This is sufficient to store around 8,000 sessions. By default, these sessions are only persistent for 10 minutes. So, if you have a higher number of users per 10 minutes or require a longer timeout, you may need to increase the memory allocated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>The NGINX Plus <kbd>http_upstream_conf</kbd> module documentation is available at: <a href="https://nginx.org/en/docs/http/ngx_http_upstream_conf_module.html#sticky" target="_blank"><span class="URLPACKT">https://nginx.org/en/docs/http/ngx_http_upstream_conf_module.html#sticky</span></a></p>


            </article>

            
        </section>
    </body></html>