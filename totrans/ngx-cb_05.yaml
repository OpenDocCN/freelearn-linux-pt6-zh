- en: Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Logging to syslog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing web access logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtual host log format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application focused logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging TLS mode and cipher information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging POST data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conditional logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Elastic Stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Akin to metrics, logging is key to monitoring how your server is performing
    and debugging errors. Thankfully, NGINX has an extensive logging module built-in
    to cover nearly all usage scenarios. In this chapter, we'll go through some of
    the various ways you can change the standard logging formats, as well as how to
    process them in more efficient manners.
  prefs: []
  type: TYPE_NORMAL
- en: Logging to syslog
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you already have your centralized server logs or your logs are analyzed by
    a standard syslog system, you can also redirect your NGINX logs to do the same.
    This is useful when using external platforms such as Loggly and Papertrail, which
    integrate via syslog.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Firstly, we need to consider where we''re pushing the logs to, syslog can be
    both local and network-based, so we''ll cover both ways. For nearly all Linux
    distributions, the default syslog service is `rsyslog` which will be listening
    on the Unix socket located at `/dev/log`. Here''s our NGINX configuration for
    local logging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The handling of the logs will now be done by `rsyslog`. By default, this will
    match the wildcard rule and write the log entries to `/var/log/syslog` for Debian-
    / Ubuntu-based systems and `/var/log/messages` for CentOS- / RedHat-based distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can confirm it''s working as expected by viewing the last line in our `syslog`
    after accessing the website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Remote syslog
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we can generate a bit more flexibility using a local syslog daemon, generally,
    the reason we want to offload the logging is to either decrease the load on our
    production systems or to use external tools for better log analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to do this is to send the logs to a syslog daemon or processing
    system that doesn't exist on the same server. This can also be used to aggregate
    logging from multiple servers, which facilitates monitoring and reporting in a
    central location.
  prefs: []
  type: TYPE_NORMAL
- en: 'To send the logs to a remote server, firstly, we need to enable the syslog
    server to listen on a network port. This is disabled on most `rsyslog`-based systems,
    so that it prevents accidental security issues. To enable, we simply uncomment
    the following in the `rsyslog.conf` (generally `/etc/rsyslog.conf`) file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After the syslog daemon is restarted (`systemctl restart rsyslog`), it will
    now be listening on port `514` for UDP log packets.
  prefs: []
  type: TYPE_NORMAL
- en: By default, syslog data is decrypted. We highly recommend that you ensure that
    the logs are sent via a VPN tunnel or similar encrypted means.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the NGINX server, we now need to update the `server` block for the access
    and debug logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After reloading NGINX to apply the new rules, we can now verify that the logs
    are hitting the syslog on the remote server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: On a Debian- / Ubuntu-based system, this will be `/var/log/syslog`. If you run
    a RedHat- / CentOS-based system, this will be logged to `/var/log/messages`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NGINX reference: [https://nginx.org/en/docs/syslog.html](https://nginx.org/en/docs/syslog.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Customizing web access logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of ways you can customize the log files, including the format,
    the information provided, and where to save them to. This customization can be
    especially handy if you're using NGINX as a proxy in front of application servers,
    where traditional web style logs may not be as useful.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a few different ways we can customize the access logs in order to
    get more relevant information or reduce the amount of logging where it''s not
    required. The standard configuration of `combined` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This format is already predefined in NGINX out of the box, which is compatible
    with the Apache combined format. What this means is that, by default, combined
    formatted logs will be compatible with most log parsers and, will be therefore,
    able to directly interpret the log data.
  prefs: []
  type: TYPE_NORMAL
- en: While having more data can be quite helpful (as we'll show later), be careful
    about deviating from a standard format. If you want to use a third-party system
    to parse the logs, you'll also need to update or customize the parser as well.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The NGINX log module documentation: [http://nginx.org/en/docs/http/ngx_http_log_module.html](http://nginx.org/en/docs/http/ngx_http_log_module.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The NGINX core variables: [http://nginx.org/en/docs/http/ngx_http_core_module.html#variables](http://nginx.org/en/docs/http/ngx_http_core_module.html#variables)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtual host log format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you're running a virtual host style environment (with multiple `server` blocks)
    with NGINX, there's one small tweak you can make to enhance the logs. By default,
    the host (defined as `$host`) isn't logged when using the default combined format.
    Having this field in the logs means that the log can be parsed externally without
    the need for additional information.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start with, we need to define our new log format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `log_format` directive needs to be outside the `server` block.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use our new log file, we update the access log format in the `server` block
    with the new format. Here''s an example with our MediaWiki recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If you had a wildcard setup for your subdomains, this would mean that you'd
    also have the correct hostname in the logs as well. Without this, the logs wouldn't
    be able to differentiate between any of the subdomains for statistical analysis
    and comparison.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the updated format, here''s an example from our logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can now see the hostname clearly in the logs.
  prefs: []
  type: TYPE_NORMAL
- en: Application focused logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we're going to customize the logs to give us a bit more information
    when it comes to applications and metrics. Additional information, such as the
    response time, can be immensely useful for measuring the responsiveness of your
    application. While it can generally be generated within your application stack,
    it can also induce some overhead or give incomplete results.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start with, we need to define our new log format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `log_format` directive needs to be outside the `server` block.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we've deviated from the combined format to change the time to the ISO
    8601 format (which will look something like `2016-07-16T21:48:36+00:00`), removed
    the HTTP referrer and user agent, but added the request processing time (`$request_time`)
    to get a better idea on how much time it's taking our application to generate
    responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our new log format defined, we can now use this for our application logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re using the default Meteor application which we had set up in [Chapter
    3](db163fa8-2a5d-40bc-b83e-61e72ec67237.xhtml), *Common Frameworks* and as we
    want applications to be very responsive, `$request_time` will provide instant
    feedback as to what calls aren''t quick enough. Here''s an example output from
    the logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As the request times are measured in milliseconds, we can see that our base
    call took 0.005 ms to complete and the WebSocket connection took 14 ms to complete.
    This additional information can now be easily searched for and, with additional
    log parsers (like the Elastic Stack detailed later in this chapter), we can set
    further searches and alerts.
  prefs: []
  type: TYPE_NORMAL
- en: Logging TLS mode and cipher information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the advent of HTTP/2 and the ever changing cryptography best practices,
    small compatibility issues can arise, which are very difficult to resolve. Browsers
    also change what they accept on a constant basis. To ensure, we know exactly what
    ciphers have been used with what protocol, we can add this additional information
    to our log files.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here''s our SSL enhanced log format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This is based on the common format and we''ve added `$ssl_protocol` and `$ssl_cipher`
    to the end. Here''s what we now have in our logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the log details, we can now see the requests from Chrome, Firefox, Safari,
    and even cURL are using `TLSv1.2` and `ECDHE-RSA-AES128-GCM-SHA256`. If we saw
    abnormal behavior with a different browser or protocol, this would greatly assist
    in the diagnosis of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Logging POST data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we have form submissions, errors in this become difficult to replay and
    debug if we don't know the value of the data. By default, NGINX doesn't log POST
    data, as it can be very bulky. However, there are certain situations where getting
    access to this data is vital to debugging issues.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to log POST data, we need to define a custom log format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: By logging `$request_body`, we'll be able to see the contents of a POST submission.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable our POST log format, all we need to do is specify the log format
    for the access logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If you just want to verify that it''s working as expected, we can use cURL
    to send post data. I''ve used Express, as set up in [Chapter 3](db163fa8-2a5d-40bc-b83e-61e72ec67237.xhtml),
    *Common Frameworks*, to accept the POST data, so we can make a call as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This posts the variables name, phone, and email through to the root (`/`) URL.
    If we look at `postdata-access.log`, we can now see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: If you still had the standard log format enabled, it would simply display a
    dash (`-`) for the request, making further debugging impossible if it wasn't captured
    by the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the corresponding log entry in a standard combined format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Of course, by logging all of the POST data, you'll use considerably more disk
    space and server overhead. We highly recommend that this is only enabled for development
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are some instances where we may want to log different data, based on some
    sort of conditional argument. For instance, if you have a beta copy of your application
    and are interested in gathering more information for it, we can log this data
    just for the beta URL. Rather than clogging up our log files with the additional
    information when it's not required, we can simply trigger it when required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To enable, we map the (`$args`) URI arguments to a `$debuglogs` variable, based
    on if a condition is set. Here''s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Like the custom log formats, this needs to be placed outside of the `server`
    directives. Then, within the `server` directive, we can make use of the variable
    and create additional logging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If we call a normal URL, it will simply log the access to the standard `djangodemo-access.log`
    in the combined format. If we call a URL with the `debug` argument set (for example:
    `http://djangodemo.nginxcookbook.com/?debug`), we now get the details logged in
    both the standard logs as well as our additional `djangodemo-debug-access.log`
    using the `applogs` format we defined in the previous recipe.'
  prefs: []
  type: TYPE_NORMAL
- en: Using the Elastic Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/a9286138-8e1e-4665-9227-9ed93b6e21c6.png)'
  prefs: []
  type: TYPE_IMG
- en: Manually digging through log files to gain insights or to detect anomalies can
    be very slow and time-consuming. To solve this issue, we're going to run through
    a quick example using the Elastic Stack (previously referred to as the **Elasticsearch
    LogstashÂ Kibana** (**ELK**) stack. Elasticsearch is a high-speed search engine
    which offers real-time indexing and searching. Data is stored as schema-less JSON
    documents and has an easy-to-use API for access.
  prefs: []
  type: TYPE_NORMAL
- en: To complement this, there's also Logstash and Kibana. Logstash is a tool that
    allows for the collection of logs, parses the data on the fly, and then pushes
    it to a storage backend such as Elasticsearch. The original creator of Logstash
    (Jordan Sissel) wrote it to parse web server log files such as those produced
    by NGINX, so it's well suited to the task.
  prefs: []
  type: TYPE_NORMAL
- en: Kibana is then the final part of the puzzle. Once the data has been collected,
    parsed, and stored, we now need to be able to read it and view it. Kibana allows
    you to easily visualize the data by allowing you to easily structure the filters
    and queries and then display the information in easy-to-read graphs and tables.
    As a graphical representation, it is far quicker and easier to interpret and see
    variances in data which may be meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: What's more, the whole ELK stack is open source and therefore freely available
    to use and extend. In fact, the stack itself has expanded with tools such as Beats
    to provide lightweight data shipping to be processed by Logstash. Once you start
    using it, it's hard to revert to flat files!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To run the Elastic Stack, you can either install it on your own systems or use
    a cloud-based instance. For this recipe, we're going to install it on a separate
    server.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To install Elastic, simply download the appropriate `.deb` or `.rpm` package
    for your particular instance. For the example, I''ll use Ubuntu 16.04\. First,
    we''ll need to install Java version 8 (required for Elastic 5.x):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll download the latest Elastic package and install it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The example is also using the 5.x release, which was in alpha during writing.
    Make sure you select the latest stable release for production use.
  prefs: []
  type: TYPE_NORMAL
- en: 'While Elasticsearch can become quite a complex system as you start to use it
    heavily, the good news is that the default configuration doesn''t require any
    changes to get started. However, we highly recommend ensuring that you only run
    Elasticsearch on a private network (where possible) to avoid accidentally exposing
    your server to the internet. Here''s what our scenario will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a919d615-cf1b-4c4f-9a44-ae0e07842317.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In our example, we have a private network on the `192.168.50.x` range for all
    systems, with our Elasticsearch server being `192.168.50.5`. Accordingly, we simply
    edit the configuration file (`/etc/elasticsearch/elasticsearch.yml`) and set the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We also set the minimum number of master nodes to `1`, so that it knows to wait
    for other master nodes. In a clustered system, you ideally want three or more
    nodes to form a quorum to ensure the data remains valid.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, all we need to do is start it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now test it quickly with a simple cURL call to ensure it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Logstash
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With Elasticsearch set up, we can now install and configure Logstash. As Logstash
    will be performing the log collection role, it needs to be installed on the same
    server as NGINX. For larger installations, you could use Filebeat (also part of
    the Elastic family) to act as a lightweight log forwarder and have Logstash on
    its own instance to parse the logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like Elasticsearch, we need Java 8 or higher to run Logstash. As this demo
    recipe is also using Ubuntu 16.04, we can install it via the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we download the latest copy of Logstash and install it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the installation is complete, we''ll now need to create the configuration
    file. This file will define where to look for the logs, how to parse them, and
    then where to send them to. While the complexity can be a bit daunting at first,
    it''s mostly a set and forget scenario once it''s working. Here''s the configuration
    which will work for most of the configurations already outlined in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: For most installations, this needs to be placed in the `/etc/logstash/conf.d/`
    folder with a `.conf` filename (for example, `nginx.conf`).
  prefs: []
  type: TYPE_NORMAL
- en: In the `input` section, we use the `file` plugin to monitor all the logs which
    have the naming pattern `*-access.log`. The `type` definition simply allows for
    easy filtering if your Elasticsearch server has logs from more than one source.
  prefs: []
  type: TYPE_NORMAL
- en: Then, in the `filter` section, we use the `grok` plugin to turn plain text data
    into a structured format. If you haven't used grok patterns before, they're similar
    to regular expressions and can look quite complex to start with. Because we have
    NGINX using the combined log format (which has already been defined as a grok
    pattern), the hard work has already been done for us.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the `output` section defines where we're sending the data. Logstash
    can send to multiple sources (about 30 different ones), but, in this instance,
    we're simply sending it to our Elasticsearch server. The hosts setting can take
    an array of Elasticsearch servers, so that in a big scenario you can load balance
    the push of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start Logstash, we can simply call the standard init system, which for our
    example is systemd:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'If we now load a page on any of the monitored sites, you should now have data
    within Elasticsearch. We can run a simple test by querying the `logstash` index
    and returning a record. To do this from the command line, run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'I pipe the cURL command through `python` to quickly format the code; by default,
    it returns it in a compressed format without whitespaces. While that cuts down
    on the packet size, it also makes it harder to read. Here''s what the output should
    look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0da4ab23-7384-4d8d-b5ee-67d1bd87b019.png)'
  prefs: []
  type: TYPE_IMG
- en: While the exact syntax won't make a lot of sense yet, the important bit is to
    note that our single log line has been parsed into separate fields. The power
    of this will become evident, once we completed the installation of Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last part of our stack is Kibana, which visualizes the Elasticsearch data.
    To install, we''ll simply download and install the single package. As per our
    initial diagram, we''re installing this on our Elasticsearch server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As we have Elasticsearch bound to the local IP (`192.168.50.5`), we need to
    edit the Kibana configuration (`/etc/kibana/kibana.yml`) and set it to look for
    this address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Kibana is open to the world by default; ideally, we won't leave it in this state.
    Of course, one of the easiest ways to secure Kibana is by proxying it through
    NGINX and we'll cover this scenario in a later chapter. For now, ensure that you
    don't accidentally expose it to the world.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start the Kibana service and get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Kibana will now be accessible via a web interface on port 5601; you should
    see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06a74ceb-c877-410e-9bef-fd8e6bf8ef71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This will be a very quick crash course in the usage of Kibana, which can be
    complex enough to warrant a whole book in itself. To be able to visualize the
    data, we first need to load an index pattern into Kibana. If you click on the
    Discover icon (the very first at the left), it will prompt you to do so. This
    screen should look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7bcfcf40-3dee-4d0d-a14a-231d18e58bc9.png)'
  prefs: []
  type: TYPE_IMG
- en: As our configuration is very simple, we can simply accept the default settings
    and hit Create. You should then see a list of fields and field types; this means
    that Kibana now knows how to sort, filter, and search for these.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the Discover icon again and it should show you the access logs in
    a combined bar graph and tabular format. If you don''t see anything, check the
    time period you''re searching for, which will be set in the top right. Here''s
    what my test data looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50bc5998-7b7a-4d1c-8a4c-010abd65df1d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This isn''t overly useful just yet, but at least we can see a quick historical
    view based on the hit times. To show other fields in the table, we can hover over
    them on the left-hand column and it should show the add button. I''ve added in
    the request, response, bytes, and httpversion to give a more detailed view like
    this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/946e6db4-d66b-4ba7-8677-772b680a7cb3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can start to quickly glance at the data and see if there are errors or anomalies.
    If we wanted to see a quick breakdown of what the requests have been, we could
    click on the field in the left-hand column and it would display the top five results
    within the search. For example, here''s our demo output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4697ad9-c6cf-44b9-a38a-50e744e04095.png)'
  prefs: []
  type: TYPE_IMG
- en: At a quick glance, we know what the most popular requests have been. We can
    also filter based on these values, by clicking on the magnifying glasses at the
    right of the dialog. The positive icon will ensure only those which match this
    request are shown and the negative icon will exclude anything which matches this
    request.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is just the beginning too, as you''ll see on the left-hand side, there''s
    the ability to create visualizations, which can then be placed in a full dashboard.
    You can add any number of line graphs, bar graphs, and similar items based on
    predetermined filters and searches. Of course, you can have multiple dashboards,
    so that you can customize them for multiple different scenarios. As one example,
    here''s a quick dashboard which was created in about five minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/abcb08f0-871a-4d88-8a52-2c778091c21c.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Elastic Stack documentation: [https://www.elastic.co/guide/index.html](https://www.elastic.co/guide/index.html)'
  prefs: []
  type: TYPE_NORMAL
