<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Reverse Proxy Advanced Topics"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Reverse Proxy Advanced Topics</h1></div></div></div><p>As we saw in the previous chapter, a reverse proxy makes connections to upstream servers on behalf of clients. These upstream servers therefore have no direct connection to the client. This is for several different reasons, such as security, scalability, and performance.</p><p>A reverse proxy server aids security because if an attacker were to try to get onto the upstream server directly, he would have to first find a way to get onto the reverse proxy. Connections to the client can be encrypted by running them over HTTPS. These SSL connections may be terminated on the reverse proxy, when the upstream server cannot or should not provide this functionality itself. NGINX can act as an SSL terminator as well as provide additional access lists and restrictions based on various client attributes.</p><p>Scalability<a id="id556" class="indexterm"/> can be achieved by utilizing a reverse proxy to make parallel connections to multiple upstream servers, enabling them to act as if they were one. If the application requires more processing power, additional upstream servers can be added to the pool served by a single reverse proxy.</p><p>Performance of an application may be enhanced through the use of a reverse proxy in several ways. The reverse proxy can cache and compress content before delivering it out to the client. NGINX as a reverse proxy can handle more concurrent client connections than a typical application server. Certain architectures configure NGINX to serve static content from a local disk cache, passing only dynamic requests to the upstream server to handle. Clients can keep their connections to NGINX alive, while NGINX terminates the ones to the upstream servers immediately, thus freeing resources on those upstream servers.</p><p>We will discuss these topics, as well as the remaining proxy module directives, in the following sections:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Security through separation</li><li class="listitem" style="list-style-type: disc">Isolating application components for scalability</li><li class="listitem" style="list-style-type: disc">Reverse proxy performance tuning</li></ul></div><div class="section" title="Security through separation"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec36"/>Security through separation</h1></div></div></div><p>We can achieve a measure of security by separating out the point to which clients connect to an application. This is one of the main reasons for using a reverse proxy in an architecture. The client connects directly only to the machine running the reverse proxy. This machine should therefore be secured well enough that an attacker cannot find a point of entry.</p><p>Security<a id="id557" class="indexterm"/> is such a large topic that we will touch only briefly on the main points to observe:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Set up a firewall in <a id="id558" class="indexterm"/>front of the reverse proxy that only allows public access to port 80 (and 443, if HTTPS connections should also be made)</li><li class="listitem" style="list-style-type: disc">Ensure that NGINX is running as an unprivileged user (typically <code class="literal">www</code>, <code class="literal">webservd</code>, or <code class="literal">www-data</code>, depending on the operating system)</li><li class="listitem" style="list-style-type: disc">Encrypt traffic where you can to prevent eavesdropping</li></ul></div><p>We will spend some time on this last point in the next section.</p><div class="section" title="Encrypting traffic with SSL"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec34"/>Encrypting traffic with SSL</h2></div></div></div><p>NGINX is often used <a id="id559" class="indexterm"/>to terminate SSL connections, either because the upstream server<a id="id560" class="indexterm"/> is not capable of using SSL or to offload the processing requirements of SSL connections. This requires that your <code class="literal">nginx</code> binary was compiled with SSL support (<code class="literal">--with_http_ssl_module</code>) and that you install an SSL certificate and key.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note09"/>Note</h3><p>For details about how to generate your own SSL certificate, please see the <span class="emphasis"><em>Using OpenSSL to generate an SSL certificate</em></span> tip in <a class="link" href="ch03.html" title="Chapter 3. Using the Mail Module">Chapter 3</a>, <span class="emphasis"><em>Using the Mail Module</em></span>.</p></div></div><p>The following is an example configuration for enabling HTTPS connections to <code class="literal">www.example.com</code>:</p><div class="informalexample"><pre class="programlisting">server {

   listen             443 default ssl;

   server_name www.example.com;


    ssl_prefer_server_ciphers  on;

    ssl_protocols    TLSv1 SSLv3;

    ssl_ciphers      RC4:HIGH:!aNULL:!MD5:@STRENGTH;

    ssl_session_cache    shared:WEB:10m;

    ssl_certificate        /usr/local/etc/nginx/www.example.com.crt;

    ssl_certificate_key /usr/local/etc/nginx/www.example.com.key;

    location / {

        proxy_set_header X-FORWARDED-PROTO https;

        proxy_pass http://upstream;

    }

}</pre></div><p>In the <a id="id561" class="indexterm"/>preceding example, we first activate the <code class="literal">ssl</code> module by<a id="id562" class="indexterm"/> using the <code class="literal">ssl</code> parameter<a id="id563" class="indexterm"/> to the<a id="id564" class="indexterm"/> <code class="literal">listen</code> directive. Then, we specify that we wish the server's ciphers to<a id="id565" class="indexterm"/> be chosen over the client's list, as we can configure the server to use the ciphers that have proven to be most secure. This prevents clients from negotiating a cipher that has been deprecated. The <code class="literal">ssl_session_cache</code> directive<a id="id566" class="indexterm"/> is set to <code class="literal">shared</code> so that all worker processes can benefit from the expensive SSL negotiation that has already been done once per client. Multiple virtual servers can use the same <code class="literal">ssl_session_cache</code> directive if they are all configured with the same name, or if this directive is specified in the <code class="literal">http</code> context. The second and third parts of the value are the name of the cache and its size, respectively. Then it is just a matter of specifying the certificate and key for this host. Note that the permissions of this key file should be set such that only the master process may read it. We set the header <code class="literal">X-FORWARDED-PROTO</code> to the value <code class="literal">https</code> so that the application running on the upstream server can recognize the fact that the original request used HTTPS.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip12"/>Tip</h3><p>
<span class="strong"><strong>SSL ciphers</strong></span>
</p><p>The preceding ciphers <a id="id567" class="indexterm"/>were chosen based on NGINX's default, which excludes those that offer no authentication (<code class="literal">aNULL</code>) as well as those using MD5. The RC4 is placed at the beginning so that ciphers not susceptible to the BEAST attack described in CVE-2011-3389 are preferred. The <code class="literal">@STRENGTH</code> string<a id="id568" class="indexterm"/> at the end is present to sort the list of ciphers in order of the encryption algorithm key length.</p></div></div><p>We <a id="id569" class="indexterm"/>have just <a id="id570" class="indexterm"/>encrypted the traffic passing between the client and the reverse proxy. It is also possible to encrypt the traffic between the reverse proxy and the upstream server:</p><div class="informalexample"><pre class="programlisting">server {
    …

    proxy_pass https://upstream;

}</pre></div><p>This is usually only reserved for those architectures in which even the internal network over which such a connection flows is considered insecure.</p></div><div class="section" title="Authenticating clients using SSL"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec35"/>Authenticating clients using SSL</h2></div></div></div><p>Some applications<a id="id571" class="indexterm"/> use information from the SSL certificate the <a id="id572" class="indexterm"/>client presents, but this information is not directly available in a reverse proxy architecture. To pass this information along to the application, you can instruct NGINX to set an additional header:</p><div class="informalexample"><pre class="programlisting">location /ssl {

    proxy_set_header ssl_client_cert $ssl_client_cert;

    proxy_pass http://upstream;

}</pre></div><p>The <code class="literal">$ssl_client_cert</code> variable<a id="id573" class="indexterm"/> contains the client's SSL certificate, in PEM format. We pass this on to the upstream server in a header of the same name. The application itself is then responsible for using this information in whatever way is appropriate.</p><p>Instead of passing the whole client certificate to the upstream server, NGINX can do some work ahead of time to see if the client is even valid. A valid client SSL certificate is one which has been signed by a recognized Certificate Authority, has a validity date in the future, and has not been revoked:</p><div class="informalexample"><pre class="programlisting">server {
    …

    ssl_client_certificate /usr/local/etc/nginx/ClientCertCAs.pem;

    ssl_crl /usr/local/etc/nginx/ClientCertCRLs.crl;

    ssl_verify_client on;

    ssl_verify_depth 3;

    error_page 495 = @noverify;

    error_page 496 = @nocert;

    location @noverify {

        proxy_pass http://insecure?status=notverified;

    }

    location @nocert {

        proxy_pass http://insecure?status=nocert;

    }

    location / {

        if ($ssl_client_verify = FAILED) {

            return 495;

        }

        proxy_pass http://secured;

    }

}</pre></div><p>The preceding<a id="id574" class="indexterm"/> configuration<a id="id575" class="indexterm"/> is constructed out of the following parts to achieve the objective of having NGINX validate client SSL certificates before passing the request on to the upstream server:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The argument to the <code class="literal">ssl_client_certificate</code> directive<a id="id576" class="indexterm"/> specifies the path to the PEM-encoded list of root CA certificates that will be considered valid signers of client certificates.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">ssl_crl</code> argument<a id="id577" class="indexterm"/> indicates the path to a certificate revocation list, issued by the Certificate Authority responsible for signing client certificates. This CRL needs to be downloaded separately and periodically refreshed.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">ssl_verify_client</code> directive<a id="id578" class="indexterm"/> states that we want NGINX to check the validity of SSL certificates presented by clients.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">ssl_verify_depth</code> directive<a id="id579" class="indexterm"/> is responsible for how many signers will be checked before declaring the certificate invalid. SSL certificates may be signed by one or more intermediate CAs. Either an intermediate CA certificate or the root CA that signed it needs to be in our <code class="literal">ssl_client_certificate</code> path <a id="id580" class="indexterm"/>for NGINX to consider the client certificate valid.</li><li class="listitem" style="list-style-type: disc">If some<a id="id581" class="indexterm"/> sort of error occurred during client <a id="id582" class="indexterm"/>certificate validation, NGINX will return the non-standard error code 495. We have defined an <code class="literal">error_page</code> that matches this code and redirects the request to a named location, to be handled by a separate proxied server. We also include a check for the value of <code class="literal">$ssl_client_verify</code> within the <code class="literal">proxy_pass</code> location, so that an invalid certificate will also return this code.</li><li class="listitem" style="list-style-type: disc">If a certificate is not valid, NGINX will return the non-standard error code 496, which we capture as well with an<a id="id583" class="indexterm"/> <code class="literal">error_page</code> directive. The <code class="literal">error_page</code> directive that we define points to a named location, which proxies the request to a separate error handler.</li></ul></div><p>Only when the client has presented a valid SSL certificate will NGINX pass the request on to the upstream server, <code class="literal">secured</code>. By doing so, we have ensured that only authenticated users actually get to place requests to the upstream server. This is an important security feature of a reverse proxy.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note10"/>Note</h3><p>NGINX from Version 1.3.7 provides the capability to use OCSP responders to verify client SSL certificates. See the <code class="literal">ssl_stapling*</code> and <code class="literal">ssl_trusted_certificate</code> directives in <a class="link" href="apa.html" title="Appendix A. Directive Reference">Appendix A</a>, <span class="emphasis"><em>Directive Reference</em></span>, for a description of how to activate this functionality.</p></div></div><p>If the application still needs some information present in the certificate, for example, to authorize a user, NGINX can deliver this information in a header:</p><div class="informalexample"><pre class="programlisting">location / {

    proxy_set_header X-HTTP-AUTH $ssl_client_s_dn;

    proxy_pass http://secured;

}</pre></div><p>Now, our <a id="id584" class="indexterm"/>application<a id="id585" class="indexterm"/> running on the upstream server <code class="literal">secured</code> can use the value of the <code class="literal">X-HTTP-AUTH</code> header to authorize the client for access to different areas. The variable <code class="literal">$ssl_client_s_dn</code> contains the subject <code class="literal">DN</code> of the client certificate. The application can use this information to match the user against a database or make a look up in a directory.</p></div><div class="section" title="Blocking traffic based on originating IP address"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec36"/>Blocking traffic based on originating IP address</h2></div></div></div><p>As client connections <a id="id586" class="indexterm"/>terminate on the reverse proxy, it is possible to limit clients based on IP address. This is useful in cases of abuse where a number of invalid connections originate from a certain set of IP addresses. As in Perl, there is more than one way to do it. We will discuss the <code class="literal">GeoIP</code> module here as a possible solution.</p><p>Your <code class="literal">nginx</code> binary will need to have been compiled with the <code class="literal">GeoIP</code> module activated (<code class="literal">--with-http_geoip_module</code>) and the MaxMind GeoIP library installed on your system. Specify the location of the precompiled database file with the <code class="literal">geoip_country</code> directive<a id="id587" class="indexterm"/> in the <code class="literal">http</code> context. This provides the most efficient way to block/allow IP addresses by country code:</p><div class="informalexample"><pre class="programlisting">geoip_country /usr/local/etc/geo/GeoIP.dat;</pre></div><p>If a client's connection comes from an IP address listed in this database, the value of the <code class="literal">$geoip_country_code</code> variable will be set to the ISO two-letter code for the originating country.</p><p>We will use the data provided by the <code class="literal">GeoIP</code> module<a id="id588" class="indexterm"/> together with the closely-named <code class="literal">geo</code> module, as well. The <code class="literal">geo</code> module provides a very basic interface for setting variables based on the IP address of a client connection. It sets up a named context within which the first parameter is the IP address to match and the second is the value that match should obtain. By combining these two modules, we can block IP addresses based on the country of origin, while allowing access from a set of specific IP addresses.</p><p>In our scenario, we are providing a service to Swiss banks. We want the public parts of the site to be indexed by Google, but are for now still restricting access to Swiss IPs. We also want a local watchdog service to be able to access the site to ensure it is still responding properly. We define a variable <code class="literal">$exclusions</code>, which will have the value <code class="literal">0</code> by default. If any of our criteria are matched, the value will be set to <code class="literal">1</code>, which we will use to control <a id="id589" class="indexterm"/>access to the site:</p><div class="informalexample"><pre class="programlisting">http {

    # the path to the GeoIP database

    geoip_country /usr/local/etc/geo/GeoIP.dat;

    # we define the variable $exclusions and list all IP addresses # allowed
    #   access by setting the value to "1"

    geo $exclusions {

        default 0;
        127.0.0.1 1;
        216.239.32.0/19 1;
        64.233.160.0/19 1;
        66.249.80.0/20  1;
        72.14.192.0/18  1;
        209.85.128.0/17 1;
        66.102.0.0/20   1;
        74.125.0.0/16   1;
        64.18.0.0/20    1;
        207.126.144.0/20 1;
        173.194.0.0/16 1;

    }

    server {

        # the country code we want to allow is "CH", for Switzerland
        if ($geoip_country_code = "CH") {

            set $exclusions 1;

        }
        location / {

            # any IP's not from Switzerland or in our list above # receive the
            # default value of "0" and are given the Forbidden HTTP# code
            if ($exclusions = "0" ) {

                return 403;

            }

            # anybody else has made it this far and is allowed access# to the
            # upstream server
            proxy_pass http://upstream;

        }

    }

}</pre></div><p>This is<a id="id590" class="indexterm"/> just one way of solving the problem of blocking access to a site based on the client's IP address. Other solutions involve saving the IP address of the client in a key-value store, updating a counter for each request, and blocking access if there have been too many requests within a certain time period.</p></div></div></div>
<div class="section" title="Isolating application components for scalability"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec37"/>Isolating application components for scalability</h1></div></div></div><p>Scaling applications<a id="id591" class="indexterm"/> can be described by moving in <a id="id592" class="indexterm"/>two dimensions, up and out. Scaling up refers to adding more resources to a machine, growing its pool of available resources to meet client demand. Scaling out means adding more machines to a pool of available responders, so that no one machine gets tied up handling the majority of clients. Whether these machines are virtualized instances running in the cloud or physical machines sitting in a datacenter, it is often more cost-effective to scale out rather than up. This is where NGINX fits in handily as a reverse proxy.</p><p>Due to its very low resource usage, NGINX acts ideally as the broker in a client-application relationship. NGINX handles the connection to the client, able to process multiple requests simultaneously. Depending on the configuration, NGINX will either deliver a file from its local cache or pass the request on to an upstream server for further processing. The upstream server can be any type of server that speaks the HTTP protocol. More client connections can be handled than if an upstream server were to respond directly:</p><div class="informalexample"><pre class="programlisting">upstream app {

    server 10.0.40.10;

    server 10.0.40.20;

    server 10.0.40.30;

}</pre></div><p>Over time, the initial set of upstream servers may need to be expanded. The traffic to the site has increased so much, that the current set can't respond in a timely enough manner. By using <a id="id593" class="indexterm"/>NGINX as the reverse proxy, this situation can easily be remedied by adding more <a id="id594" class="indexterm"/>upstream servers.</p><div class="mediaobject"><img src="graphics/7447OS_05_01.jpg" alt="Isolating application components for scalability"/></div><p>Adding more upstream servers can be done as follows:</p><div class="informalexample"><pre class="programlisting">upstream app {

    server 10.0.40.10;

    server 10.0.40.20;
    server 10.0.40.30;

    server 10.0.40.40;

    server 10.0.40.50;

    server 10.0.40.60;

}</pre></div><p>Perhaps the time has come for the application to be rewritten, or to be migrated onto a server with a different application stack.  Before moving the whole application over, one server can be brought into the active pool for testing under real load with real clients.  This server could be given fewer requests to help minimize any negative reactions should problems arise.</p><div class="mediaobject"><img src="graphics/7447OS_05_02.jpg" alt="Isolating application components for scalability"/></div><p>This <a id="id595" class="indexterm"/>is done with the following <a id="id596" class="indexterm"/>configuration:</p><div class="informalexample"><pre class="programlisting">upstream app {

    server 10.0.40.10 weight 10;

    server 10.0.40.20 weight 10;

    server 10.0.40.30 weight 10;

    server 10.0.40.100 weight 2;

}</pre></div><p>Alternatively, perhaps it is time for scheduled maintenance on a particular upstream server, so it should not receive any new requests. By marking that server as <code class="literal">down</code> in the configuration, we can proceed with that maintenance work:</p><div class="mediaobject"><img src="graphics/7447OS_05_03.jpg" alt="Isolating application components for scalability"/></div><p>The following configuration describes how to mark the server <code class="literal">down</code>:</p><div class="informalexample"><pre class="programlisting">upstream app {

    server 10.0.40.10;

    server 10.0.40.20;

    server 10.0.40.30 down;

}</pre></div><p>Unresponsive <a id="id597" class="indexterm"/>upstream servers should<a id="id598" class="indexterm"/> be handled quickly. Depending on the application, the timeout directives can be set aggressively low:</p><div class="informalexample"><pre class="programlisting">location / {

    proxy_connect_timeout 5;

    proxy_read_timeout 10;

    proxy_send_timeout 10;

}</pre></div><p>Be careful, though, that the upstream servers can usually respond within the time set by the timeout, or <a id="id599" class="indexterm"/>NGINX may deliver a <span class="strong"><strong>504 Gateway Timeout Error</strong></span>
<a id="id600" class="indexterm"/> when no upstream <a id="id601" class="indexterm"/>servers respond within this time.</p></div>
<div class="section" title="Reverse proxy performance tuning"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec38"/>Reverse proxy performance tuning</h1></div></div></div><p>NGINX can be tuned in a <a id="id602" class="indexterm"/>number of ways to get the most out of the application for which it is acting as a reverse proxy. By buffering, caching, and compressing, NGINX can be configured to make the client's experience as snappy as possible.</p><div class="section" title="Buffering"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec37"/>Buffering</h2></div></div></div><p>Buffering <a id="id603" class="indexterm"/>can be <a id="id604" class="indexterm"/>described with the help of the following figure:</p><div class="mediaobject"><img src="graphics/7447OS_05_04.jpg" alt="Buffering"/></div><p>The most important factor to consider performance-wise when proxying is buffering. NGINX, by default, will try to read as much as possible from the upstream server as fast as possible before returning that response to the client. It will buffer the response locally so that it can deliver it to the client all at once. If any part of the request from the client or the response from the upstream server is written out to disk, performance might drop. This is a trade-off between RAM and disk. So it is very important to consider the following<a id="id605" class="indexterm"/> directives when configuring NGINX to act as a reverse proxy:</p><div class="section" title="Table: Proxy module buffering directives"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl6sec19"/>Table: Proxy module buffering directives</h3></div></div></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Directive</p>
</th><th style="text-align: left" valign="bottom">
<p>Explanation</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_buffer_size</code>
<a id="id606" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The size of the buffer used for the first part of the response from the upstream server, in which the response headers are found.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_buffering</code>
<a id="id607" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Activates buffering of proxied content; when switched off, responses are sent synchronously to the client as soon as they are received, provided the <code class="literal">proxy_max_temp_file_size</code> parameter is set to <code class="literal">0</code>. Setting this to <code class="literal">0</code> and turning <code class="literal">proxy_buffering</code> to <code class="literal">on</code> ensures that there is no disk usage during proxying, while still enabling buffering.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_buffers</code>
<a id="id608" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The number and size of buffers used for responses from upstream servers.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_busy_buffers_size</code>
<a id="id609" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The total size of buffer space allocated to sending the response to the client while still being read from the upstream server. This is typically set to two <code class="literal">proxy_buffers</code>.</p>
</td></tr></tbody></table></div><p>In addition to the preceding directives, the upstream server may influence buffering by setting the <code class="literal">X-Accel-Buffering</code> header. The default value of this header is <code class="literal">yes</code>, meaning that responses will be buffered. Setting the value to <code class="literal">no</code> is useful for Comet and HTTP streaming applications, where it is important to not buffer the response.</p><p>By measuring the average request and response sizes going through the reverse proxy, the proxy buffer sizes can be tuned optimally. Each buffer directive counts per connection, in addition to an OS-dependent per-connection overhead, so we can calculate how many simultaneous client connections we can support with the amount of memory on a system.</p><p>The default values for the <code class="literal">proxy_buffers</code> directive (<code class="literal">8 4k</code> or <code class="literal">8 8k</code>, depending on the operating system), enable a large number of simultaneous connections. Let's figure out just how many connections that is. On a typical 1 GB machine, where only NGINX runs, most of the memory can be dedicated to its use. Some will be used by the operating system for the filesystem cache and other needs, so let's be conservative and estimate that NGINX would have up to 768 MB.</p><p>Eight 4 KB buffers is 32,768 bytes (8 * 4 * 1024) per active connection.</p><p>The 768 MB we allocated to NGINX is 805,306,368 bytes (768 * 1024 * 1024).</p><p>Dividing the two, we come up with 805306368 / 32768 = 24576 active connections.</p><p>So, NGINX would be able to handle just under 25,000 simultaneous, active connections in its default configuration, assuming that these buffers will be constantly filled. There are a number of other factors that come into play, such as cached content and idle connections, but this gives us a good ballpark estimate to work with.</p><p>Now, if we <a id="id610" class="indexterm"/>take the following numbers as our average request and response sizes, we see that eight 4 KB buffers just aren't enough to process a typical request. We want NGINX to buffer as much of the response as possible so that the user receives it all at once, provided the user is on a fast link.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Average request size: 800 bytes</li><li class="listitem" style="list-style-type: disc">Average response size: 900 KB<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note11"/>Note</h3><p>The tuning examples in the rest of this section will use more memory at the expense of concurrent, active connections. They are optimizations, and shouldn't be understood as recommendations for a general configuration. NGINX is already optimally tuned to provide for many, slow clients and a few, fast upstream servers. As the trend in computing is more towards mobile users, the client connection is considerably slower than a broadband user's connection. So, it's important to know your users and how they will be connecting, before embarking on any optimizations.</p></div></div></li></ul></div><p>We would adjust our buffer sizes accordingly so that the whole response would fit in the buffers:</p><div class="informalexample"><pre class="programlisting">http {

    proxy_buffers 30 32k;

}</pre></div><p>This means, of course, that we would be able to handle far fewer concurrent users.</p><p>Thirty 32 KB buffers is 983,040 bytes (30 * 32 * 1024) per connection.</p><p>The 768 MB we allocated to NGINX is 805,306,368 bytes (768 * 1024 * 1024).</p><p>Dividing the two, we come up with 805306368 / 983040 = 819.2 active connections.</p><p>That isn't too many concurrent connections at all. Let's adjust the number of buffers down, and ensure that NGINX will start transferring something to the client while the rest of the response is read into the remaining <code class="literal">proxy_buffers</code> space:</p><div class="informalexample"><pre class="programlisting">http {

    proxy_buffers 4 32k;

    proxy_busy_buffers_size 64k;

}</pre></div><p>Four 32 KB buffers is 131,072 bytes (4 * 32 * 1024) per connection.</p><p>The 768 MB we allocated to NGINX is 805,306,368 bytes (768 * 1024 * 1024).</p><p>Dividing the <a id="id611" class="indexterm"/>two, we come up with 805306368 / 131072 = 6144 active connections.</p><p>For a reverse-proxy machine, we may therefore want to scale up by adding more memory (6 GB RAM will yield us approximately 37,000 connections) or scale out by adding more 1 GB machines behind a load balancer, up to the number of concurrent, active users we can expect.</p></div></div><div class="section" title="Caching"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec38"/>Caching</h2></div></div></div><p>Caching <a id="id612" class="indexterm"/>can be described <a id="id613" class="indexterm"/>with the following figure:</p><div class="mediaobject"><img src="graphics/7447OS_05_05.jpg" alt="Caching"/></div><p>NGINX is also capable of caching the response from the upstream server, so that the same request asked again doesn't have to go back to the upstream server to be served. The preceding figure illustrates this as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>1a</strong></span>: A client makes a request</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>1b</strong></span>: The request's cache key is not currently found in the cache, so NGINX requests it from the upstream server</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>1c</strong></span>: The upstream responds and NGINX places the response corresponding to that request's cache key into the cache</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>1d</strong></span>: The response is delivered to the client</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>2a</strong></span>: Another client<a id="id614" class="indexterm"/> makes a request that has a matching cache key</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>2b</strong></span>: NGINX<a id="id615" class="indexterm"/> is able to serve the response directly from the cache without needing to first get the response from the upstream server</li></ul></div><div class="section" title="Table: Proxy module caching directives"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl6sec20"/>Table: Proxy module caching directives</h3></div></div></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Directive</p>
</th><th style="text-align: left" valign="bottom">
<p>Explanation</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache</code>
<a id="id616" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Defines a shared memory zone to be used for caching.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_bypass</code>
<a id="id617" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>One or more string variables, which when non-empty or non-zero, will cause the response to be taken from the upstream server instead of the cache.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_key</code>
<a id="id618" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>A string used as the key for storing and retrieving cache values. Variables may be used, but care should be taken to avoid caching multiple copies of the same content.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_lock</code>
<a id="id619" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Enabling this directive will prevent multiple requests to the upstream server(s) during a cache miss. The requests will wait for the first to return and make an entry into the cache key. This lock is per worker.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_lock_timeout</code>
<a id="id620" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The length of time a request will wait for an entry to appear in the cache or for the <code class="literal">proxy_cache_lock</code> to be released.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_min_uses</code>
<a id="id621" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The number of requests for a certain key needed before a response is cached.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_path</code>
<a id="id622" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>A directory in which to place the cached responses and a shared memory zone (<code class="literal">keys_zone=name:size</code>) to store active keys and response metadata. Optional parameters are:</p>
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">levels</code>: Colon-separated length of subdirectory name at each level (1 or 2), maximum of three levels deep</li><li class="listitem" style="list-style-type: disc"><code class="literal">inactive</code>: The maximum length of time an inactive response stays in the cache before being ejected</li><li class="listitem" style="list-style-type: disc"><code class="literal">max_size</code>: The maximum size of the cache; when the size exceeds this value, a cache manager process removes the least recently used items</li><li class="listitem" style="list-style-type: disc"><code class="literal">loader_files</code>: The maximum number of cached files whose metadata are loaded per iteration of the cache loader process</li><li class="listitem" style="list-style-type: disc"><code class="literal">loader_sleep</code>: The number of milliseconds paused between each iteration of the cache loader process</li><li class="listitem" style="list-style-type: disc"><code class="literal">loader_threshold</code>: The maximum length of time a cache loader iteration may take</li></ul></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_use_stale</code>
<a id="id623" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The cases under which it is acceptable to serve stale cached data when an error occurs while accessing the upstream server. The <code class="literal">updating</code> parameter indicates the case when fresh data are being loaded.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_cache_valid</code>
<a id="id624" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Indicates the length of time for which a cached response with response code 200, 301, or 302 is valid. If an optional response code is given before the time parameter, that time is only for that response code. The special parameter <code class="literal">any</code> indicates that any response code should be cached for that length of time.</p>
</td></tr></tbody></table></div><p>The following <a id="id625" class="indexterm"/>configuration is designed to cache all responses for six hours, up to a total cache size of 1 GB. Any items that stay fresh, that is, are called within the six hour timeout, are valid for up to one day. After this time, the upstream server will be called again to provide the response. If the upstream isn't able to respond due to an error, timeout, invalid header, or if the cached item is being updated, a stale cache element may be used. The shared memory zone, <a id="id626" class="indexterm"/>
<span class="strong"><strong>CACHE</strong></span>, is defined to be 10 MB large and is referenced within the <code class="literal">location</code> where the cache keys need to be set and looked-up.</p><div class="informalexample"><pre class="programlisting">http {

    # we set this to be on the same filesystem as proxy_cache_path
    proxy_temp_path /var/spool/nginx;

    # good security practice dictates that this directory is owned by the
    #  same user as the user directive (under which the workers run)
    proxy_cache_path /var/spool/nginx keys_zone=CACHE:10m levels=1:2 inactive=6h max_size=1g;

    server {

        location / {

            # using include to bring in a file with commonly-used settings
            include proxy.conf;

            # referencing the shared memory zone defined above
            proxy_cache CACHE;

            proxy_cache_valid any 1d;

            proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504;

            proxy_pass http://upstream;

        }

    }

}</pre></div><p>Using this <a id="id627" class="indexterm"/>configuration, NGINX will set up a series of directories under <code class="literal">/var/spool/nginx</code> that will first differentiate on the last character of the MD5 hash of the URI, followed by the next two characters from the last. For example, the response for "/this-is-a-typical-url" will be stored as:</p><div class="informalexample"><pre class="programlisting">/var/spool/nginx/3/f1/614c16873c96c9db2090134be91cbf13</pre></div><p>In addition to the <code class="literal">proxy_cache_valid</code> directive, a number of headers control how NGINX caches responses. The header values take precedence over the directive.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">X-Accel-Expires</code> header can be set by the upstream server to control cache behavior:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">An integer value indicates the time in seconds for which a response may be cached</li><li class="listitem" style="list-style-type: disc">If the value of this header is <code class="literal">0</code>, caching for that response is disabled completely</li></ul></div></li><li class="listitem" style="list-style-type: disc">A value beginning with <code class="literal">@</code> indicates the time in seconds since the epoch. The response is valid only up to this absolute time.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">Expires</code> and <code class="literal">Cache-Control</code> headers have the same precedence level.</li><li class="listitem" style="list-style-type: disc">If the value of the <code class="literal">Expires</code> header is in the future, the response will be cached until then.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">Cache-Control</code> header can have multiple values:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">no-cache</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">no-store</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">private</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">max-age</code></li></ul></div></li><li class="listitem" style="list-style-type: disc">The only value for which the response is actually cached is a <code class="literal">max-age</code>, which is numeric and non-zero, that is, <code class="literal">max-age=x</code> where <code class="literal">x</code> &gt; 0.</li><li class="listitem" style="list-style-type: disc">If the <code class="literal">Set-Cookie</code> header is present, the response is not cached.<p>This may be overridden, though, by using the <code class="literal">proxy_ignore_headers</code> directive:</p><div class="informalexample"><pre class="programlisting">proxy_ignore_headers Set-Cookie;</pre></div></li><li class="listitem" style="list-style-type: disc">But if doing so, be sure to make the cookie value part of the <code class="literal">proxy_cache_key</code>:<div class="informalexample"><pre class="programlisting">proxy_cache_key "$host$request_uri $cookie_user";</pre></div></li></ul></div><p>Care <a id="id628" class="indexterm"/>should be taken when doing this, though, to prevent multiple response bodies from being cached for the same URI. This can happen when public content inadvertently has the <code class="literal">Set-Cookie</code> header set for it, and this then becomes part of the key used to access this data. Separating public content out to a different location is one way to ensure that the cache is being used effectively. For example, serving images from an <code class="literal">/img</code> location where a different <code class="literal">proxy_cache_key</code> is defined:</p><div class="informalexample"><pre class="programlisting">server {

    proxy_ignore_headers Set-Cookie;

    location /img {

        proxy_cache_key "$host$request_uri";

        proxy_pass http://upstream;

    }

    location / {

        proxy_cache_key "$host$request_uri $cookie_user";

        proxy_pass http://upstream;

    }

}</pre></div></div><div class="section" title="Storing"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec25"/>Storing</h3></div></div></div><p>Related to the <a id="id629" class="indexterm"/>concept of a cache is a<a id="id630" class="indexterm"/> <span class="strong"><strong>store</strong></span>. If you are serving large, static files that will never change, that is, there is no reason to expire the entries, then NGINX offers something called a store to help serve these files faster. NGINX will store a local copy of any files that you configure it to fetch. These files will remain on disk and the upstream server will not be asked for them again. If any of these files should change upstream, they need to be deleted by some external process, or NGINX will continue serving them, so for smaller, static files, using the cache is more appropriate.</p><p>The following configuration summarizes the directives used to store these files:</p><div class="informalexample"><pre class="programlisting">http {

    proxy_temp_path /var/www/tmp;

    server {

        root /var/www/data

        location /img {

            error_page 404 = @store;

        }

        location @store {

            internal;

            proxy_store on;

            proxy_store_access group:r all:r;

            proxy_pass http://upstream;

        }

    }

}</pre></div><p>In this configuration, we define a <code class="literal">server</code> with a <code class="literal">root</code> under the same filesystem as the <code class="literal">proxy_temp_path</code>. The <code class="literal">location</code> directive <code class="literal">/img</code> will inherit this <code class="literal">root</code>, serving files of the same name as the <a id="id631" class="indexterm"/>URI path under <code class="literal">/var/www/data</code>. If a file is not found (error code 404), the named <code class="literal">location</code> directive <code class="literal">@store</code> is called to fetch the file from the upstream. The <code class="literal">proxy_store</code> directive<a id="id632" class="indexterm"/> indicates that we want to store files under the inherited <code class="literal">root</code> with permissions <code class="literal">0644</code> (the <code class="literal">user:rw</code> is understood, while <code class="literal">group</code> or <code class="literal">all</code> are specified in <code class="literal">proxy_store_access</code>). That's all it takes for NGINX to store a local copy of static files served by the upstream server.</p></div></div><div class="section" title="Compressing"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec39"/>Compressing</h2></div></div></div><p>Compressing <a id="id633" class="indexterm"/>can be described <a id="id634" class="indexterm"/>with the following figure:</p><div class="mediaobject"><img src="graphics/7447OS_05_06.jpg" alt="Compressing"/></div><p>Optimizing for bandwidth can help reduce a response's transfer time. NGINX has the capability of compressing a response it receives from an upstream server before passing it on to the client. The <code class="literal">gzip</code> module<a id="id635" class="indexterm"/>, enabled by default, is often used on a reverse proxy to compress content where it makes sense. Some file types do not compress well. Some clients do not respond well to compressed content. We can take both cases into account in our configuration:</p><div class="informalexample"><pre class="programlisting">http {

    gzip on;

    gzip_http_version 1.0;

    gzip_comp_level 2;

    gzip_types text/plain text/css application/x-javascript text/xml application/xml application/xml+rss text/javascript application/javascript application/json;

    gzip_disable msie6;

}</pre></div><p>Here we've <a id="id636" class="indexterm"/>specified<a id="id637" class="indexterm"/> that we want files of the preceding MIME types to be compressed at a gzip compression level of 2 if the request has come over at least HTTP/1.0, except if the user agent reports being an older version of Internet Explorer. We've placed this configuration in the <code class="literal">http</code> context so that it will be valid for all servers we define.</p><p>The following table lists the directives<a id="id638" class="indexterm"/> available with the <code class="literal">gzip</code> module:</p><div class="section" title="Table: Gzip module directives"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl6sec21"/>Table: Gzip module directives</h3></div></div></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Directive</p>
</th><th style="text-align: left" valign="bottom">
<p>Explanation</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">gzip</code>
<a id="id639" class="indexterm"/>
<a id="id640" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Enables or disables the compression of responses.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">gzip_buffers</code>
<a id="id641" class="indexterm"/>
<a id="id642" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Specifies the number and size of buffers used for compressing a response.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">gzip_comp_level</code>
<a id="id643" class="indexterm"/>
<a id="id644" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The gzip compression level (1-9).</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">gzip_disable</code>
<a id="id645" class="indexterm"/>
<a id="id646" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>A regular expression of <code class="literal">User-Agents</code> that shouldn't receive a compressed response. The special value <code class="literal">msie6</code> is a shortcut for <code class="literal">MSIE [4-6]\.</code> excluding <code class="literal">MSIE 6.0; ... SV1</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">gzip_min_length</code>
<a id="id647" class="indexterm"/>
<a id="id648" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The minimum length of a response before compression is considered, determined by the <code class="literal">Content-Length</code> header.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">gzip_http_version</code>
<a id="id649" class="indexterm"/>
<a id="id650" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The <a id="id651" class="indexterm"/>minimum HTTP version of a request before compression is considered.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">gzip_proxied</code>
<a id="id652" class="indexterm"/>
<a id="id653" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Enables or disables compression if the request has already come through a proxy. Takes one or more of the following parameters:</p>
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">off</code>: Disables compression</li><li class="listitem" style="list-style-type: disc"><code class="literal">expired</code>: Enables compression if the response should not be cached, as determined by the <code class="literal">Expires</code> header</li><li class="listitem" style="list-style-type: disc"><code class="literal">no-cache</code>: Enables compression if the <code class="literal">Cache-Control</code> header is equal to <code class="literal">no-cache</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">no-store</code>: Enables compression if the <code class="literal">Cache-Control</code> header is equal to <code class="literal">no-store</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">private</code>: Enables compression if the <code class="literal">Cache-Control</code> header is equal to <code class="literal">private</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">no_last_modified</code>: Enables compression if the response doesn't have a <code class="literal">Last-Modified</code> header</li><li class="listitem" style="list-style-type: disc"><code class="literal">no_etag</code>: Enables compression if the response doesn't have an <code class="literal">ETag</code> header</li><li class="listitem" style="list-style-type: disc"><code class="literal">auth</code>: Enables compression if the request contains an <code class="literal">Authorization</code> header</li><li class="listitem" style="list-style-type: disc"><code class="literal">any</code>: Enables compression for any response whose request includes the <code class="literal">Via</code> header</li></ul></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">gzip_types</code>
<a id="id654" class="indexterm"/>
<a id="id655" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>The <a id="id656" class="indexterm"/>MIME types that should be compressed, in addition to the default value <code class="literal">text/html</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">gzip_vary</code>
<a id="id657" class="indexterm"/>
<a id="id658" class="indexterm"/>
</p>
</td><td style="text-align: left" valign="top">
<p>Enables or disables the response header <code class="literal">Vary: Accept-Encoding</code> if gzip is active.</p>
</td></tr></tbody></table></div><p>When gzip compression <a id="id659" class="indexterm"/>is enabled and you find large files being truncated, the likely culprit is <code class="literal">gzip_buffers</code>. The default value of <code class="literal">32 4k</code> or <code class="literal">16 8k</code> buffers (depending on the platform) leads to a total buffer size of 128 KB. This means that the file NGINX is to compress cannot be larger than 128 KB. If you're using an unzipped large JavaScript library, you may find yourself over this limit. If that is the case, just increase the number of buffers so that the total buffer size is large enough to fit the whole file.</p><div class="informalexample"><pre class="programlisting">http {

    gzip on;

    gzip_min_length 1024;

    gzip_buffers 40 4k;

    gzip_comp_level 5;

    gzip_types text/plain application/x-javascript application/json;

}</pre></div><p>For example, the preceding configuration will enable compression of any file up to 40 * 4 * 1024 = 163840 bytes (or 160 KB) large. We also use the <code class="literal">gzip_min_length</code> directive to tell NGINX to only compress a file if it is larger than 1 KB. A <code class="literal">gzip_comp_level</code> of 4 or 5 is usually a good trade-off between speed and compressed file size. Measuring on your hardware is the best way to find the right value for your configuration.</p><p>Besides<a id="id660" class="indexterm"/> on-the-fly compression of responses, NGINX is capable of delivering precompressed files, using the <code class="literal">gzip_static</code> module. This module is not compiled by default, but can be enabled with the <code class="literal">--with-http_gzip_static_module</code> compile-time switch. The module itself has one directive, <code class="literal">gzip_static</code>, but also uses the following directives of the <code class="literal">gzip</code> module in order to determine when to check for precompressed files:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">gzip_http_version</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">gzip_proxied</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">gzip_disable</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">gzip_vary</code></li></ul></div><p>In the following configuration, we enable delivery of precompressed files if the request contains an <code class="literal">Authorization</code> header and if the response contains one of the <code class="literal">Expires</code> or <code class="literal">Cache-Control</code> headers disabling caching:</p><div class="informalexample"><pre class="programlisting">http {

    gzip_static on;

    gzip_proxied expired no-cache no-store private auth;

}</pre></div></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec39"/>Summary</h1></div></div></div><p>We have seen in this chapter how NGINX can be used effectively as a reverse proxy. It can act in three roles, either individually or in some combination, which are to enhance security, to enable scalability, and/or to enhance performance. Security is achieved through separation of the application from the end user. NGINX can be combined with multiple upstream servers to achieve scalability. The performance of an application relates directly to how responsive it is to a user's request. We explored different mechanisms to achieve a more responsive application. Faster response times mean happier users.</p><p>Up next is an exploration of NGINX as an HTTP server. We have so far only discussed how NGINX can act as a reverse proxy, but there is so much more that NGINX is capable of.</p></div></body></html>