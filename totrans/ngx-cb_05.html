<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Logging</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Logging to syslog</li>
<li>Customizing web access logs</li>
<li>Virtual host log format</li>
<li>Application focused logging</li>
<li>Logging TLS mode and cipher information</li>
<li>Logging POST data</li>
<li>Conditional logging</li>
<li>Using the Elastic Stack</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>Akin to metrics, logging is key to monitoring how your server is performing and debugging errors. Thankfully, NGINX has an extensive logging module built-in to cover nearly all usage scenarios. In this chapter, we'll go through some of the various ways you can change the standard logging formats, as well as how to process them in more efficient manners.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logging to syslog</h1>
                </header>
            
            <article>
                
<p>If you already have your centralized server logs or your logs are analyzed by a standard syslog system, you can also redirect your NGINX logs to do the same. This is useful when using external platforms such as Loggly and Papertrail, which integrate via syslog.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Firstly, we need to consider where we're pushing the logs to, syslog can be both local and network-based, so we'll cover both ways. For nearly all Linux distributions, the default syslog service is <kbd>rsyslog</kbd> which will be listening on the Unix socket located at <kbd>/dev/log</kbd>. Here's our NGINX configuration for local logging:</p>
<pre>server { 
    listen              80; 
    server_name         syslog.nginxcookbook.com; 
    access_log syslog:server=unix:/dev/log; 
    error_log syslog:server=unix:/dev/log; 
    location /favicon.ico { access_log off; log_not_found off; } 
    root /var/www; 
} </pre>
<p>The handling of the logs will now be done by <kbd>rsyslog</kbd>. By default, this will match the wildcard rule and write the log entries to <kbd>/var/log/syslog</kbd> for Debian- / Ubuntu-based systems and <kbd>/var/log/messages</kbd> for CentOS- / RedHat-based distributions.</p>
<p>We can confirm it's working as expected by viewing the last line in our <kbd>syslog</kbd> after accessing the website:</p>
<pre><strong>tail -n 1 /var/log/syslog</strong>
    
<strong>Jun 22 23:40:53 nginx-220-ubuntu nginx-220-ubuntu nginx: <br/>106.71.217.155 - - [22/Jun/2016:23:40:53 +1000] "GET / HTTP/1.1" 200 <br/>192 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) <br/>AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36"</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Remote syslog</h1>
                </header>
            
            <article>
                
<p>While we can generate a bit more flexibility using a local syslog daemon, generally, the reason we want to offload the logging is to either decrease the load on our production systems or to use external tools for better log analysis.</p>
<p>The simplest way to do this is to send the logs to a syslog daemon or processing system that doesn't exist on the same server. This can also be used to aggregate logging from multiple servers, which facilitates monitoring and reporting in a central location.</p>
<p>To send the logs to a remote server, firstly, we need to enable the syslog server to listen on a network port. This is disabled on most <kbd>rsyslog</kbd>-based systems, so that it prevents accidental security issues. To enable, we simply uncomment the following in the <kbd>rsyslog.conf</kbd> (generally <kbd>/etc/rsyslog.conf</kbd>) file:</p>
<pre>module(load="imudp") 
input(type="imudp" port="514") </pre>
<p>After the syslog daemon is restarted (<kbd>systemctl restart rsyslog</kbd>), it will now be listening on port <kbd>514</kbd> for UDP log packets.</p>
<div class="packt_infobox">By default, syslog data is decrypted. We highly recommend that you ensure that the logs are sent via a VPN tunnel or similar encrypted means.</div>
<p>On the NGINX server, we now need to update the <kbd>server</kbd> block for the access and debug logs:</p>
<pre>access_log syslog:server=202.74.71.220,tag=nginx,severity=info combined; 
error_log syslog:server=202.71.71.220,tag=nginx,severity=error debug; </pre>
<p>After reloading NGINX to apply the new rules, we can now verify that the logs are hitting the syslog on the remote server:</p>
<pre><strong>Aug 09 23:39:02 nginx-220-ubuntu nginx: 106.71.219.248 - - [09/Aug/2016:23:39:02 +1000] "GET / HTTP/1.1" 200 184 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36"</strong>
<strong>Aug 09 23:39:06 nginx-220-ubuntu nginx: 106.71.219.248 - - [09/Aug/2016:23:39:06 +1000] "GET /404 HTTP/1.1" 404 199 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36"</strong></pre>
<p>On a Debian- / Ubuntu-based system, this will be <kbd>/var/log/syslog</kbd>. If you run a RedHat- / CentOS-based system, this will be logged to <kbd>/var/log/messages</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>NGINX reference: <a href="https://nginx.org/en/docs/syslog.html" target="_blank"><span class="URLPACKT">https://nginx.org/en/docs/syslog.html</span></a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Customizing web access logs</h1>
                </header>
            
            <article>
                
<p>There are a number of ways you can customize the log files, including the format, the information provided, and where to save them to. This customization can be especially handy if you're using NGINX as a proxy in front of application servers, where traditional web style logs may not be as useful.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>There are a few different ways we can customize the access logs in order to get more relevant information or reduce the amount of logging where it's not required. The standard configuration of <kbd>combined</kbd> is as follows:</p>
<pre>log_format combined '$remote_addr - $remote_user [$time_local] ' 
                   '"$request" $status $body_bytes_sent ' 
                   '"$http_referer" "$http_user_agent"'; </pre>
<p>This format is already predefined in NGINX out of the box, which is compatible with the Apache combined format. What this means is that, by default, combined formatted logs will be compatible with most log parsers and, will be therefore, able to directly interpret the log data.</p>
<p>While having more data can be quite helpful (as we'll show later), be careful about deviating from a standard format. If you want to use a third-party system to parse the logs, you'll also need to update or customize the parser as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The NGINX log module documentation: <a href="http://nginx.org/en/docs/http/ngx_http_log_module.html" target="_blank"><span class="URLPACKT">http://nginx.org/en/docs/http/ngx_http_log_module.html</span></a></li>
<li>The NGINX core variables: <a href="http://nginx.org/en/docs/http/ngx_http_core_module.html#variables" target="_blank"><span class="URLPACKT">http://nginx.org/en/docs/http/ngx_http_core_module.html#variables</span></a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Virtual host log format</h1>
                </header>
            
            <article>
                
<p>If you're running a virtual host style environment (with multiple <kbd>server</kbd> blocks) with NGINX, there's one small tweak you can make to enhance the logs. By default, the host (defined as <kbd>$host</kbd>) isn't logged when using the default combined format. Having this field in the logs means that the log can be parsed externally without the need for additional information.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To start with, we need to define our new log format:</p>
<pre>log_format vhostlogs '$host $remote_addr - $remote_user '  
                     '[$time_local] "$request" $status ' 
                     '$body_bytes_sent "$http_referer" '   
                     '"$http_user_agent"'; </pre>
<div class="packt_infobox">The <kbd>log_format</kbd> directive needs to be outside the <kbd>server</kbd> block.</div>
<p>To use our new log file, we update the access log format in the <kbd>server</kbd> block with the new format. Here's an example with our MediaWiki recipe:</p>
<pre>access_log  /var/log/nginx/mediawiki-vhost-access.log vhostlogs; </pre>
<p>If you had a wildcard setup for your subdomains, this would mean that you'd also have the correct hostname in the logs as well. Without this, the logs wouldn't be able to differentiate between any of the subdomains for statistical analysis and comparison.</p>
<p>With the updated format, here's an example from our logs:</p>
<pre>mediawiki.nginxcookbook.com 106.71.219.248 - - [06/Aug/2016:21:34:42 +1000] "GET /wiki/Main_Page HTTP/1.1" 200 12926 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36" </pre>
<p>We can now see the hostname clearly in the logs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Application focused logging</h1>
                </header>
            
            <article>
                
<p>In this recipe, we're going to customize the logs to give us a bit more information when it comes to applications and metrics. Additional information, such as the response time, can be immensely useful for measuring the responsiveness of your application. While it can generally be generated within your application stack, it can also induce some overhead or give incomplete results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To start with, we need to define our new log format:</p>
<pre>log_format applogs '$remote_addr $remote_user $time_iso8601' 
                   '"$request" $status $body_bytes_sent ' 
                   '$request_time $upstream_response_time'; </pre>
<div class="packt_infobox">The <kbd>log_format</kbd> directive needs to be outside the <kbd>server</kbd> block.</div>
<p>Here, we've deviated from the combined format to change the time to the ISO 8601 format (which will look something like <kbd>2016-07-16T21:48:36+00:00</kbd>), removed the HTTP referrer and user agent, but added the request processing time (<kbd>$request_time</kbd>) to get a better idea on how much time it's taking our application to generate responses.</p>
<p>With our new log format defined, we can now use this for our application logs:</p>
<pre>access_log  /var/log/nginx/meteor-access.log  applogs; </pre>
<p>We're using the default Meteor application which we had set up in <a href="db163fa8-2a5d-40bc-b83e-61e72ec67237.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Common Frameworks</em> and as we want applications to be very responsive, <kbd>$request_time</kbd> will provide instant feedback as to what calls aren't quick enough. Here's an example output from the logs:</p>
<pre>106.70.67.24 - 2016-08-07T20:47:07+10:00"GET / HTTP/1.1" 200 2393 0.005 
106.70.67.24 - 2016-08-07T20:47:07+10:00"GET /sockjs/070/tsjnlv82/websocket HTTP/1.1" 101 468 14.175  </pre>
<p>As the request times are measured in milliseconds, we can see that our base call took 0.005 ms to complete and the WebSocket connection took 14 ms to complete. This additional information can now be easily searched for and, with additional log parsers (like the Elastic Stack detailed later in this chapter), we can set further searches and alerts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logging TLS mode and cipher information</h1>
                </header>
            
            <article>
                
<p>With the advent of HTTP/2 and the ever changing cryptography best practices, small compatibility issues can arise, which are very difficult to resolve. Browsers also change what they accept on a constant basis. To ensure, we know exactly what ciphers have been used with what protocol, we can add this additional information to our log files.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Here's our SSL enhanced log format:</p>
<pre>log_format ssl_logs '$remote_addr - $remote_user [$time_local] ' 
                    '"$request" $status $body_bytes_sent ' 
                    '"$http_referer" "$http_user_agent"' 
                    '[$ssl_protocol|$ssl_cipher]'; </pre>
<p>This is based on the common format and we've added <kbd>$ssl_protocol</kbd> and <kbd>$ssl_cipher</kbd> to the end. Here's what we now have in our logs:</p>
<pre>106.70.67.24 - - [07/Aug/2016:22:39:20 +1000] "GET / HTTP/2.0" 304 118 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2490.71 Safari/537.36"[TLSv1.2|ECDHE-RSA-AES128-GCM-SHA256] 
106.70.67.24 - - [07/Aug/2016:22:39:20 +1000] "GET / HTTP/2.0" 304 118 "-" "Mozilla/5.0 (Linux; Android 5.1.1; Nexus 6 Build/LYZ28E) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.20 Mobile Safari/537.36"[TLSv1.2|ECDHE-RSA-AES128-GCM-SHA256] 
106.70.67.24 - - [07/Aug/2016:22:39:29 +1000] "GET / HTTP/2.0" 304 118 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36"[TLSv1.2|ECDHE-RSA-AES128-GCM-SHA256] 
106.70.67.24 - - [07/Aug/2016:22:42:31 +1000] "GET / HTTP/1.1" 200 215 "-" "curl/7.40.0"[TLSv1.2|ECDHE-RSA-AES128-GCM-SHA256] </pre>
<p>In the log details, we can now see the requests from Chrome, Firefox, Safari, and even cURL are using <kbd>TLSv1.2</kbd> and <kbd>ECDHE-RSA-AES128-GCM-SHA256</kbd>. If we saw abnormal behavior with a different browser or protocol, this would greatly assist in the diagnosis of the problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logging POST data</h1>
                </header>
            
            <article>
                
<p>When we have form submissions, errors in this become difficult to replay and debug if we don't know the value of the data. By default, NGINX doesn't log POST data, as it can be very bulky. However, there are certain situations where getting access to this data is vital to debugging issues.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In order to log POST data, we need to define a custom log format:</p>
<pre>log_format post_logs '[$time_local] "$request" $status '  
                     '$body_bytes_sent "$http_referer" '        
                     '"$http_user_agent" [$request_body]'; </pre>
<p>By logging <kbd>$request_body</kbd>, we'll be able to see the contents of a POST submission.</p>
<p>To enable our POST log format, all we need to do is specify the log format for the access logs:</p>
<pre>access_log  /var/log/nginx/postdata-access.log post_logs; </pre>
<p>If you just want to verify that it's working as expected, we can use cURL to send post data. I've used Express, as set up in <a href="db163fa8-2a5d-40bc-b83e-61e72ec67237.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Common Frameworks</em>, to accept the POST data, so we can make a call as follows:</p>
<pre><strong>curl http://express.nginxcookbook.com/ -d 'name=johnsmith,phone=123123,email=john@smith.demo' </strong></pre>
<p>This posts the variables name, phone, and email through to the root (<kbd>/</kbd>) URL. If we look at <kbd>postdata-access.log</kbd>, we can now see the following:</p>
<pre>[09/Aug/2016:22:45:35 +1000] "POST / HTTP/1.1" 200 18 "-""curl/7.43.0" [name=johnsmith,phone=123123,email=john@smith.demo] </pre>
<p>If you still had the standard log format enabled, it would simply display a dash (<kbd>-</kbd>) for the request, making further debugging impossible if it wasn't captured by the application.</p>
<p>Here's the corresponding log entry in a standard combined format:</p>
<pre>106.70.67.24 - - [09/Aug/2016:22:45:35 +1000] "POST / HTTP/1.1" 200 18 "-" "curl/7.43.0" </pre>
<p>Of course, by logging all of the POST data, you'll use considerably more disk space and server overhead. We highly recommend that this is only enabled for development environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Conditional logging</h1>
                </header>
            
            <article>
                
<p>There are some instances where we may want to log different data, based on some sort of conditional argument. For instance, if you have a beta copy of your application and are interested in gathering more information for it, we can log this data just for the beta URL. Rather than clogging up our log files with the additional information when it's not required, we can simply trigger it when required.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To enable, we map the (<kbd>$args</kbd>) URI arguments to a <kbd>$debuglogs</kbd> variable, based on if a condition is set. Here's the code:</p>
<pre>map $args $debuglogs { 
    default     0; 
    debug       1; 
} </pre>
<p>Like the custom log formats, this needs to be placed outside of the <kbd>server</kbd> directives. Then, within the <kbd>server</kbd> directive, we can make use of the variable and create additional logging:</p>
<pre>access_log /var/log/nginx/djangodemo-debug-access.log applogs if=$debuglogs; 
access_log  /var/log/nginx/djangodemo-access.log  combined; </pre>
<p>If we call a normal URL, it will simply log the access to the standard <kbd>djangodemo-access.log</kbd> in the combined format. If we call a URL with the <kbd>debug</kbd> argument set (for example: <kbd>http://djangodemo.nginxcookbook.com/?debug</kbd>), we now get the details logged in both the standard logs as well as our additional <kbd>djangodemo-debug-access.log</kbd> using the <kbd>applogs</kbd> format we defined in the previous recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the Elastic Stack</h1>
                </header>
            
            <article>
                
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="72" src="assets/a9286138-8e1e-4665-9227-9ed93b6e21c6.png" width="236"/></div>
<p>Manually digging through log files to gain insights or to detect anomalies can be very slow and time-consuming. To solve this issue, we're going to run through a quick example using the Elastic Stack (previously referred to as the <strong>Elasticsearch LogstashÂ Kibana</strong> (<strong>ELK</strong>) stack. Elasticsearch is a high-speed search engine which offers real-time indexing and searching. Data is stored as schema-less JSON documents and has an easy-to-use API for access.</p>
<p>To complement this, there's also Logstash and Kibana. Logstash is a tool that allows for the collection of logs, parses the data on the fly, and then pushes it to a storage backend such as Elasticsearch. The original creator of Logstash (Jordan Sissel) wrote it to parse web server log files such as those produced by NGINX, so it's well suited to the task.</p>
<p>Kibana is then the final part of the puzzle. Once the data has been collected, parsed, and stored, we now need to be able to read it and view it. Kibana allows you to easily visualize the data by allowing you to easily structure the filters and queries and then display the information in easy-to-read graphs and tables. As a graphical representation, it is far quicker and easier to interpret and see variances in data which may be meaningful.</p>
<p>What's more, the whole ELK stack is open source and therefore freely available to use and extend. In fact, the stack itself has expanded with tools such as Beats to provide lightweight data shipping to be processed by Logstash. Once you start using it, it's hard to revert to flat files!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To run the Elastic Stack, you can either install it on your own systems or use a cloud-based instance. For this recipe, we're going to install it on a separate server.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Elasticsearch</h1>
                </header>
            
            <article>
                
<p>To install Elastic, simply download the appropriate <kbd>.deb</kbd> or <kbd>.rpm</kbd> package for your particular instance. For the example, I'll use Ubuntu 16.04. First, we'll need to install Java version 8 (required for Elastic 5.x):</p>
<pre><strong>apt install openjdk-8-jre</strong>  </pre>
<p>Next, we'll download the latest Elastic package and install it:</p>
<pre><strong>wget https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/deb/elasticsearch/5.0.0-alpha4/elasticsearch-5.0.0-alpha4.deb </strong>
    
<strong>dpkg -i elasticsearch-5.0.0-alpha4.deb</strong>  </pre>
<div class="packt_infobox">The example is also using the 5.x release, which was in alpha during writing. Make sure you select the latest stable release for production use.</div>
<p>While Elasticsearch can become quite a complex system as you start to use it heavily, the good news is that the default configuration doesn't require any changes to get started. However, we highly recommend ensuring that you only run Elasticsearch on a private network (where possible) to avoid accidentally exposing your server to the internet. Here's what our scenario will look like:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="274" src="assets/a919d615-cf1b-4c4f-9a44-ae0e07842317.png" width="362"/></div>
<p>In our example, we have a private network on the <kbd>192.168.50.x</kbd> range for all systems, with our Elasticsearch server being <kbd>192.168.50.5</kbd>. Accordingly, we simply edit the configuration file (<kbd>/etc/elasticsearch/elasticsearch.yml</kbd>) and set the following:</p>
<pre># Set the bind address to a specific IP (IPv4 or IPv6): 
# 
network.host: 192.168.50.5  
discovery.zen.minimum_master_nodes: 1 </pre>
<p>We also set the minimum number of master nodes to <kbd>1</kbd>, so that it knows to wait for other master nodes. In a clustered system, you ideally want three or more nodes to form a quorum to ensure the data remains valid.</p>
<p>So, all we need to do is start it:</p>
<pre><strong>systemctl start elasticsearch </strong>  </pre>
<p>We can now test it quickly with a simple cURL call to ensure it works:</p>
<pre><strong>curl http://192.168.50.5:9200</strong>  </pre>
<p>The output should look similar to this:</p>
<pre>{ 
    "name" : "Nebulon", 
    "cluster_name" : "elasticsearch", 
    "version" : { 
        "number" : "5.0.0-alpha4", 
        "build_hash" : "3f5b994", 
        "build_date" : "2016-06-27T16:23:46.861Z", 
        "build_snapshot" : false, 
        "lucene_version" : "6.1.0" 
    }, 
    "tagline" : "You Know, for Search" 
} </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logstash</h1>
                </header>
            
            <article>
                
<p>With Elasticsearch set up, we can now install and configure Logstash. As Logstash will be performing the log collection role, it needs to be installed on the same server as NGINX. For larger installations, you could use Filebeat (also part of the Elastic family) to act as a lightweight log forwarder and have Logstash on its own instance to parse the logs.</p>
<p>Like Elasticsearch, we need Java 8 or higher to run Logstash. As this demo recipe is also using Ubuntu 16.04, we can install it via the following:</p>
<pre><strong>apt install openjdk-8-jre</strong>  </pre>
<p>Next, we download the latest copy of Logstash and install it:</p>
<pre><strong>wget https://download.elastic.co/logstash/logstash/packages/debian/logstash-5.0.0-alpha4.deb</strong>
<strong>dpkg -i logstash-5.0.0-alpha4.deb</strong>  </pre>
<p>Once the installation is complete, we'll now need to create the configuration file. This file will define where to look for the logs, how to parse them, and then where to send them to. While the complexity can be a bit daunting at first, it's mostly a set and forget scenario once it's working. Here's the configuration which will work for most of the configurations already outlined in this book:</p>
<pre>input { 
    file { 
        type =&gt; nginx_access 
        path =&gt; ["/var/log/nginx/*-access.log"] 
    } 
} 
 
filter { 
    grok { 
        match =&gt;{"message" =&gt; "%{COMBINEDAPACHELOG}" } 
    } 
} 
 
output { 
    elasticsearch { hosts =&gt; ["192.168.50.5:9200"] } 
    stdout { codec =&gt; rubydebug } 
} </pre>
<p>For most installations, this needs to be placed in the <kbd>/etc/logstash/conf.d/</kbd> folder with a <kbd>.conf</kbd> filename (for example, <kbd>nginx.conf</kbd>).</p>
<p>In the <kbd>input</kbd> section, we use the <kbd>file</kbd> plugin to monitor all the logs which have the naming pattern <kbd>*-access.log</kbd>. The <kbd>type</kbd> definition simply allows for easy filtering if your Elasticsearch server has logs from more than one source.</p>
<p>Then, in the <kbd>filter</kbd> section, we use the <kbd>grok</kbd> plugin to turn plain text data into a structured format. If you haven't used grok patterns before, they're similar to regular expressions and can look quite complex to start with. Because we have NGINX using the combined log format (which has already been defined as a grok pattern), the hard work has already been done for us.</p>
<p>Lastly, the <kbd>output</kbd> section defines where we're sending the data. Logstash can send to multiple sources (about 30 different ones), but, in this instance, we're simply sending it to our Elasticsearch server. The hosts setting can take an array of Elasticsearch servers, so that in a big scenario you can load balance the push of the data.</p>
<p>To start Logstash, we can simply call the standard init system, which for our example is systemd:</p>
<pre><strong>systemctl start logstash</strong>  </pre>
<p>If we now load a page on any of the monitored sites, you should now have data within Elasticsearch. We can run a simple test by querying the <kbd>logstash</kbd> index and returning a record. To do this from the command line, run the following:</p>
<pre><strong>curl http://192.168.50.5:9200/logstash-*/_search -d '{ "size": 1 }' |  python -m json.tool</strong>  </pre>
<p>I pipe the cURL command through <kbd>python</kbd> to quickly format the code; by default, it returns it in a compressed format without whitespaces. While that cuts down on the packet size, it also makes it harder to read. Here's what the output should look like:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="546" src="assets/0da4ab23-7384-4d8d-b5ee-67d1bd87b019.png" width="666"/></div>
<p>While the exact syntax won't make a lot of sense yet, the important bit is to note that our single log line has been parsed into separate fields. The power of this will become evident, once we completed the installation of Kibana.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kibana</h1>
                </header>
            
            <article>
                
<p>The last part of our stack is Kibana, which visualizes the Elasticsearch data. To install, we'll simply download and install the single package. As per our initial diagram, we're installing this on our Elasticsearch server:</p>
<pre><strong>wget https://download.elastic.co/kibana/kibana/kibana-5.0.0-alpha4-amd64.deb</strong>
<strong>dpkg -i kibana-5.0.0-alpha4-amd64.deb</strong>  </pre>
<p>As we have Elasticsearch bound to the local IP (<kbd>192.168.50.5</kbd>), we need to edit the Kibana configuration (<kbd>/etc/kibana/kibana.yml</kbd>) and set it to look for this address:</p>
<pre><strong># The URL of the Elasticsearch instance to use for all your queries.</strong>
<strong>elasticsearch.url: "http://192.168.50.5:9200"</strong>  </pre>
<p>Kibana is open to the world by default; ideally, we won't leave it in this state. Of course, one of the easiest ways to secure Kibana is by proxying it through NGINX and we'll cover this scenario in a later chapter. For now, ensure that you don't accidentally expose it to the world.</p>
<p>Let's start the Kibana service and get started:</p>
<pre><strong>systemctl start kibana</strong>  </pre>
<p>Kibana will now be accessible via a web interface on port 5601; you should see something like this:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="236" src="assets/06a74ceb-c877-410e-9bef-fd8e6bf8ef71.png" width="484"/></div>
<p>This will be a very quick crash course in the usage of Kibana, which can be complex enough to warrant a whole book in itself. To be able to visualize the data, we first need to load an index pattern into Kibana. If you click on the Discover icon (the very first at the left), it will prompt you to do so. This screen should look like the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="405" src="assets/7bcfcf40-3dee-4d0d-a14a-231d18e58bc9.png" width="450"/></div>
<p>As our configuration is very simple, we can simply accept the default settings and hit <span class="KeyPACKT"><span class="packt_screen">Create</span></span>. You should then see a list of fields and field types; this means that Kibana now knows how to sort, filter, and search for these.</p>
<p>Click on the Discover icon again and it should show you the access logs in a combined bar graph and tabular format. If you don't see anything, check the time period you're searching for, which will be set in the top right. Here's what my test data looks like:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="279" src="assets/50bc5998-7b7a-4d1c-8a4c-010abd65df1d.png" width="653"/></div>
<p>This isn't overly useful just yet, but at least we can see a quick historical view based on the hit times. To show other fields in the table, we can hover over them on the left-hand column and it should show the add button. I've added in the <span class="packt_screen">request</span>, <span class="packt_screen">response</span>, <span class="packt_screen">bytes</span>, and <span class="packt_screen">httpversion</span> to give a more detailed view like this one:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="297" src="assets/946e6db4-d66b-4ba7-8677-772b680a7cb3.png" width="779"/></div>
<p>We can start to quickly glance at the data and see if there are errors or anomalies. If we wanted to see a quick breakdown of what the requests have been, we could click on the field in the left-hand column and it would display the top five results within the search. For example, here's our demo output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="329" src="assets/a4697ad9-c6cf-44b9-a38a-50e744e04095.png" width="274"/></div>
<p>At a quick glance, we know what the most popular requests have been. We can also filter based on these values, by clicking on the magnifying glasses at the right of the dialog. The positive icon will ensure only those which match this request are shown and the negative icon will exclude anything which matches this request.</p>
<p>This is just the beginning too, as you'll see on the left-hand side, there's the ability to create visualizations, which can then be placed in a full dashboard. You can add any number of line graphs, bar graphs, and similar items based on predetermined filters and searches. Of course, you can have multiple dashboards, so that you can customize them for multiple different scenarios. As one example, here's a quick dashboard which was created in about five minutes:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="365" src="assets/abcb08f0-871a-4d88-8a52-2c778091c21c.png" width="677"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>The Elastic Stack documentation: <a href="https://www.elastic.co/guide/index.html" target="_blank"><span class="URLPACKT">https://www.elastic.co/guide/index.html</span></a></p>


            </article>

            
        </section>
    </body></html>