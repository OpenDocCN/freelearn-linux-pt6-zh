- en: Chapter 4. Optimizing Website Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most popular reasons to migrate to Nginx is striving for better
    performance. Over the years, Nginx has acquired a certain reputation of being
    a silver bullet, a speed beast. Sometimes, this reputation may harm the project,
    but it is definitely earned. In many situations, that is exactly what happens:
    you *add* Nginx to a website setup as if it is a concoction ingredient and the
    website magically becomes faster. We will not explain the basics of how to set
    up Nginx because you probably know it all pretty well. In this chapter, we are
    going to delve a little into why this happens and what are the less-known options
    that will help you squeeze more out of your website.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover these topics in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: How Nginx processes the requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nginx caching subsystems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the upstreams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some new Nginx features such as thread pools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other performance issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overwhelming majority of all performance problems people have with Nginx-powered
    websites are actually on the upstreams. We will try to at least mention some of
    the methods you may use to tackle the challenge of optimizing your upstream application
    servers, but we will concentrate on the Nginx itself mostly. You will have to
    understand the inner workings of Nginx and reverse proxying in general, and we
    are devoting a good part of the chapter to explain the principles implemented
    in Nginx that let it run around other older web servers in terms of performance.
  prefs: []
  type: TYPE_NORMAL
- en: The bad news is that you probably won't be able to optimize Nginx very much.
    If you embarked on a project of making your website sufficiently, significantly
    faster, and started with inserting Nginx between the application and the users,
    then you have probably already done the most important steps in moving towards
    your goal. Nginx is extremely optimal in the sense of avoiding doing extra, unneeded
    work, and that is the core of any optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Still, some of the configuration defaults may be too conservative for the sake
    of compatibility, and we will try to talk about this.
  prefs: []
  type: TYPE_NORMAL
- en: Why Nginx is so fast?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The question is intentionally formulated in an oversimplified way. This is
    what you might hear from your boss or client—let us migrate from old technologies
    to Nginx because it will make our website faster and users happier. The migration
    process is described in thousands of online articles and even some books, and
    we will not write about it here. Many of our readers have probably gone down that
    path several times and know the facts: first, it is usually true that websites
    get faster and second, no, it is not usually a full migration. You will rarely
    dispose of Apache completely and plug Nginx in its place. Although this "total
    conversion" also happens, most of the time you start with inserting Nginx between
    Apache and the Internet. To understand why this is okay, why this helps at all,
    and how to move forward from there, read on.'
  prefs: []
  type: TYPE_NORMAL
- en: To describe the main conceptual change that is implemented by using Nginx as
    a reverse proxy we will use, for simplicity, the processing model of Apache 1.x,
    that is, a very old piece of software written in premultithreading traditions.
    The latest Apache version, which is 2.x, may use another, slightly more efficient
    model, which is based on threads instead of processes. But in comparison to Nginx,
    those two models look very similar, and the older one is easier to understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a simple diagram of how one HTTP request-response pair is processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why Nginx is so fast?](img/B04329_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is an explanation of the diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: A user's browser opens a connection to your server using TCP.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A web server software that runs on your server and listens to a particular set
    of TCP ports, accepts the connection, dedicates a part of itself to processing
    this connection, separates this part, and returns to listening and accepting other
    incoming connections. In the case of the Apache 1.x model, the separated part
    is a child process that has been forked beforehand and is waiting in the pool.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are usually some limits in place on how many concurrent connections may
    be processed and they are enforced on this step. It is very important to understand
    that this is the part where scaling happens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This dedicated part of the web server software reads the actual request URI,
    interprets it, and finds the relevant file or any other way to generate the response.
    Maybe that would even be an error message; it doesn't matter. It starts sending
    this response into the connection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The user's browser receives the bytes of the response one by one and generates
    pixels on the user's screen. This is actually a real job and a long one. Data
    is sent over hundreds of kilometers of wires and optical fiber, emitted into the
    air as electromagnetic waves and then "condensed" by induction into current again.
    From the viewpoint of your server, most of your users are on excruciatingly slow
    networks. The web server is literally feeding those browsers large amounts of
    data through a straw.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is nothing that could be done to solve the fifth point. The last mile
    will always be the slowest link in the chain between your server and the user.
    Nginx makes a conceptual optimization on step 2 and scales much better this way.
    Let us explain that at a greater length.
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to slow client connections, a snapshot of any popular website server software
    at any particular moment in time looks like this: a couple of requests that are
    actually processed in the sense that there is some important work being done by
    the CPU, memory, and disks and then a couple of thousands of requests for which
    all processing is done, responses are already generated and are very slowly, piece
    by piece inserted into the narrow connections to the users'' browsers. Again,
    this is a simplified model, but still very adequate to explain what actually happens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement scaling on step 2, the original Apache 1.x uses a mechanism that
    is very natural for all UNIX-based systems—it forks. There are some optimizations,
    for example, in the form of having a pool of processes forked beforehand (hence,
    the "prefork" model), and Apache 2.x may use threads instead of processes (also
    with pregenerated pools and all), but the idea is the same: scaling is achieved
    by handling individual requests to a group of some OS-level entities, each of
    which is able to work on a request and then send the data to the client. The problem
    is that those entities are rather big; you don''t just need a group, but more
    like a horde of them, and most of the time, they do a very simple thing: they
    send bytes from a buffer into a TCP connection.'
  prefs: []
  type: TYPE_NORMAL
- en: Nginx and other state machine-based servers significantly optimize step 2 by
    not making big, complex OS-level processes or threads do a simple job while hogging
    the memory at the same time. This is the essence of why Nginx suddenly makes your
    website faster—it manages to slowly feed all those thousands of very bandwidth-limited
    client connections using very little memory, saving on RAM.
  prefs: []
  type: TYPE_NORMAL
- en: 'An inquisitive reader may ask the question here about why adding Nginx as a
    reverse proxy without removing Apache still saves memory and speeds up websites.
    We believe that you already should have all the knowledge to come up with the
    correct answer for that. We will mention the most important part as a hint: the
    horde of Apaches is not needed anymore because Apache only does the response generation—the
    smartest and hardest thing—while offloading the dumb job of pushing bytes to thousands
    of slow connections. The reverse proxy is acting as a proxy client on behalf of
    all the users'' browsers with the very important distinction: this client is sitting
    very close to the server and is capable of receiving the bytes of the response
    lightning fast.'
  prefs: []
  type: TYPE_NORMAL
- en: So, the secret sauce to Nginx's performance is not its magical code quality
    (although it is written very well), but the fact that it saves up on system resources,
    mostly memory, by not making huge copies of data for each individual request it
    is processing. Interestingly enough, modern operating systems all have different
    low-level mechanisms to avoid excessive copying of data. Long gone are times when
    `fork()` literally created a whole copy of all code and data. As virtual memory
    and network subsystems get more and more sophisticated, we may end up with a system
    where the state machine as a model to code tight event-processing loops won't
    be needed any more. As of now, they still bring noticeable improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing individual upstreams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may remember from previous chapters that Nginx has two main methods of generating
    a response to a request, one being very specific—reading a static file from the
    filesystem, and the other including a whole family of the so-called upstream modules.
    An upstream is an external server to which Nginx proxies the request. The most
    popular upstream is `ngx_proxy`, others are `ngx_fastcgi`, `ngx_memcached`, `ngx_scgi`,
    and so on. Because serving only static files is not usually enough for a modern
    website, upstreams are an essential part of any comprehensive setup. As we mentioned
    in the beginning of this chapter, upstreams themselves are usually the reason
    why your website has performance troubles. Your developers are responsible for
    this part because this is where all the web application processing happens. In
    the following sections, we are going to briefly describe the major stacks or platforms
    used to implement business logic on the upstream behind Nginx and the directions
    you should at least look in for clues about what to optimize.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing static files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Any web application will contain static resources that do not change and do
    not depend on the user currently using the application. Those are usually known
    as static files in webmaster parlance and consist of all the static images, CSS,
    JavaScript, and some other extra data, for example, `cross-domain.xml` files that
    are used by access control policies of the browsers. Serving the data directly
    from the application is usually supported to facilitate simple setups without
    any frontend, intermediate, accelerating server such as Nginx. Nginx's built-in
    HTTP proxy will happily serve them, and in the case of local caching, may even
    do that without any noticeable performance loss. However, such a setup is not
    recommended as a long-term solution if you strive for maximum performance.
  prefs: []
  type: TYPE_NORMAL
- en: One universal step that we feel the need to recommend (or remind of) is moving
    as much of the static data from the upstream under the control of Nginx. It will
    make your application more fragmented, but it will also be a very good performance
    optimization method trumping many of other potential and much harder to implement
    methods. If your upstreams serve static files, then you need to make them available
    as files to Nginx and serve them directly. This might be the first thing you do
    when you receive a new legacy upstream to optimize. It is also a very easy task
    to accomplish yourself or implement as a part of the whole deployment process.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing PHP backends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For several years, the modern way to run PHP applications behind Nginx front
    is the PHP-FPM or FastCGI Process Manager. As you may guess, it uses the FastCGI
    protocol and will require FastCGI upstream module in Nginx. However, when dealing
    with inherited legacy PHP websites, you may still meet the older ways of running
    the code, which will be your first candidates for optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: There is the official Apache way using the `mod_php` Apache module. This module
    embeds PHP interpreter directly into (each and every!) Apache process. Most of
    the time, you will inherit Apache websites configured to run in this way. The
    main good side of embedded interpreters is well known—the code may be saved in
    some intermediate form between requests and not reinterpreted every time. The
    `mod_php` Apache module does that wonderfully and some people call it the single
    reason why PHP gained popularity on the Web in the first place. Well, the way
    to deal with `mod_php` in 2016 is getting rid of it, together with Apache.
  prefs: []
  type: TYPE_NORMAL
- en: Many PHP codebases can be moved from `mod_php` to PHP-FPM almost effortlessly.
    After this, you will change your main Nginx upstream from HTTP proxying to directly
    speaking FastCGI protocol with your PHP scripts that are kept running and ready
    by the FPM.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, your developers will need to invest some resources into mostly restructuring
    and refactoring code to be runnable in a separate process without any help from
    Apache. One particularly difficult case is a code that relies heavily on calling
    into the Apache internals. Fortunately, this is not nearly as common in PHP codebases
    as it was in the `mod_perl` codebases. I will mention dealing with Perl-based
    websites later.
  prefs: []
  type: TYPE_NORMAL
- en: Another really old (and odd) way to run PHP is CGI scripts. Each and every web
    administrator did or will write a fair amount of temporary CGI scripts. You know,
    the kind of temporary scripts that live on and on through generations of hardware,
    managers, and business models. They rarely power parts of production that are
    user-oriented. Anyway, CGI was not popular with PHP at all because of the ubiquity
    and rather good quality of `mod_php` and Apache. Nevertheless, you may have some
    in your legacy, especially if that code had or has some chances to run on Windows.
  prefs: []
  type: TYPE_NORMAL
- en: CGI scripts are executed as separate processes for each request/response pair
    and therefore are prohibitively expensive. The only upsides of using CGI are increased
    compatibility with other Apache modules and another degree of privilege separation.
    Those are trumped by the performance compromises in all but the most exotic scenarios.
    By the way, Nginx will make a CGI-powered portion of your website significantly
    better by buffering the output and releasing the resources on the backend. You
    still have to plan the rewrite of those parts to be run as FastCGI under FPM as
    soon as possible.
  prefs: []
  type: TYPE_NORMAL
- en: PHP-FPM uses the same prefork model as does Apache 1.x and that renders some
    of the familiar knobs under your control. For example, you may configure the number
    of working processes FPM starts, the upper limit of the requests that may be processed
    by one child, and also the size of the available child processes pool. All those
    parameters may be set via the `php-fpm.conf` file, which is usually installed
    directly in `/etc` and following a good convention includes `/etc/php-fpm.d/*.conf`.
  prefs: []
  type: TYPE_NORMAL
- en: Java backends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Java ecosystem is so huge that there is a whole bookshelf devoted solely
    to different Java web servers. We cannot delve deeper into such a topic. If you
    as an administrator have never had any experience with Java web applications,
    you will be happy to know that most of the time, those apps run their own web
    servers that do not depend on Apache. This is a list of popular Java web servers
    that you may encounter inside your upstreams: Apache Tomcat, Jetty, and Jboss/WildFly.
    Java applications are usually built on top of huge and comprehensive frameworks
    that employ a web server as one of the components. Your Nginx web accelerator
    will talk to the Java upstream via normal HTTP protocol using the `ngx_proxy`
    module. All the usual `ngx_proxy` optimizations apply, therefore. See a note on
    caching later in this chapter for examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is little you can do to make a Java application perform better without
    getting your hands dirty deep inside the code. Some of the steps available from
    the level of system administration are:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right JVM. Many Java web servers support several different Java
    Virtual Machine implementations. The HotSpot JVM from Oracle (Sun) is considered
    one of the best, and you will probably start with that. But there are others;
    some of them are commercially available, for example, Azul Zing VM. They might
    provide you with a little performance boost. Unfortunately, changing JVM vendor
    is a huge step prone to incompatibility errors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tuning threading parameters. Java code is traditionally written using threads
    that are a native and natural feature of the language. JVMs are free to implement
    threads using whatever resources they have. Usually, you will have a choice of
    using either operating system-level threads or the so-called "green threads,"
    which are implemented in userland. Both approaches have advantages and disadvantages.
    Threads are usually grouped into pools, which are preforked in a fashion that
    is very similar to what Apache 1.x does with processes. There are a number of
    models that thread pools use to optimize both memory and performance, and you,
    as administrator, will be able to tune them up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimizing Python and Ruby backends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python and Ruby both built their strength as more open and clear alternatives
    to Perl and PHP in the age when web applications were already one of the dominant
    way to deploy business logic. They both started late and with a clear goal of
    being very web-friendly. There were both the `mod_python` and `mod_ruby` Apache
    modules that embedded interpreters into the Apache web server processes, but they
    quickly went out of fashion. The Python community developed the **Web Server Gateway
    Interface** (**WSGI**) protocol as a generic interface to write web applications
    regardless of deployment options. This allowed free innovation in the actual web
    server space that mostly converged on a couple of standalone WSGI servers or containers
    (such as gunicorn and uWSGI) and `mod_wsgi` Apache module. They all may be used
    to run a Python web application without changing any code.
  prefs: []
  type: TYPE_NORMAL
- en: So, it was very natural that Nginx developed its own WSGI upstream module, `ngx_wsgi`,
    which you should use to replace any other WSGI implementation. The actual migration
    path may be a little bit more complex. If the backend application used to run
    under Apache + `mod_wsgi`, then, by all means, switch to `ngx_wsgi` immediately
    and ditch Apache. Otherwise, for the sake of smoothness and stability, you may
    start with a simpler `ngx_proxy` configuration and then move to `ngx_wsgi`.
  prefs: []
  type: TYPE_NORMAL
- en: You may also encounter an application that uses long-polling (named Comet sometimes)
    and WebSockets, and runs on a special web server, for example, Tornado (of the
    FriendFeed fame). These are problems mostly because synchronous communication
    between the web server and the clients defeats the main advantage of Nginx as
    an accelerating reverse proxy—the part of the server that processes a request
    won't be made available quickly for another request by handling the byte pushing
    to the Nginx frontend. Modern Nginx supports proxying both Comet requests and
    Web Sockets, of course, but without any acceleration that you may have gotten
    used to.
  prefs: []
  type: TYPE_NORMAL
- en: The Ruby ecosystem went a slightly different way because there was (and still
    is) a so-called killer app for Ruby, that is, the Ruby on Rails web application
    framework. Most of the Ruby web applications are built on Ruby on Rails, and there
    was even a joke that it is high time to rename the whole language Ruby on Rails
    because nobody uses Ruby without those Rails. It is a wonderfully designed and
    executed web framework with many revolutionary ideas that inspired a whole wave
    of rapid application development techniques throughout the industry. It also decoupled
    the application developers from the problems of deploying their work by providing
    the web server that could be shared on the Internet right away.
  prefs: []
  type: TYPE_NORMAL
- en: The current Ruby on Rails preferred deployment options are either using Phusion
    Passenger or running a cluster of Unicorn web servers. Both options are fine for
    your task of migrating to Nginx. Phusion Passenger is a mature example of providing
    its own in-process code as it contains modules for both Apache and Nginx web servers.
    So, if you are lucky, you will switch from one to the other effortlessly. Passenger
    will still run worker processes outside of your main Nginx workers, but the module
    allows Nginx to communicate freely. It is a good example of a custom upstream
    module. See [https://www.phusionpassenger.com/library/deploy/nginx/deploy/ruby/](https://www.phusionpassenger.com/library/deploy/nginx/deploy/ruby/)
    Passenger guide for the actual instructions. Passenger may also run in the standalone
    mode exposing HTTP to the world. That is also the way Unicorn deploys Ruby applications.
    You know the way to deal with that—the universal helper `ngx_proxy`.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Perl backends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Perl was the first widely used server-side programming language for the Web.
    We may say that it is Perl that brought the notion of dynamically generated web
    pages to popularity and paved the way for the web applications galore we experience
    today. There are still plenty of Perl-powered web businesses of various sizes,
    from the behemoths such as [https://www.booking.com](https://www.booking.com)
    to smaller, feisty, ambitious startups such as DuckDuckGo. You might also have
    seen a couple of MovableType-powered blogs. This is a professional blogging platform
    developed by SixApart and then resold several times.
  prefs: []
  type: TYPE_NORMAL
- en: Perl is also the most popular language to write CGI scripts, and that is also
    the single reason why it is considered slow. CGI is a simple interface to run
    external programs from inside a web server. It is rather inefficient because it
    usually involves forking an operating system-level process and then shutting it
    down after a single request. This model plus the interpreting nature of Perl means
    that Perl CGI scripts are so suboptimal that they are used as a model of inefficient
    web development platforms.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a user-facing, dynamic web page generated by a CGI script run from
    Apache, you have to get rid of it. See below for details.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of more advanced ways to run Perl code in production. Partly
    inspired by the `mod_php` success, there is a long-running project named `mod_perl`,
    which is an Apache module embedding the Perl interpreter into Apache processes.
    It is also highly successful because it is stable and robust, and powers a lot
    of heavily loaded websites. Alas, it is also rather complex, both for the developer
    and the administrator. Another difference from the `mod_php` Apache module is
    that `mod_perl` failed to provide strong separation of environments, which is
    vital for the virtual hosting businesses.
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, if you have inherited a website based on `mod_perl`, you have several
    options. First, there might be a cheap way to move to the PSGI or FastCGI models
    that will allow you to get rid of Apache. The module Apache::Registry,which emulates
    a CGI environment inside `mod_perl,` may be a great sign of such situation. Second,
    the code may be written in a way that couples it tightly with Apache. The `mod_perl`
    module provides an interface to hook deeply into Apache's internals, which while
    providing several interesting capabilities for the developer, also makes it much
    harder to migrate. The developers will have to investigate the methods used in
    the software and make a final decision. They may decide to leave Apache + `mod_perl`
    alone and continue to use it as a heavy and over-capable process manager.
  prefs: []
  type: TYPE_NORMAL
- en: Moving CGI to `mod_perl` nowadays is never a good way forward, we do not recommend
    it.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of FastCGI managers for Perl that are similar to PHP-FPM
    described earlier. They all are very lucky options for you as the Nginx administrator
    because most of the time the migration will be smooth and easy.
  prefs: []
  type: TYPE_NORMAL
- en: One of the interesting recent modes to run Perl code in web servers is the so-called
    **Perl Server Gateway Interface** (**PSGI**). It is more or less a direct port
    of Rack architecture from the Ruby stack to Perl. It is interesting that PSGI
    was invented and implemented in the world where Nginx was already popular. Therefore,
    if you have a web application that uses PSGI, it was most probably tested and
    run behind Nginx. No need to port anything. PSGI might be the most important target
    architecture to upgrade CGI or the `mod_perl` applications.
  prefs: []
  type: TYPE_NORMAL
- en: Bigger Perl web frameworks usually have a number of ways to run the applications.
    For example, both Dancer and the older Catalyst provide the glue scripts to run
    the same application as a separate web server (which you might expose to the world
    with the help of the Nginx `ngx_proxy` upstream), as a `mod_perl` application
    or even as a CGI script. Not all of those methods are suitable for production,
    but they will definitely help in migration. Never accept "we should rewrite everything
    from scratch" as a recommendation from the developers before weighing other options.
    If the application was written during the last 3–4 years, it should definitely
    have PSGI implemented directly or via its framework.
  prefs: []
  type: TYPE_NORMAL
- en: PSGI applications are run with the help of special PSGI servers, such as Starman
    or Starlet, that speak simple HTTP to the outside world. Nginx will use the `ngx_proxy`
    upstream for such applications.
  prefs: []
  type: TYPE_NORMAL
- en: Using thread pools in Nginx
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using asynchronous, event-driven architecture serves Nginx well as it allows
    to save up on the precious RAM and CPU context switches while processing thousands
    and millions of slow clients in separate connections. Unfortunately, event loops,
    such as the one that power Nginx, easily fail when facing blocking operations.
    Nginx was born on FreeBSD, which has several advantages over Linux, and one of
    the relevant ones is a robust, asynchronous input/output implementation. Basically,
    the OS kernel is able to not block on traditionally blocking operations like reading
    data from disks by having its own kernel-level background threads. Linux, on the
    other hand, requires more work from the application side, and very recently, in
    version 1.7.11, the Nginx team released its own thread pools feature to work better
    on Linux. You may find a good introduction into the problem and the solution in
    this official Nginx blog post at [https://www.nginx.com/blog/thread-pools-boost-performance-9x/](https://www.nginx.com/blog/thread-pools-boost-performance-9x/).
    We will provide an example of the configuration you may use to turn on thread
    pools on your web server. Remember that you will only need this on Linux.
  prefs: []
  type: TYPE_NORMAL
- en: 'To turn on background threads that will perform blocking input/output operations
    without stalling the main loop you use the directive `aio` in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You may know the `aio` directive that is used to turn on the Async IO interface,
    so it is a natural fit for its use to be extended this way.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation is rather simple to explain from a very high level. Transparently
    to you, Nginx will run a number (pool) of background, userland-level threads that
    fulfill the input/output tasks. Nginx will continue to run the main event loop
    in parallel to waiting for the slow disk or the network.
  prefs: []
  type: TYPE_NORMAL
- en: The caching layer of Nginx
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If there is one universally known and acclaimed algorithm to speed things up,
    it is caching. Pragmatically speaking, caching is a process of not doing the same
    work many times. Ideally, each distinct computational unit should be executed
    once. This, of course, never happens in the real world. Still, techniques to minimize
    repetitions by rearranging work or using saved results are very popular. They
    form a huge discipline named "dynamic programming."
  prefs: []
  type: TYPE_NORMAL
- en: In the context of a web server, caching usually means saving the generated response
    in a file so that the next time when the same request is received; it could be
    processed by reading this file and not computing the response again. Now please
    refer to the steps outlined in the first section of this chapter. For many of
    the real-world websites, the actual computing of the responses is not the bottleneck;
    transferring those responses to the slow clients is. That's why the most efficient
    caching happens right in the browser, or as developers prefer to say, on the client
    side.
  prefs: []
  type: TYPE_NORMAL
- en: Emitting caching headers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All browsers (and even many non-browser HTTP clients) support client-side caching.
    It is a part of the HTTP standards, albeit one of the most complex to understand.
    Web servers do not control client-side caching to full extent, obviously, but
    they may issue recommendations about what to cache and how, in the form of special
    HTTP response headers. This is a topic thoroughly discussed in many great articles
    and guides, so we will mention it shortly, and with a lean towards problems you
    may face and how to troubleshoot them.
  prefs: []
  type: TYPE_NORMAL
- en: In spite of the fact that browsers have been supporting caching on their side
    for at least 20 years, configuring cache headers was always a little confusing,
    mostly due to the fact that there are two sets of headers designed for the same
    purpose, but having different scopes and totally different formats.
  prefs: []
  type: TYPE_NORMAL
- en: There is the `Expires:` header, which was designed as a quick and dirty solution
    and also the new (relatively) almost omnipotent `Cache-Control:` header, which
    tries to support all the different ways an HTTP cache could work.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an example of a modern HTTP request-response pair containing the caching
    headers. These are the request headers sent from the browser (here Firefox 41,
    but it does not matter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the response headers are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We highlighted the parts that are relevant. Note that some directives may be
    sent by both sides of the conversation. The browser sent the `Cache-Control: max-age=0`
    header because the user pressed the *F5* key. This is an indication that the user
    wants to receive a response that is fresh. Normally, the request will not contain
    this header and will allow any intermediate cache to respond with a stale but
    still nonexpired response.'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the server we talked to responded with a gzipped HTML page encoded
    in UTF-8 and indicated that the response is okay to use for half an hour. It used
    both mechanisms available, the modern `Cache-Control:max-age=1800` header and
    the very old `Expires:Sun, 10 Oct 2015 14:12:34 GMT` header.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `X-Cache: "EXPIRED"` header is not a standard HTTP header, but was also
    probably (there is no way to know for sure from the outside) emitted by Nginx.
    It may be an indication that there are, indeed, intermediate caching proxies between
    the client and the server, and one of them added this header for debugging purposes.
    The header may also show that the backend software uses some internal caching.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another possible source of this header is a debugging technique used to find
    problems in the Nginx cache configuration. The idea is to use the cache hit or
    miss status, which is available in one of the handy internal Nginx variables as
    a value for an extra header, and then you are able to monitor the status from
    the client side. This is the code that will add such a header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Nginx has a special directive that transparently sets up both standard cache
    control headers, and it is named `expires`. This is a piece of the `nginx.conf`
    file using the `expires` directive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The pattern uses the so-called noncapturing parentheses, which is a feature
    first appeared in Perl regular expressions. The effect of this regexp is the same
    as that of a simpler `\.(css|js)$` pattern, but the regular expression engine
    is specifically instructed not to create a variable containing the actual string
    from inside the parentheses. This is a simple optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the `expires` directive declares that the content of the `css` and `js`
    files will expire after a year of storage. The actual headers as received by the
    client will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The last two lines contain the same information in wildly different forms. The
    `Expires:` header is exactly one year after the date in the `Date:` header, whereas
    `Cache-Control:` specifies the age in seconds so that the client can do the date
    arithmetics itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last directive in the provided configuration extract adds another `Cache-Control:`
    header with a value of `public`. What this means is that the content of the HTTP
    resource is not access-controlled and therefore may be cached not only for one
    particular user but also anywhere else. A simple and effective strategy that was
    used in offices to minimize consumed bandwidth was to have an office-wide caching
    proxy server. When one user requested a resource from a website on the Internet
    and that resource had a `Cache-Control: public` designation, the company cache
    server would store that to serve to other users on the office network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This may not be as popular today due to cheap bandwidth, but because history
    has a tendency to repeat itself, you need to know how and why `Cache-Control:
    public` works.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Nginx `expires` directive is surprisingly expressive. It may take a number
    of different values. See this table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `off` | This value turns off the Nginx cache headers logic. Nothing will
    be added, and more importantly, the existing headers received from upstreams will
    not be modified. |'
  prefs: []
  type: TYPE_TB
- en: '| `epoch` | This is an artificial value used to purge a stored resource from
    all caches by setting the `Expires` header to **"1 January, 1970 00:00:01 GMT"**.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `max` | This is the opposite of the "epoch" value. The **Expires** header
    will be equal to **"31 December 2037 23:59:59 GMT"**, and the **Cache-Control
    max-age** set to 10 years. This basically means that the HTTP responses are guaranteed
    to never change, so clients are free to never request the same thing twice and
    may use their own stored values. |'
  prefs: []
  type: TYPE_TB
- en: '| Specific duration | An actual specific duration value means an expiry deadline
    from the time of the respective request. For example, `expires 10w`. A negative
    value for this directive will emit a special header `Cache-Control: no-cache`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `"modified" specific time` | If you add the keyword "modified" before the
    time value, then the expiration moment will be computed relatively to the modification
    time of the file that is served. |'
  prefs: []
  type: TYPE_TB
- en: '| `"@" specific time` | A time with an @ prefix specifies an absolute time-of-day
    expiry. This should be less than 24 hours. For example, Expires @17h;. |'
  prefs: []
  type: TYPE_TB
- en: Many web applications choose to emit the caching headers themselves, and this
    is a good thing. They have more information about which resources change often
    and which never change. Tampering with the headers that you receive from the upstream
    may or may not be a thing you want to do. Sometimes, adding headers to a response
    while proxying it may produce a conflicting set of headers and therefore create
    unpredictable behavior.
  prefs: []
  type: TYPE_NORMAL
- en: The static files that you serve with Nginx should have the `expires` directive
    in place. However, the general advice about upstreams is to always examine the
    caching headers you get and refrain from overoptimizing by setting up a more aggressive
    caching policy.
  prefs: []
  type: TYPE_NORMAL
- en: The corporate caching proxy configuration that we described earlier in this
    chapter together with an erroneous `public` caching policy on nonpublic resources
    may result in a situation where some users will see pages that were generated
    for other users behind the same caching proxy. The way to make that happen is
    surprisingly easy. Imagine that your client is a book shop. Their web application
    serves both public pages with book details, cover images, and so on and private
    resources with recommendation pages and the shopping cart. Those will probably
    have the same URL for all users and once, by mistake, declared as `public` with
    the expiration date in the distant future, they may freely be cached by intermediate
    proxies. Some more intelligent proxies will automatically notice cookies and either
    add them to the cache key or refrain from caching. But then again, less sophisticated
    proxies do exist, and there are a number of reports when they do show pages that
    belong to other people.
  prefs: []
  type: TYPE_NORMAL
- en: There are even techniques such as adding a random number to all URLs to defeat
    such caching configurations by making all URLs unique.
  prefs: []
  type: TYPE_NORMAL
- en: We would also like to describe a combination of unique URLs and long expiration
    dates, which are widely used today. Modern websites are very dynamic, both in
    the sense of what happens to the document after it is loaded and how often the
    client-side code changes. It is not unusual to have not only daily but even hourly
    releases. This is a luxury of the web as an application delivery mechanism, and
    people seize the opportunity. How to combine rapid releases with caching? The
    first idea was to code the version into the URLs. It works surprisingly well.
    After each release, all the URLs change; the old ones start to slowly expire in
    the cache stores of different levels, whereas the new ones are requested directly
    from the origin server.
  prefs: []
  type: TYPE_NORMAL
- en: One clever trick was developed upon this scheme, and it uses a hash of the content
    of the resource instead of the version number as a unique element of the URL.
    This reduces extra cache misses when a new release does not change all the files.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing this trick is done on the application side. Nginx administrator
    is only responsible for setting up long expiration date by using, for example,
    the `expires max` directive.
  prefs: []
  type: TYPE_NORMAL
- en: The one obvious thing that limits the effect of the client-side caching is that
    many different users may issue the same or similar requests, and those will all
    reach the web server. The next step to never doing the same work many times is
    caching on the server.
  prefs: []
  type: TYPE_NORMAL
- en: Caching in Nginx upstream modules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Caching infrastructure is implemented as a part of the upstream interface if
    you excuse us to use object-oriented programming terminology. Each of those upstream
    modules has a group of very similar directives, which allow you to configure the
    local caching of responses from that particular upstream.
  prefs: []
  type: TYPE_NORMAL
- en: The basic scheme is very simple—once a request is determined as an upstream
    material, it is rerouted to the relevant module. If there's caching configured
    for that upstream, the cache is first searched for an existing response to this
    request. Only when a cached response is not found, the actual proxying is performed.
    After this, the newly generated response is saved into the cache while being sent
    to the client.
  prefs: []
  type: TYPE_NORMAL
- en: It is interesting that while caching on the reverse proxy is known for a while,
    Nginx gained its fame as a magical accelerator without implementing it. The reason
    should be evident from the first section—radical changes in RAM consumption alone
    brought a lot of performance gains. Until the introduction of version 0.7.44,
    Nginx did not have any caching facilities built in. At that time, web administrators
    used either the famous squid HTTP proxy for caching or the `mod_accel` module
    for Apache. By the way, `mod_accel` module was created by Nginx's author Igor
    Sysoev and turned out to be the testbed for all the ideas about proper reverse
    proxying that were later implemented in Nginx.
  prefs: []
  type: TYPE_NORMAL
- en: Let us examine the caching directives of the most popular upstream module, `ngx_proxy`.
    Just to remind, this module hands over the request to another HTTP server. This
    is exactly how Nginx is run as a reverse proxy in front of Apache, for example.
    The full description is available in the great Nginx documentation at [http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache](http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache).
    We won't repeat the documentation, but we will provide additional facts and ideas
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: '| Directive | Additional information |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `proxy_cache_path` | This directive is clearly the main one of the whole
    caching family. It specifies the storage parameters of the cache store starting
    with the path on the filesystem. You should definitely familiarize yourself with
    all the options. The most important are the `inactive` and `max_size` options,
    which control how the Nginx cache manager removes unused data from the cache store.
    One required parameter in this directive is the `keys_zone`, which links the cache
    store to the "zone". See in the later text. |'
  prefs: []
  type: TYPE_TB
- en: '| `proxy_cache` | This is the main switch directive. It is required if you
    want any caching. It has a single somewhat cryptic parameter named "zone," which
    will be explained in detail further on. The value "off" will switch the caching
    off. It may be needed in cases when there is a `proxy_cache` directive further
    up the scope stack. |'
  prefs: []
  type: TYPE_TB
- en: '| `proxy_cache_bypass` | This directive allows you to easily specify conditions
    on which some responses will never be cached. |'
  prefs: []
  type: TYPE_TB
- en: '| `proxy_cache_key` | This directive creates a key that is used to identify
    objects in the cache. By default, the URL is used, but people add things to it
    quite commonly. Different responses should never have equal keys. Anything that
    may change the content of the page should be in the key. Besides obvious cookie
    values, you may want to add the client IP address if your pages depend on it (for
    example, use some form of geotargeting via the GeoIP database). |'
  prefs: []
  type: TYPE_TB
- en: '| `proxy_cache_lock` | This is a binary on/off switch defaulting to off. If
    you turn it on, then simultaneous requests for the same ("same" here means "having
    the same cache key") resource will not be run in parallel. Only the first request
    will be executed while the rest are blocked waiting.The `proxy_cache_lock_*` family
    of directives might be interesting when you have some very expensive responses
    to generate. |'
  prefs: []
  type: TYPE_TB
- en: '| `proxy_cache_lock_age``proxy_cache_lock_timeout` | These two specify additional
    lock parameters. Refer to the documentation for details. |'
  prefs: []
  type: TYPE_TB
- en: '| `proxy_cache_methods` | This is a list of HTTP methods that are cacheable.
    Besides the obvious "GET" and "HEAD" methods, you might want to sometimes cache
    less popular methods such as "OPTIONS" or "PROPFIND" from WebDAV. There might
    be cases when you want to cache responses even to "POST", "PUT," and "DELETE"
    although that would be a very serious bending of the rules and you should really
    know what you are doing. |'
  prefs: []
  type: TYPE_TB
- en: '| `proxy_cache_min_uses` | This numeric parameter with a default value of "1"
    may be useful to optimize huge cache stores by not caching responses to rare requests.
    Remember that the effective cache is not the one that stores more but the one
    that stores useful things that get requested again and again. |'
  prefs: []
  type: TYPE_TB
- en: '| `proxy_cache_purge` | This directive specifies the additional conditions
    on which objects are deleted from the cache store before expiration. It may be
    used as a way to forcefully invalidate a cache entry. A good cache key design
    should not require invalidation, but we all know how often good designs of anything
    happen in real life. |'
  prefs: []
  type: TYPE_TB
- en: '| `proxy_cache_revalidate` | This is also a Boolean directive. HTTP conditional
    requests with headers "If-None-Match" or "If-Modified-Since" may update the validity
    of objects in the cache even if they do not return any new content to the requesting
    client. For this, specify "on". |'
  prefs: []
  type: TYPE_TB
- en: '| `proxy_cache_use_stale` | This is an interesting directive that sometimes
    allows responding with an expired response from the cache. The main case to do
    this is an upstream failure. Sometimes, responding with a stale content is better
    than rejecting the request on the basis of the famous "Internal server error".
    From the user''s point of view, this is very often the case. |'
  prefs: []
  type: TYPE_TB
- en: '| `proxy_cache_valid` | This is a very rough cache expiration specification.
    Usually, you should control the validity of the cached data via response headers.
    However, if you need something quick or something broad, this directive will help
    you. |'
  prefs: []
  type: TYPE_TB
- en: One very important concept that is used in caching subsystems throughout all
    the upstream modules is that of the cache zone. A zone is a named memory region,
    which is accessible by its name from all Nginx processes. Readers familiar with
    the concept of System V-shared memory or IPC via mmap-ed regions will instantly
    see the similarity. Zones were chosen as an abstraction for the cache state storage,
    which should be shared between all the worker processes. You may configure many
    caches inside your Nginx instance, but you will always specify a zone for each
    cache. You may link different caches to the same zone, and the information about
    the cached objects will be shared. Zones also act as objects encapsulating the
    actual cache storage configuration such as where on the filesystem the cached
    objects will persist, how the storage hierarchy will be organized, when to purge
    the expired objects, and how to load the objects from disk into memory on restart.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, an administrator first sets up at least one zone with all the
    relevant storage parameters with the directive `*_cache_path` and then plugs subtrees
    of the whole URL space into those zones with the directive `*_cache`.
  prefs: []
  type: TYPE_NORMAL
- en: Zones are set up globally, usually in the `http` scope while individual caches
    are linked to zones with the simple `*_cache` directive in the relevant contexts,
    for example, locations down the path tree or the whole server blocks.
  prefs: []
  type: TYPE_NORMAL
- en: We should remind you that the described caching subsystem directives' family
    exists for all the upstream modules of Nginx. You will substitute `proxy_` for
    the other upstream moniker to end up with a whole other family of directives that
    do exactly the same, maybe with some slight variations for responses generated
    by upstreams of another type. For example, here for the information on how to
    cache FastCGI responses at [http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html#fastcgi_cache](http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html#fastcgi_cache).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us provide some real-world caching configuration examples that will help
    you grasp the idea better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This is a canonically simple cache configuration with one zone named `cache1`
    and one cache configured under location `/` in one server. Several important details
    are worth mentioning. The temporary files directory configured with the `proxy_temp_path`
    directive is highly recommended to be on the same filesystem as the main cache
    storage because otherwise, Nginx will not be able to quickly move files between
    the temporary and permanent storage and will instead perform an expensive file
    copy operation.
  prefs: []
  type: TYPE_NORMAL
- en: The `key_zone` size specifies the amount of memory dedicated to the zone. This
    memory is used to store the keys and metainformation about the objects in the
    cache and not the actual cached responses (objects). The limit on the object storage
    is specified in the `max_size` parameter. Nginx spawns a separate process named
    `cache manager`, which will constantly scan all the cache zones and remove the
    least used objects when the `max_size` is exceeded.
  prefs: []
  type: TYPE_NORMAL
- en: The `proxy_cache_valid` directive combination specifies a much shorter period
    of validity for the negative 404 results. The idea behind it is that 404 might
    actually be fixed, at least some of them may appear due to some misconfiguration.
    It makes sense to retry such requests more frequently. You should also consider
    the load on the upstream when making decisions about validity periods. Many computationally
    heavy search algorithms require much more resources to give a negative answer.
    It is quite understandable that to make sure that a looked for entity is absent
    may require checking everywhere instead of stopping after the first found instance.
    This is a very simplified description of a search algorithm, but it is short enough
    so that you will remember to always check the request processing time in the logs
    for negative responses and their relative amount before shortening the cache validity
    interval.
  prefs: []
  type: TYPE_NORMAL
- en: Two important parameters of the cache are left out in the above configuration,
    and this means that you will fly with default values. The `proxy_cache_methods`
    defaults to only caching GET and HEAD requests, which may not be optimal for your
    web application. And `proxy_cache_key` defaults to `$scheme$proxy_host$request_uri,`
    which may be dangerous if your web application make similar requests for different
    users. Read about these directives and either add uniqueness to the key or fall
    back to uncached behavior via `proxy_cache_bypass`.
  prefs: []
  type: TYPE_NORMAL
- en: Another example that we would like to present is much more complex. Let us devote
    a separate section to it.
  prefs: []
  type: TYPE_NORMAL
- en: Caching static files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When scaling a website horizontally, you will inevitably find yourself in the
    situation of having many identical Nginx-powered servers behind a low-level balancer.
    All of them will proxy the requests to the same upstream server farm, and there
    will be no problems with synchronizing the active, dynamic content served by your
    website. But if you follow the advice about having all the static content present
    locally to allow Nginx to serve it in the most native and efficient way possible,
    you will end up with a task of having many identical copies of the same files
    everywhere.
  prefs: []
  type: TYPE_NORMAL
- en: The other way to do the same task is a setup where a farm of Nginx instances
    is used to serve a huge library of static files, for example, video or music.
    Having a copy of that library on each Nginx node is out of the question because
    it is too big.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, there are many possible solutions for these two cases. One choice
    is having a secondary smaller farm of Nginx servers serving the files to the main
    farm, which will employ caching inside the `ngx_proxy` upstream.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting solution uses a network filesystem mounted on the nodes.
    The traditional Unix NFS has a bad reputation, but in reality, on current Linux
    kernels, it is stable enough to be used in production. Two of the alternatives
    are AFS and SMBFS. The files under the mount point will look local to Nginx, but
    they will still be downloaded over the network, which is much slower than reading
    a good, local SSD. Luckily, modern Linux kernels have the ability to locally cache
    files from the NFS and AFS. It is named FS-Cache and uses a separate userland
    daemon, `cachefilesd`, to store local copies of files from a network filesystem.
    You may read about FS-Cache at [https://people.redhat.com/dhowells/fscache/FS-Cache.pdf](https://people.redhat.com/dhowells/fscache/FS-Cache.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: FS-Cache configuration is rather straightforward, and we will not focus on it.
    There is another way to do it, which follows the philosophy of Nginx much more
    closely. SlowFS is a third-party, upstream-like module for Nginx, which provides
    a simple interface to a filesystem subtree. The interface includes caching capabilities,
    which are standard to all other Nginx upstreams.
  prefs: []
  type: TYPE_NORMAL
- en: '**SlowFS** is open source under a very permissive license and is available
    either from the author''s website or directly from GitHub as a repository. Refer
    to [http://labs.frickle.com/nginx_ngx_slowfs_cache](http://labs.frickle.com/nginx_ngx_slowfs_cache).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example SlowFS configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This configuration installs a transparent caching layer over files available
    locally in `/var/www/nfs`. It does not matter how these files are actually stored,
    they still will be cached according to the parameters specified with the `slowfs_*`
    family of directives. But obviously, you will only note any speed-up if `/var/db/cache`
    is much faster than `/var/www/nfs`.
  prefs: []
  type: TYPE_NORMAL
- en: Replacing external redirects with internal ones
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As modern frontend frameworks grow more and more complex, there is an alarming
    rise in the number of the so-called client-side redirects. Nginx has a great facility
    that will allow you to save some traffic and precious client waiting time on client
    redirects. First, let us briefly refresh your knowledge of those redirects.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the HTTP responses are documents consisting of three principal parts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s the HTTP code (200: Ok, 404: Not found, and so on)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a number of loosely structured key-value pairs in the form of headers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a relatively large, opaque, optional body
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a lot of good HTTP response codes documentation on the Internet (and
    also some hilarious pieces given at [http://httpstatusdogs.com/](http://httpstatusdogs.com/))—the
    ones that are relevant to our discussion are in the fourth hundred, that is, between
    300 and 399.
  prefs: []
  type: TYPE_NORMAL
- en: Responses with those codes are indications that a browser should immediately
    make another request instead of the original one. This is why they are called
    redirects. The semantic differences between various 3xx codes are less important
    here.
  prefs: []
  type: TYPE_NORMAL
- en: What is important is that many redirects are superfluous. HTTP clients (for
    example, browsers) spend time on redirects that serve no particular reason besides
    cleaning up the URL in the address bar. Does Yahoo really need to redirect me
    from `yahoo.de` to `ru.yahoo.com`, `www.yahoo.com`, and [https://www.yahoo.com](https://www.yahoo.com)
    by making my browser issue three additional requests that could easily be avoided?
    If a website under your control does such things, you may address the question
    to the respective developers. You may also suggest an easy fix; see later in the
    text.
  prefs: []
  type: TYPE_NORMAL
- en: There is a cool, little web service that allows you to see the redirects chain
    as well as some other metainformation that may be useful for debugging. It may
    be referred to at [https://httpstatus.io/](https://httpstatus.io/).
  prefs: []
  type: TYPE_NORMAL
- en: You may go and check whether some of your websites make unneeded redirects,
    which may cost your slow mobile users' precious seconds before they actually get
    to the content of your site.
  prefs: []
  type: TYPE_NORMAL
- en: '![Replacing external redirects with internal ones](img/B04329_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Nginx has a feature named "internal redirects". The idea is that all the intermediate
    HTTP request-response pairs are processed right inside the server. The client
    gets the content from the end of the chain in response to the original request.
    There are a number of methods to enable internal redirects in Nginx, but probably
    the most flexible is the `X-Accel-Redirect` response header that an upstream behind
    Nginx may generate.
  prefs: []
  type: TYPE_NORMAL
- en: For the internal redirects to work with this method, you will have to change
    the configuration of your upstream software. Instead of generating true redirects
    via HTTP 3xx response codes coupled with the `Location:` response header, you
    will have to generate the earlier-mentioned `X-Accel-Redirect:` header. This is
    literally the only change you will have to make. There are a number of places
    where you need to be careful; all of them concerning the security model of the
    browsers. The geographic redirects as shown with the Yahoo! example are actually
    quite rare nowadays, so optimizing them may not be worth the troubles you will
    get by issuing cookies on the wrong domain. But the `example.com` to `www.example.com`
    redirects are still very popular and look like perfect candidates for internal
    redirects.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed several approaches to finding performance problems
    in your Nginx installation. We mostly focused on working with legacy websites
    that you might have inherited and are optimizing. The reason for this is that
    Nginx in itself rarely has any specific problems with being fast enough.
  prefs: []
  type: TYPE_NORMAL
- en: As an operations specialist, you increased your value for the business by gaining
    knowledge on how to speed up existing working websites having load and customers
    but based on some pre-Nginx technologies that were a limiting factor.
  prefs: []
  type: TYPE_NORMAL
