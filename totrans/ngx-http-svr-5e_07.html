<html><head></head><body>
<div id="_idContainer044">
<h1 class="chapter-number" id="_idParaDest-142"><a id="_idTextAnchor653"/><span class="koboSpan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-143"><a id="_idTextAnchor654"/><span class="koboSpan" id="kobo.2.1">Introduction to Load Balancing and Optimization</span></h1>
<p><span class="koboSpan" id="kobo.3.1">As much as NGINX will help your servers hold the load, there are always limits to what a single machine can process; an aging hard drive or limited bandwidth will eventually induce a bottleneck, resulting in longer request-serving times, which, in turn, leads to the disappointment of </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">your visitors.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">As your websites grow more popular and your single machine begins to suffer, you will be tempted to simply get a bigger and more expensive server. </span><span class="koboSpan" id="kobo.5.2">But this would not be a cost-efficient approach in the long run, and remember that the more strain a server is exposed to, the more likely it is to suffer from </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">hardware failure.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">In this chapter, we will investigate two concepts, the first of which is load balancing: the art of distributing a load across several servers and managing this distribution efficiently. </span><span class="koboSpan" id="kobo.7.2">The second part will explore the subject of thread pools: a new mechanism relieving servers under heavy loads (more specifically, loads induced by blocking operations) by serving requests in a slightly </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">different manner.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">This chapter covers the </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.11.1">Introducing </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">load balancing</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Using NGINX as a TCP </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">load balancer</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Exploring thread pools and </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">I/O mechanisms</span></span><a id="_idTextAnchor655"/></li>
</ul>
<h1 id="_idParaDest-144"><a id="_idTextAnchor656"/><span class="koboSpan" id="kobo.17.1">Introducing load balancing</span></h1>
<p><span class="koboSpan" id="kobo.18.1">All of the most visited websites in the world are built over carefully planned server architectures; fast page loads and download speeds are a requirement for long-term traffic growth. </span><span class="koboSpan" id="kobo.18.2">The concept</span><a id="_idIndexMarker439"/><span class="koboSpan" id="kobo.19.1"> of </span><strong class="bold"><span class="koboSpan" id="kobo.20.1">load balancing</span></strong><span class="koboSpan" id="kobo.21.1"> has the potential to solve problems pertaining to scalability, availability, and performance. </span><span class="koboSpan" id="kobo.21.2">After a quick description of the concept, we will elaborate on how NGINX offers to implement such </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">an architecture.</span></span><a id="_idTextAnchor657"/></p>
<h2 id="_idParaDest-145"><a id="_idTextAnchor658"/><span class="koboSpan" id="kobo.23.1">Understanding the concept of load balancing</span></h2>
<p><span class="koboSpan" id="kobo.24.1">To put it simply, the concept of </span><a id="_idIndexMarker440"/><span class="koboSpan" id="kobo.25.1">load balancing consists of distributing the workload (CPU load, hard disk load, or other forms) across several servers, in a manner that is completely transparent to </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">your visitors.</span></span></p>
<p><span class="koboSpan" id="kobo.27.1">In the case of a single-server architecture, client requests are received and processed by one machine. </span><span class="koboSpan" id="kobo.27.2">A machine has a limited capacity of operation; for example, a web server that is able to respond to 1,000 HTTP requests per second. </span><span class="koboSpan" id="kobo.27.3">If the server receives more than 1,000 requests per second, the 1,001st client request received in that second will not be served in a timely manner. </span><span class="koboSpan" id="kobo.27.4">From then on, page-serving speeds would begin to increase, resulting in a degraded experience for </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">its visitors:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer039">
<span class="koboSpan" id="kobo.29.1"><img alt="Figure 7.1: An example of how request tops are managed" src="image/B21787_07_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.30.1">Figure 7.1: An example of how request tops are managed</span></p>
<p><span class="koboSpan" id="kobo.31.1">Distributing a load across several servers increases the overall request-serving capacity; with two servers at your disposal, you could theoretically allow 2,000 HTTP requests to be served per second. </span><span class="koboSpan" id="kobo.31.2">With three servers, you could serve 3,000 requests, and </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">so on.</span></span></p>
<p><span class="koboSpan" id="kobo.33.1">There are several techniques available for achieving load balancing, with </span><strong class="bold"><span class="koboSpan" id="kobo.34.1">DNS load balancing</span></strong><span class="koboSpan" id="kobo.35.1"> being </span><a id="_idIndexMarker441"/><span class="koboSpan" id="kobo.36.1">one of the most commonly implemented techniques. </span><span class="koboSpan" id="kobo.36.2">When a person wishes to visit your website, their web browser will resolve your domain name (</span><strong class="source-inline"><span class="koboSpan" id="kobo.37.1">example.com</span></strong><span class="koboSpan" id="kobo.38.1">) into an IP address (</span><strong class="source-inline"><span class="koboSpan" id="kobo.39.1">1.2.3.4</span></strong><span class="koboSpan" id="kobo.40.1">). </span><span class="koboSpan" id="kobo.40.2">To achieve DNS load balancing, simply associate multiple IP addresses to your domain. </span><span class="koboSpan" id="kobo.40.3">Upon visiting your website, the operating systems of your visitors will select one of these IP addresses following</span><a id="_idIndexMarker442"/><span class="koboSpan" id="kobo.41.1"> a </span><strong class="bold"><span class="koboSpan" id="kobo.42.1">simple round-robin</span></strong><span class="koboSpan" id="kobo.43.1"> algorithm, thus ensuring that on a global scale, all of your servers receive more or less the same amount </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">of traffic:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer040">
<span class="koboSpan" id="kobo.45.1"><img alt="Figure 7.2: Another example of how request tops are managed" src="image/B21787_07_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.46.1">Figure 7.2: Another example of how request tops are managed</span></p>
<p><span class="koboSpan" id="kobo.47.1">Albeit simple to implement, this </span><a id="_idIndexMarker443"/><span class="koboSpan" id="kobo.48.1">load-balancing method cannot always be applied to high-traffic websites because it has several </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">major issues:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.50.1">What if the IP address selected by a visitorâ€™s operating system points to a server that is </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">temporarily unavailable?</span></span></li>
<li><span class="koboSpan" id="kobo.52.1">What if your architecture is made of several types of servers, some of which are capable of handling more requests </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">than others?</span></span></li>
<li><span class="koboSpan" id="kobo.54.1">What if a visitor connects to a particular server and logs in to their user account, only to get switched to another server 10 minutes later, losing their </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">session data?</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.56.1">The last of these issues is also known as the </span><strong class="bold"><span class="koboSpan" id="kobo.57.1">session affinity</span></strong><span class="koboSpan" id="kobo.58.1"> problem and is further detailed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">next sectio</span><a id="_idTextAnchor659"/><span class="koboSpan" id="kobo.60.1">n</span><a id="_idTextAnchor660"/><span class="koboSpan" id="kobo.61.1">.</span></span></p>
<h2 id="_idParaDest-146"><a id="_idTextAnchor661"/><span class="koboSpan" id="kobo.62.1">Session affinity</span></h2>
<p><span class="koboSpan" id="kobo.63.1">Session affinity</span><a id="_idIndexMarker444"/><span class="koboSpan" id="kobo.64.1"> is an expression that designates the persistent assignment of a client to a particular server in a load-balanced infrastructure. </span><span class="koboSpan" id="kobo.64.2">We use the word </span><em class="italic"><span class="koboSpan" id="kobo.65.1">session</span></em><span class="koboSpan" id="kobo.66.1"> to describe a set of requests performed by a client to a server. </span><span class="koboSpan" id="kobo.66.2">When a visitor browses a website, they often visit more than one page: they log in to their account, they add a product to their shopping cart, they check out, and so on. </span><span class="koboSpan" id="kobo.66.3">Until they close their web browser (or a tab), all of their subsequent page views are part of a session, which is most of the time stateful: the server conserves data relative to the operations performed during the visit. </span><span class="koboSpan" id="kobo.66.4">In our example, that server would remember the contents of the shopping cart and the </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">login credentials.</span></span></p>
<p><span class="koboSpan" id="kobo.68.1">If, at some point during the session, the visitor were to switch servers and connect to </span><strong class="bold"><span class="koboSpan" id="kobo.69.1">Server B</span></strong><span class="koboSpan" id="kobo.70.1">, they would lose any session information contained on </span><strong class="bold"><span class="koboSpan" id="kobo.71.1">Server A</span></strong><span class="koboSpan" id="kobo.72.1">. </span><span class="koboSpan" id="kobo.72.2">The visitor would then lose the contents of their shopping cart, as well as their login credentials (they would get </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">logged out):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer041">
<span class="koboSpan" id="kobo.74.1"><img alt="Figure 7.3: When switching the backend server, we might lose the existing session" src="image/B21787_07_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.75.1">Figure 7.3: When switching the backend server, we might lose the existing session</span></p>
<p><span class="koboSpan" id="kobo.76.1">For that reason, it is of utmost importance to maintain </span><a id="_idIndexMarker445"/><span class="koboSpan" id="kobo.77.1">session affinity: in other words, to ensure that a visitor remains assigned to a particular server at all times. </span><span class="koboSpan" id="kobo.77.2">The DNS load-balancing method does not ensure session affinity, but fortunately, NGINX will help you </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">achieve</span><a id="_idTextAnchor662"/> <a id="_idTextAnchor663"/><span class="koboSpan" id="kobo.79.1">it.</span></span></p>
<h2 id="_idParaDest-147"><a id="_idTextAnchor664"/><span class="koboSpan" id="kobo.80.1">The upstream module</span></h2>
<p><span class="koboSpan" id="kobo.81.1">The implementation of load</span><a id="_idIndexMarker446"/><span class="koboSpan" id="kobo.82.1"> balancing in NGINX is particularly clever as it allows you to distribute a load at several levels of your infrastructure. </span><span class="koboSpan" id="kobo.82.2">It isnâ€™t limited to proxying HTTP requests across backend servers; it also offers to distribute requests across FastCGI backends (FastCGI, uWSGI, SCGI, and more), or even distribute queries to Memcached servers. </span><span class="koboSpan" id="kobo.82.3">Any directive that ends with </span><strong class="source-inline"><span class="koboSpan" id="kobo.83.1">_pass</span></strong><span class="koboSpan" id="kobo.84.1">, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.85.1">proxy_pass</span></strong><span class="koboSpan" id="kobo.86.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.87.1">fastcgi_pass</span></strong><span class="koboSpan" id="kobo.88.1">, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.89.1">memcached_pass</span></strong><span class="koboSpan" id="kobo.90.1">, accepts a reference to a group </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">of servers.</span></span></p>
<p><span class="koboSpan" id="kobo.92.1">The first step is to declare this group of servers with the help of the </span><em class="italic"><span class="koboSpan" id="kobo.93.1">upstream</span></em><span class="koboSpan" id="kobo.94.1"> block, which must be placed within the http block. </span><span class="koboSpan" id="kobo.94.2">Within the </span><strong class="source-inline"><span class="koboSpan" id="kobo.95.1">upstream</span></strong><span class="koboSpan" id="kobo.96.1"> block, declare one or more servers with the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.97.1">server</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.98.1"> directive:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.99.1">
http {
Â Â Â Â upstream MyUpstream {
Â Â Â Â Â Â Â Â server 10.0.0.201;
Â Â Â Â Â Â Â Â server 10.0.0.202;
Â Â Â Â Â Â Â Â server 10.0.0.203;
}
[...]
}</span></pre> <p><span class="koboSpan" id="kobo.100.1">Alternatively, you can also use </span><strong class="source-inline"><span class="koboSpan" id="kobo.101.1">include</span></strong><span class="koboSpan" id="kobo.102.1"> inside your upstream block to load servers from an </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">external file:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.104.1">
http {
Â Â Â Â upstream MyUpstream {
Â Â Â Â Â Â Â Â include myUpstreamServers.txt
}
[...]
}</span></pre> <p><span class="koboSpan" id="kobo.105.1">Now that your server group is declared, you can reference it in your virtual host configuration. </span><span class="koboSpan" id="kobo.105.2">For example, you can distribute incoming HTTP requests across the server group simply by </span><a id="_idIndexMarker447"/><span class="No-Break"><span class="koboSpan" id="kobo.106.1">proxying them:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.107.1">
server {
Â Â Â Â server_name example.com;
Â Â Â Â listen 80;
Â Â Â Â root /home/example.com/www;
# Proxy all requests to the MyUpstream server group
proxy_pass http://MyUpstream;
Â Â Â Â [...]
}</span></pre> <div>
<div class="IMG---Figure" id="_idContainer042">
<span class="koboSpan" id="kobo.108.1"><img alt="Figure 7.4: An example of Nginx acting as a relay for internal servers" src="image/B21787_07_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.109.1">Figure 7.4: An example of Nginx acting as a relay for internal servers</span></p>
<p><span class="koboSpan" id="kobo.110.1">In this most basic </span><a id="_idIndexMarker448"/><span class="koboSpan" id="kobo.111.1">state of configuration, requests are distributed across the three servers of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.112.1">MyUpstream</span></strong><span class="koboSpan" id="kobo.113.1"> group according to a simple round-robin algorithm, without maintaining </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">session affi</span><a id="_idTextAnchor665"/><span class="koboSpan" id="kobo.115.1">n</span><a id="_idTextAnchor666"/><span class="koboSpan" id="kobo.116.1">ity.</span></span></p>
<h2 id="_idParaDest-148"><a id="_idTextAnchor667"/><span class="koboSpan" id="kobo.117.1">Request distribution mechanisms</span></h2>
<p><span class="koboSpan" id="kobo.118.1">NGINX offers several ways to </span><a id="_idIndexMarker449"/><span class="koboSpan" id="kobo.119.1">solve the problems we mentioned earlier. </span><span class="koboSpan" id="kobo.119.2">The first and simplest of them is the </span><strong class="source-inline"><span class="koboSpan" id="kobo.120.1">weight</span></strong><span class="koboSpan" id="kobo.121.1"> flag, which can be enabled in the definition of your </span><span class="No-Break"><span class="koboSpan" id="kobo.122.1">server group:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.123.1">
upstream MyUpstream {
Â Â Â Â server 10.0.0.201 weight=3;
Â Â Â Â server 10.0.0.202 weight=2;
Â Â Â Â server 10.0.0.203;
}</span></pre> <p><span class="koboSpan" id="kobo.124.1">By default, servers have a weight of </span><strong class="source-inline"><span class="koboSpan" id="kobo.125.1">1</span></strong><span class="koboSpan" id="kobo.126.1">, unless you specify otherwise. </span><span class="koboSpan" id="kobo.126.2">Such a configuration enables you to give more importance to particular servers; the higher their weight, the more requests they will receive from NGINX. </span><span class="koboSpan" id="kobo.126.3">In this example, for every six HTTP requests received, NGINX will systematically distribute </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.128.1">Three requests to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.129.1">10.0.0.201</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.130.1">server (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.131.1">weight=3</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">)</span></span></li>
<li><span class="koboSpan" id="kobo.133.1">Two requests to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.134.1">10.0.0.202</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.135.1">server (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.136.1">weight=2</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">)</span></span></li>
<li><span class="koboSpan" id="kobo.138.1">One request to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.139.1">10.0.0.203</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.140.1">server (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.141.1">weight=1</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.143.1">For every 12 requests, NGINX will distribute </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.145.1">Six requests to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.146.1">10.0.0.201</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.147.1">server (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.148.1">weight=3</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">)</span></span></li>
<li><span class="koboSpan" id="kobo.150.1">Four requests to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.151.1">10.0.0.202</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.152.1">server (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.153.1">weight=2</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">)</span></span></li>
<li><span class="koboSpan" id="kobo.155.1">Two requests to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.156.1">10.0.0.203</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.157.1">server (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.158.1">weight=1</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.160.1">NGINX also includes a mechanism that will verify the state of servers in a group. </span><span class="koboSpan" id="kobo.160.2">If a server doesnâ€™t respond in time, the request will be re-sent to the next server in the group. </span><span class="koboSpan" id="kobo.160.3">There are several flags that can be assigned to servers in an upstream block that will allow you to better control </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">this mechanism:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.162.1">fail_timeout=N</span></strong><span class="koboSpan" id="kobo.163.1">, where </span><strong class="source-inline"><span class="koboSpan" id="kobo.164.1">N</span></strong><span class="koboSpan" id="kobo.165.1"> is the number of seconds before a request is considered to </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">have failed.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.167.1">max_fails=N</span></strong><span class="koboSpan" id="kobo.168.1">, where </span><strong class="source-inline"><span class="koboSpan" id="kobo.169.1">N</span></strong><span class="koboSpan" id="kobo.170.1"> is the number of attempts that should be performed on a server before NGINX gives up and switches to the next server. </span><span class="koboSpan" id="kobo.170.2">By default, NGINX only tries once. </span><span class="koboSpan" id="kobo.170.3">If all servers become unresponsive, NGINX will wait for </span><strong class="source-inline"><span class="koboSpan" id="kobo.171.1">fail_timeout</span></strong><span class="koboSpan" id="kobo.172.1"> to expire before resetting all server fail counts and </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">trying again.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.174.1">max_conns=N</span></strong><span class="koboSpan" id="kobo.175.1">, where </span><strong class="source-inline"><span class="koboSpan" id="kobo.176.1">N</span></strong><span class="koboSpan" id="kobo.177.1"> is the number of maximum concurrent connections that can be sent to that server. </span><span class="koboSpan" id="kobo.177.2">By default, NGINX will not limit </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">concurrent connections.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.179.1">backup</span></strong><span class="koboSpan" id="kobo.180.1"> marks the server as a backup server, instructing NGINX to use it only in the case of failure of another server (it is not </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">used otherwise).</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.182.1">down</span></strong><span class="koboSpan" id="kobo.183.1"> marks the server as permanently unavailable, instructing NGINX not to use </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">it anymore.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.185.1">Finally, NGINX offers</span><a id="_idIndexMarker450"/><span class="koboSpan" id="kobo.186.1"> plenty of options to achieve session affinity. </span><span class="koboSpan" id="kobo.186.2">They come in the form of directives that should be inserted within the upstream block. </span><span class="koboSpan" id="kobo.186.3">The simplest of them is </span><strong class="source-inline"><span class="koboSpan" id="kobo.187.1">ip_hash</span></strong><span class="koboSpan" id="kobo.188.1">; this directive instructs NGINX to calculate a hash from the first 3 bytes of the client IPv4 address (or the full IPv6 address) and, based on that hash, keep the client assigned to a particular server. </span><span class="koboSpan" id="kobo.188.2">As long as the client IP address remains the same, NGINX will always forward requests to the same server in the </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">upstream group:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.190.1">
upstream {
Â Â Â Â server 10.0.0.201 weight=3;
Â Â Â Â server 10.0.0.202 weight=2;
Â Â Â Â server 10.0.0.203;
Â Â Â Â ip_hash;
}</span></pre> <p><span class="koboSpan" id="kobo.191.1">Some</span><a id="_idIndexMarker451"/><span class="koboSpan" id="kobo.192.1"> administrators may deem this method too unreliable, considering the fact that a majority of internet service providers across the globe still provide dynamic IP addresses, renewed on a 24-hour basis. </span><span class="koboSpan" id="kobo.192.2">So why not use your own distribution key? </span><span class="koboSpan" id="kobo.192.3">Instead of the client IP address, you could separate requests based on the criteria of your choice, thanks to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.193.1">hash</span></strong><span class="koboSpan" id="kobo.194.1"> directive. </span><span class="koboSpan" id="kobo.194.2">Since the directive allows variables, you could decide to separate requests based on a </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">cookie value:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.196.1">
upstream {
Â Â Â Â server 10.0.0.201;
Â Â Â Â server 10.0.0.202;
Â Â Â Â hash $cookie_username;
}</span></pre> <p><span class="koboSpan" id="kobo.197.1">Based on the data contained in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.198.1">username</span></strong><span class="koboSpan" id="kobo.199.1"> cookie, your visitors will be assigned to the first or the second server in the </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">upstream group.</span></span></p>
<p><span class="koboSpan" id="kobo.201.1">We have just seen how to use NGINX as an HTTP load balancer. </span><span class="koboSpan" id="kobo.201.2">In the next section, weâ€™ll look at how to get the NGINX load balancer working, but this time using TCP instead </span><a id="_idTextAnchor668"/><span class="No-Break"><span class="koboSpan" id="kobo.202.1">o</span><a id="_idTextAnchor669"/><span class="koboSpan" id="kobo.203.1">f HTTP.</span></span></p>
<h1 id="_idParaDest-149"><a id="_idTextAnchor670"/><span class="koboSpan" id="kobo.204.1">Using NGINX as a TCP/UDP load balancer</span></h1>
<p><span class="koboSpan" id="kobo.205.1">Until recently, the </span><a id="_idIndexMarker452"/><span class="koboSpan" id="kobo.206.1">open source version of NGINX would only allow load balancing in the context of HTTP requests. </span><span class="koboSpan" id="kobo.206.2">In the meantime, the commercial subscription NGINX Plus took the concept one step further: using NGINX as a TCP/UDP load balancer. </span><span class="koboSpan" id="kobo.206.3">This would pave the way to much broader possibilities; you could then set up NGINX to distribute the load across any form of networked serversâ€”database servers, email servers, literally everything that communicates via TCP. </span><span class="koboSpan" id="kobo.206.4">In May 2015, the authors decided that TCP/UDP load balancing should be part of the open source version. </span><span class="koboSpan" id="kobo.206.5">As of NGINX 1.9.0, the stream module is included in the source code readily available </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">at </span></span><a href="https://nginx.org/"><span class="No-Break"><span class="koboSpan" id="kobo.208.1">https://ng</span><span id="_idTextAnchor671"/><span class="koboSpan" id="kobo.209.1">i</span><span id="_idTextAnchor672"/><span class="koboSpan" id="kobo.210.1">nx.org/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.211.1">.</span></span></p>
<h2 id="_idParaDest-150"><a id="_idTextAnchor673"/><span class="koboSpan" id="kobo.212.1">The stream module</span></h2>
<p><span class="koboSpan" id="kobo.213.1">The way TCP/UDP load balancing </span><a id="_idIndexMarker453"/><span class="koboSpan" id="kobo.214.1">works in NGINX is remarkably similar to HTTP load balancing. </span><span class="koboSpan" id="kobo.214.2">However, since the module that brings forth the new set of directives is not included in the default build, you will need to run the </span><strong class="source-inline"><span class="koboSpan" id="kobo.215.1">configure</span></strong><span class="koboSpan" id="kobo.216.1"> command with the following flag before building </span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">the program:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.218.1">
 --with-stream</span></pre> <p><span class="koboSpan" id="kobo.219.1">The stream module offers a new block called </span><strong class="bold"><span class="koboSpan" id="kobo.220.1">stream</span></strong><span class="koboSpan" id="kobo.221.1">, which must be placed at the root of the configuration file (outside of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.222.1">http</span></strong><span class="koboSpan" id="kobo.223.1"> block). </span><span class="koboSpan" id="kobo.223.2">In this block, you must declare two sets </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">of directives:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.225.1">server</span></strong><span class="koboSpan" id="kobo.226.1"> declares a TCP/UDP server listening on a particular port, and optionally, a network interface, with or </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">without SSL</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.228.1">upstream</span></strong><span class="koboSpan" id="kobo.229.1"> defines a server group in a similar manner as </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">seen previously</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.231.1">In your server blocks, the requests will be sent to the server group with the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.232.1">proxy_pass</span></strong></span><span class="No-Break"> <a id="_idTextAnchor674"/><span class="koboSpan" id="kobo.233.1">d</span><a id="_idTextAnchor675"/><span class="koboSpan" id="kobo.234.1">irective.</span></span></p>
<h2 id="_idParaDest-151"><a id="_idTextAnchor676"/><span class="koboSpan" id="kobo.235.1">An example of MySQL load balancing</span></h2>
<p><span class="koboSpan" id="kobo.236.1">If you already understand </span><a id="_idIndexMarker454"/><span class="koboSpan" id="kobo.237.1">how HTTP load balancing works in NGINX, the following example will look spectacularly simple to you. </span><span class="koboSpan" id="kobo.237.2">We will configure NGINX to receive MySQL connections and balance them across two </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">backend servers:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.239.1">
stream {
Â Â upstream MyGroup {
Â Â Â Â # use IP address-based distribution
Â Â Â Â hash $remote_addr;
Â Â Â Â server 10.0.0.201 weight=2;
Â Â Â Â server 10.0.0.202;
Â Â Â Â server 10.0.0.203 backup; # use as backup only
Â Â }
Â Â server {
Â Â Â Â # listen on the default MySQL port
Â Â Â Â listen 3306;
Â Â Â Â proxy_pass MyGroup; # forward requests to upstream
Â Â }
}</span></pre> <p><span class="koboSpan" id="kobo.240.1">Thatâ€™s all there is to</span><a id="_idIndexMarker455"/><span class="koboSpan" id="kobo.241.1"> it. </span><span class="koboSpan" id="kobo.241.2">All directives and options offered by the upstream module are still there, but keep in mind that you wonâ€™t be able to use HTTP-based variables (such as cookies) to achieve session affinity. </span><span class="koboSpan" id="kobo.241.3">The stream module comes with a lot more options and flags, but they are not detailed here, as this falls outside the scope of an HTTP server; additional documentation can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">at </span></span><a href="https://nginx.org/"><span class="No-Break"><span class="koboSpan" id="kobo.243.1">https://nginx.org/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.244.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.245.1">We now have an overview of how to run a load balancer using NGINX for both HTTP and TCP/UDP requests. </span><span class="koboSpan" id="kobo.245.2">In the next section, weâ€™ll look at threads and I/O to better understand and improve your serverâ€™s resource management under</span><a id="_idTextAnchor677"/> <a id="_idTextAnchor678"/><span class="No-Break"><span class="koboSpan" id="kobo.246.1">heavy loads.</span></span></p>
<h1 id="_idParaDest-152"><a id="_idTextAnchor679"/><span class="koboSpan" id="kobo.247.1">Exploring thread pools and I/O mechanisms</span></h1>
<p><span class="koboSpan" id="kobo.248.1">Before making important financial decisions, such as investing in an additional server or two, you should look to optimize your current setup to make the most of your </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">existing in</span><a id="_idTextAnchor680"/><span class="koboSpan" id="kobo.250.1">f</span><a id="_idTextAnchor681"/><span class="koboSpan" id="kobo.251.1">rastructure.</span></span></p>
<h2 id="_idParaDest-153"><a id="_idTextAnchor682"/><span class="koboSpan" id="kobo.252.1">Relieving worker processes</span></h2>
<p><span class="koboSpan" id="kobo.253.1">In the case of websites </span><a id="_idIndexMarker456"/><span class="koboSpan" id="kobo.254.1">that require heavy I/O operations, such as file uploads or downloads, the asynchronous architecture of NGINX can present a certain disadvantage: while the master process is able to absorb incoming connections asynchronously, worker processes can be blocked for relatively long periods of time by certain tasks (the most common of which is reading data from hard disk drives or </span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">network drives).</span></span></p>
<p><span class="koboSpan" id="kobo.256.1">Consider a simplified configuration with two worker processes; each HTTP request received by NGINX gets assigned to either process. </span><span class="koboSpan" id="kobo.256.2">Within a process, operations are performed sequentially: receiving and parsing the request, reading the requested file from its storage location, and finally, preparing and sending the response to the client. </span><span class="koboSpan" id="kobo.256.3">If for some reason you were to serve files stored on a network drive with a latency of about 100 ms, both of your worker processes would be spending most of their time waiting for the files. </span><span class="koboSpan" id="kobo.256.4">As</span><a id="_idIndexMarker457"/><span class="koboSpan" id="kobo.257.1"> a result, your server would only be able to serve 18 to 20 requests </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">per second:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer043">
<span class="koboSpan" id="kobo.259.1"><img alt="Figure 7.5: An explanation of how worker processes and latency work" src="image/B21787_07_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.260.1">Figure 7.5: An explanation of how worker processes and latency work</span></p>
<p><span class="koboSpan" id="kobo.261.1">This isnâ€™t just a problem that occurs for network drives. </span><span class="koboSpan" id="kobo.261.2">Even regular hard disk drives can take a certain time to fetch a file if it isnâ€™t in the cache; a 10 ms latency isnâ€™t insignificant when you multiply it </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">by 1,000!</span></span></p>
<p><span class="koboSpan" id="kobo.263.1">The solution that has been made available as of NGINX 1.7.11 is called </span><strong class="bold"><span class="koboSpan" id="kobo.264.1">thread pools</span></strong><span class="koboSpan" id="kobo.265.1">. </span><span class="koboSpan" id="kobo.265.2">The basic principle behind this solution</span><a id="_idIndexMarker458"/><span class="koboSpan" id="kobo.266.1"> is that instead of reading files synchronously within the worker process, NGINX delegates the operation to a thread. </span><span class="koboSpan" id="kobo.266.2">This immediately liberates the worker </span><a id="_idIndexMarker459"/><span class="koboSpan" id="kobo.267.1">process, which can then move on to the next request in the queue. </span><span class="koboSpan" id="kobo.267.2">Whenever the thread finishes performing the operation, the worker process finalizes and sends the response to the client. </span><span class="koboSpan" id="kobo.267.3">It is a pretty simple concept to understand, and thankfully, itâ€™s just as simp</span><a id="_idTextAnchor683"/><span class="koboSpan" id="kobo.268.1">l</span><a id="_idTextAnchor684"/><span class="koboSpan" id="kobo.269.1">e </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">to configure.</span></span></p>
<h2 id="_idParaDest-154"><a id="_idTextAnchor685"/><span class="koboSpan" id="kobo.271.1">AIO, Sendfile, and DirectIO</span></h2>
<p><span class="koboSpan" id="kobo.272.1">In order to enable support for thread pools, NGINX must be built with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.273.1">--with-threads</span></strong><span class="koboSpan" id="kobo.274.1"> parameter; this functionality doesnâ€™t come by default. </span><span class="koboSpan" id="kobo.274.2">The first step of the configuration is to define a thread pool with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.275.1">thread_pool</span></strong><span class="koboSpan" id="kobo.276.1"> directive at the root of your </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">configuration file.</span></span></p>
<p><span class="koboSpan" id="kobo.278.1">Syntax: </span><strong class="source-inline"><span class="koboSpan" id="kobo.279.1">thread_pool name </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.280.1">threads=N [max_queue=Q];</span></strong></span></p>
<p><span class="koboSpan" id="kobo.281.1">In this syntax, </span><strong class="source-inline"><span class="koboSpan" id="kobo.282.1">name</span></strong><span class="koboSpan" id="kobo.283.1"> is the name you wish to give to the thread pool, </span><strong class="source-inline"><span class="koboSpan" id="kobo.284.1">N</span></strong><span class="koboSpan" id="kobo.285.1"> is the number of threads that should be spawned, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.286.1">Q</span></strong><span class="koboSpan" id="kobo.287.1"> is the maximum number of operations allowed in the queue. </span><span class="koboSpan" id="kobo.287.2">By default, a thread pool exists with the name </span><strong class="source-inline"><span class="koboSpan" id="kobo.288.1">default</span></strong><span class="koboSpan" id="kobo.289.1">, coming with 32 threads and a maximum queue of </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">65,536 operations.</span></span></p>
<p><span class="koboSpan" id="kobo.291.1">In </span><strong class="source-inline"><span class="koboSpan" id="kobo.292.1">location</span></strong><span class="koboSpan" id="kobo.293.1"> blocks that require it, simply insert the </span><strong class="source-inline"><span class="koboSpan" id="kobo.294.1">aio</span></strong><span class="koboSpan" id="kobo.295.1"> directive</span><a id="_idIndexMarker460"/><span class="koboSpan" id="kobo.296.1"> and specify the thread </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">pool name:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.298.1">
thread_pool MyPool threads=64;
[...]
location /downloads/ {
Â Â Â Â aio threads=MyPool;
}</span></pre> <p><span class="koboSpan" id="kobo.299.1">Alternatively, insert </span><strong class="source-inline"><span class="koboSpan" id="kobo.300.1">aio threads</span></strong><span class="koboSpan" id="kobo.301.1"> without a pool name if you want to use the default thread pool. </span><span class="koboSpan" id="kobo.301.2">It is also possible to use both </span><strong class="source-inline"><span class="koboSpan" id="kobo.302.1">sendfile</span></strong><span class="koboSpan" id="kobo.303.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.304.1">aio</span></strong><span class="koboSpan" id="kobo.305.1"> in the </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">same location:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.307.1">
location /downloads/ {
aio threads;
directio 8k;
sendfile on;
}</span></pre> <p><span class="koboSpan" id="kobo.308.1">If the file requested by the client is over </span><strong class="source-inline"><span class="koboSpan" id="kobo.309.1">8k</span></strong><span class="koboSpan" id="kobo.310.1"> (the value specified with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.311.1">directio</span></strong><span class="koboSpan" id="kobo.312.1"> directive), </span><strong class="source-inline"><span class="koboSpan" id="kobo.313.1">aio</span></strong><span class="koboSpan" id="kobo.314.1"> will be used. </span><span class="koboSpan" id="kobo.314.2">Otherwise, the file will be sent via </span><strong class="source-inline"><span class="koboSpan" id="kobo.315.1">sendfile</span></strong><span class="koboSpan" id="kobo.316.1">. </span><span class="koboSpan" id="kobo.316.2">For a deeper dive into the</span><a id="_idIndexMarker461"/><span class="koboSpan" id="kobo.317.1"> specifics of </span><strong class="source-inline"><span class="koboSpan" id="kobo.318.1">sendfile</span></strong><span class="koboSpan" id="kobo.319.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.320.1">directio</span></strong><span class="koboSpan" id="kobo.321.1">, we encourage you to consult the official </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">NGINX </span></span><span class="No-Break"><a id="_idIndexMarker462"/></span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">documentation.</span></span></p>
<p><span class="koboSpan" id="kobo.324.1">We now have a better understanding of how to manage NGINX server resources, thanks in particular to thread pools. </span><span class="koboSpan" id="kobo.324.2">Now itâ€™s time to summarize what weâ€™ve learne</span><a id="_idTextAnchor686"/><span class="koboSpan" id="kobo.325.1">d in </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">this chapter.</span></span></p>
<h1 id="_idParaDest-155"><a id="_idTextAnchor687"/><span class="koboSpan" id="kobo.327.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.328.1">Before adapting your infrastructure to increasingly high traffic, you should always look for solutions offered by your current set of tools. </span><span class="koboSpan" id="kobo.328.2">If traffic causes your server to become unresponsive because of blocking operations, such as slow disk reads, you should give thread pools a try. </span><span class="koboSpan" id="kobo.328.3">If this turns out to be insufficient, load balancing is the next best thing. </span><span class="koboSpan" id="kobo.328.4">Thankfully, as we have discovered in this chapter, implementing a load-balanced architecture is made particularly easy by NGINX; you can even use it to distribute the load of other server applications such as MySQL, email, </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">and more.</span></span></p>
<p><span class="koboSpan" id="kobo.330.1">Now that we have seen a basic yet comprehensive approach to the most advanced mechanisms offered by NGINX, letâ€™s move on to deploying NGINX in a cloud infrastructure (docker) with the knowledge gained throughout </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">this book.</span></span></p>
</div>
</body></html>