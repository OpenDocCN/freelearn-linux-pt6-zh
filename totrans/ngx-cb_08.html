<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Load Balancing</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following:</p>
<ul>
<li>Basic balancing techniques</li>
<li>Round-robin load balancing</li>
<li>Least connected load balancing</li>
<li>Hash-based load balancing</li>
<li>Testing and debugging NGINX load balancing</li>
<li>TCP / application load balancing</li>
<li>NGINX as an SMTP load balancer</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>Load balancing serves two main purposesâ€”to provide further fault tolerance and to distribute the load. This is achieved by dividing incoming requests against one or more backend servers, so that you get the combined output of these multiple servers. As most load balancer configurations are generally configured as a reverse proxy (as detailed in the previous chapter), this makes NGINX a great choice.</p>
<p>By increasing your fault tolerance, you can ensure the reliability and uptime of your website or application. In the realms of Google or Facebook, where seconds of downtime can cause chaos, load balancers are a critical part of their business. Likewise, if you have occasional issues with your web server, or want to be able to conduct maintenance without bringing your site down, then a load balancer will greatly enhance your setup.</p>
<p>The distributed load side of a load balancer allows you to horizontally scale your website or application. This is a much easier way to scale your website rather than simply throwing more hardware at a single server, especially when it comes to high levels of concurrency. Using a load balancer, we can increase the number of servers easily, as shown in the following figure:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="435" src="assets/ce9f3ba9-1869-4385-851a-b0fab4b5f69d.png" width="431"/></div>
<p>With the right configuration and monitoring, you can also add and remove these web servers on the fly. This is what many refer to as <strong>elastic computing</strong>, where resources can be provisioned in an automated fashion. When implemented correctly, this can deliver cost savings, while ensuring that you can handle peak loads without any issue.</p>
<p>Of course, there are a few caveats here. The first is that your application or website must be able to run in a distributed manner. As your web server doesn't have a centralized filesystem by default, handling file uploads must be done in a way that all servers still retain access. You can use a clustered filesystem to achieve this (for example, GlusterFS) or use a centralized object or file storage system, such as AWS S3.</p>
<p>Your database also needs to be accessible by all your web servers. If your users log in to your system, you'll also need to ensure that the session tracking uses a database so that it's accessible from all servers.</p>
<p>Thankfully though, if you're using a modern framework (as we covered in <a href="db163fa8-2a5d-40bc-b83e-61e72ec67237.xhtml"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Common Frameworks</em>) or a modern <strong>Content Management System</strong> (<strong>CMS</strong>), then these aspects have been previously implemented and documented.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Basic balancing techniques</h1>
                </header>
            
            <article>
                
<p>The three scheduling algorithms <span class="MsoCommentReference">which</span> NGINX supports are round-robin, least connections, and hashing.</p>
<p>Load balancers configured in a <strong>round-robin</strong> fashion distribute requests across the servers in a sequential basis; the first request goes to the first server, the second request to the second server, and so on. This repeats until each server in the pool has processed a request and the next will simply be at the top again. The following diagram explains the round robin scheduling algorithm:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="560" src="assets/c3cbe6d3-abea-4e77-bc0e-5843db3e337f.png" width="419"/></div>
<p>This is the most simplistic method to implement, and it has both positive and negative sides. The positive is that no configuration is required on the server side. The negative is that there's no ability to check the load of the servers to even out the requests.</p>
<p>When configured to use the <strong>least connection</strong> method of load balancing, NGINX distributes the requests to the servers with the least amount of active connections. This provides a very rudimentary level of load-based distribution; however, it's based on connections rather than actual server load.</p>
<p>This may not always be the most effective method, especially if one particular server has the least amount of connections due to a high resource load or an internal issue.</p>
<p>The third method supported by NGINX is the <strong>hash</strong> method. This uses a key to determine how to map the request with one of the upstream servers. Generally, this is set to the client's IP address, which allows you to map the requests to the same upstream server each time.</p>
<p>If your application doesn't use any form of centralized session tracking, then this is one way to make load balancing more compatible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Round robin load balancing</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>To test the load balancing, you'll need to be able to run multiple versions of your app, each on different ports.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>We'll start off with a basic round-robin configuration, with our upstream servers coming locally. We'll define the <kbd>upstream</kbd> block directive at the <kbd>http</kbd> block level, outside of the <kbd>server</kbd> block:</p>
<pre>upstream localapp { 
    server 127.0.0.1:8080; 
    server 127.0.0.1:8081; 
    server 127.0.0.1:8082; 
}  </pre>
<p>Then, we'll define our <kbd>server</kbd> block directive:</p>
<pre>server { 
    listen       80; 
    server_name  load.nginxcookbook.com; 
    access_log  /var/log/nginx/load-access.log  combined; 
    location / { 
        proxy_pass http://localapp; 
    } 
}  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In our <kbd>upstream</kbd> block directive, we define the servers and the name of the backend servers. In our recipe, we simply defined these as three instances on the localhost on ports <kbd>8080</kbd>, <kbd>8081</kbd>, and <kbd>8082</kbd>. In many scenarios, this can also be external servers (in order to horizontally balance resources).</p>
<p>In our <kbd>server</kbd> block directive, instead of connecting directly to a local application, as in the previous recipe, we connect to our upstream directive which we named <kbd>localapp</kbd>.</p>
<p>As we didn't specify an algorithm to use for load balancing, NGINX defaults to the round-robin configuration. Each of our entries is loaded in sequential order as new requests come in, unless one of the servers fails to respond.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Also, it's possible to weight the servers, meaning it will preference upstream servers with a higher weight. If your servers aren't exactly the same, you can use weighting to preference your higher capacity systems so that they receive more requests. Consider this example:</p>
<pre>upstream localapp { 
    server 127.0.0.1:8080 weight=2; 
    server 127.0.0.1:8081; 
    server 127.0.0.1:8082; 
}  </pre>
<p>Because we set the first server with a weighted value of <kbd>2</kbd>, it will receive twice as many requests as the others.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>The NGINX upstream module: <a href="http://nginx.org/en/docs/http/ngx_http_upstream_module.html"><span class="URLPACKT">http://nginx.org/en/docs/http/ngx_http_upstream_module.html</span></a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Least connected load balancing</h1>
                </header>
            
            <article>
                
<p>While the default load balancing algorithm is round-robin, it doesn't take into consideration either the server load or the response times. With the <strong>least connected</strong> method, we distribute connections to the upstream server with the least number of active connections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>To test the load balancing, you'll need to be able to run multiple versions of your app, each on different ports.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The <kbd>upstream</kbd> block directive looks exactly the same as the round-robin configuration, except we now explicitly tell NGINX to use the least connected methodâ€”a reminder that this needs to remain outside of the <kbd>server</kbd> block directive. Here's our <kbd>upstream</kbd> block:</p>
<pre>upstream localapp { 
    least_conn; 
 
    server 127.0.0.1:8080; 
    server 127.0.0.1:8081; 
    server 127.0.0.1:8082; 
} </pre>
<p>Then, we'll define our <kbd>server</kbd> block directive:</p>
<pre>server { 
    listen       80; 
    server_name  load.nginxcookbook.com; 
    access_log  /var/log/nginx/load-access.log  combined; 
    location / { 
        proxy_pass http://localapp; 
    } 
} </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Like the round-robin configuration, we have three upstream servers defined with the name <kbd>localapp</kbd>. For this configuration, we explicitly tell NGINX to use the <kbd>least_conn</kbd> method of load balancing.</p>
<p>As each new request comes in, NGINX determines which upstream server has the least amount of connections and directs requests to this server.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hash-based load balancing</h1>
                </header>
            
            <article>
                
<p>When you need to ensure that hash-based load balancing is the optimal choice, commonly, the client's IP address is used as the pattern to match so that any issues with cookies and per upstream server session tracking is sticky. This means that every subsequent request from the same hash will always route to the same upstream server (unless there's a fault with the upstream server).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The <kbd>upstream</kbd> block directive looks exactly the same as the round-robin configuration, except we now explicitly tell NGINX to use the hash methodâ€”a reminder that this needs to remain outside of the <kbd>server</kbd> block directive. Here's our <kbd>upstream</kbd> block:</p>
<pre>upstream localapp { 
    hash $remote_addr consistent; 
    server 127.0.0.1:8080; 
    server 127.0.0.1:8081; 
    server 127.0.0.1:8082; 
} </pre>
<p>Then, we'll define our <kbd>server</kbd> block directive:</p>
<pre>server { 
    listen       80; 
    server_name  load.nginxcookbook.com; 
    access_log  /var/log/nginx/load-access.log  combined; 
    location / { 
        proxy_pass http://localapp; 
    } 
} </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>For this hash method, we used the client IP (<kbd>$remote_addr</kbd>) as the determining factor to build up the hash map.</p>
<p>The <kbd>consistent</kbd> parameter at the end of the hash line implements the Ketama consistent hashing method, which helps to minimize the amount or remapping (and therefore potential disruption or cache loss) if you need to add or remove servers from your <kbd>upstream</kbd> block directive. If your upstream servers remain constant, then you can omit this parameter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>For those who have used older versions of NGINX, the <kbd>ip_hash</kbd> method is still available, but with one distinct difference. The <kbd>ip_hash</kbd> method uses the first three octets of an IPv4 address (for example, <kbd>1.2.3.XXX</kbd>) to produce the hash map. This means that if the requesting IP comes from a different IP within the standard class C range, it'll be sent to the same upstream server. Here's an example of how it's implemented:</p>
<pre>upstream localapp { 
    ip_hash; 
    server 127.0.0.1:8080; 
    server 127.0.0.1:8081; 
    server 127.0.0.1:8082; 
}  </pre>
<p>While this method still works, if you need better consistency for ip_hash mapping, then using <kbd>hash $remote_addr</kbd> will match the full IP address.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The consistent hashing Wikipedia page: <a href="https://en.wikipedia.org/wiki/Consistent_hashing"><span class="URLPACKT">https://en.wikipedia.org/wiki/Consistent_hashing</span></a></li>
<li>The <kbd>hash</kbd> directive documentation: <a href="http://nginx.org/en/docs/http/ngx_http_upstream_module.html#hash"><span class="URLPACKT">http://nginx.org/en/docs/http/ngx_http_upstream_module.html#hash</span></a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing and debugging NGINX load balancing</h1>
                </header>
            
            <article>
                
<p>As load balancing can introduce extra complexities into your environment, it's important to ensure that you can test your setup thoroughly. Especially when you're trying to debug corner cases, being able to build a test platform becomes critical to reproducing faults or issues.</p>
<p>Ideally, you want this combined with real-time metrics as well, especially if you want to be able to overlay this with data from your upstream servers. As we'll cover in <a href="fff6205a-633e-4c22-bf99-420c274e6379.xhtml"><span class="ChapterrefPACKT">Chapter 13</span></a>, <em>NGINX Plus â€“ The Commercial Offering</em>, the commercial release of NGINX contains a live monitoring module which provides information such as the current connections, upstream server statuses, and load information.</p>
<p>While there are a lot of programs and cloud services around which you can generate load to test your load balancer from a client perspective, I didn't find many tools to have test instances for the upstream side of the problem. So, like any programmer, I simply wrote my own!</p>
<p>HTest is an open source tool, which emulates a web server under varying conditions. It is written in Go and is therefore able to take advantage of the high levels of concurrency and optimization; HTest can serve more than 150,000 requests a second on very modest hardware.</p>
<p>Rather than just serving a static page, HTest allows you to vary the response times (so that you can emulate your typical application responses), failure rates (where a percentage of responses return a <kbd>500</kbd> error), and can also introduce some jitter so that the results are more realistic.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>No prerequisite steps are required for this recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>After you have downloaded the latest HTest executable (within the releases section of the GitHub page), you can simply call HTest:</p>
<pre><strong>./htest</strong>  </pre>
<p>By default, HTest will listen on port <kbd>8000</kbd> on <kbd>127.0.0.1</kbd> and be ready to serve requests. There's a basic web page, which is returned as part of the result and looks like this:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="340" src="assets/3eeb6e2d-4e3e-407c-a56c-df333983dff2.png" width="585"/></div>
<p>As each request comes in, the hit counter will be incremented, and if any flags for delays, failures, or jitter have been set, then these will also be shown.</p>
<p>We can easily start HTest on different ports, which when testing a load balancer on a single server, is required:</p>
<pre><strong>./htest -port 8001</strong>  </pre>
<p>Details such as the response delay can be set in either of two ways. If you want to start HTest with a delay, this can also be done via a command-line flag:</p>
<pre><strong>./htest -responsedelay 20</strong>  </pre>
<p>This value is in <strong>milliseconds</strong> (<strong>ms</strong>). Alternatively, we can also set the delay after the program has started with a simple cURL call:</p>
<pre><strong>curl http://localhost:8000/svar/?responsedelay=10</strong>  </pre>
<p>The advantage to this is that it can be scripted so that you have a stepped test routine, or you can vary it manually to measure the effect it has on your overall load balancer configuration. Here's the full reference table for configuration items:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>Field</strong></p>
</td>
<td>
<p><strong>Value</strong></p>
</td>
<td>
<p><strong>Example</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>responsedelay</kbd></p>
</td>
<td>
<p>Delay time (in ms)</p>
</td>
<td>
<p>10</p>
</td>
</tr>
<tr>
<td>
<p><kbd>failurerate</kbd></p>
</td>
<td>
<p>Failure rate of requests (in percent)</p>
</td>
<td>
<p>5</p>
</td>
</tr>
<tr>
<td>
<p><kbd>jitter</kbd></p>
</td>
<td>
<p>Variance of the results (in percent)</p>
</td>
<td>
<p>2</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>To further aid in tracing and testing load balancer configurations, we can add some additional headers in for testing. This allows us to then see further information as to which upstream server processed the request, as well as how long it took. Here are the additional headers I added for testing:</p>
<pre>add_header upstream-addr $upstream_addr; 
add_header upstream-connect-time $upstream_connect_time; </pre>
<p>If it's a production system, you can enable this only when a conditional flag is set, as we detailed in <a href="3aa7298c-9fc0-4f41-9dfa-6db2e4e5e345.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Logging</em>.</p>
<p>This allows us to see which upstream server processed our request (<kbd>$upstream_addr</kbd>) and how long it took to connect to the upstream server (<kbd>$upstream_connect_time</kbd>). This helps to give an indication as to where any possible connection delays are occurring, and also from which upstream server.</p>
<p>If you need to track the upstream server response time (how long it took to return data), this needs to be logged and cannot be set as a header. This is because headers are sent to the browser before the request from the upstream server has returned, and therefore the time at that point is still unknown.</p>
<p>For example, here are the results from a very simple test:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="182" src="assets/e45d6bf4-1753-436f-81f8-136dae57453f.png" width="339"/></div>
<p>As this is based on our previous recipes, we can see that the connection was made to a server running locally on port <kbd>8081</kbd>, and because the load on the server was very low, there was no connection delay (<kbd>0.000</kbd> seconds). An increase in connection times can indicate load or network issues at the load balancer end, which is typically hard to diagnose, as the finger is usually pointed at the upstream server as the cause of delays.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>The HTest repository: <a href="https://github.com/timbutler/htest"><span class="URLPACKT">https://github.com/timbutler/htest</span></a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TCP / application load balancing</h1>
                </header>
            
            <article>
                
<p>While most people know NGINX for its outstanding role as a web and proxy server, most won't have used it beyond the standard web roles. Some of the key functionalities come from the fact that NGINX is incredibly flexible in how it operates. With the introduction of the stream module in 1.9, NGINX can also load balance TCP and UDP applications as well.</p>
<p>This opens up the possibility to load balance applications which don't have either any internal task distribution or any ability to scale beyond one server.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To use TCP load balancing, we first need to double-check whether the NGINX version has the stream module compiled. To do this, you can run the following command:</p>
<pre><strong>nginx -V</strong>  </pre>
<p>This will generate an output, displaying all of the compiled modules. The stream module is available if you see <kbd>--with-stream</kbd> in the output. Consider this example:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="230" src="assets/8a983d25-8ed6-450c-9957-ff08e5582a51.png" width="694"/></div>
<p>If you don't have the required version, we covered how to install an updated version in <a href="69685f00-24c3-428c-b607-01a4e9a2784d.xhtml"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Let's Get Started</em>.</p>
<p>Next, we need to define a <kbd>stream</kbd> block directive, which must be situated outside of the HTTP block directive or replace it altogether. Unlike previous examples, where this could be within the <kbd>/etc/nginx/conf.d/</kbd> directive, this <kbd>stream</kbd> block needs to be set within the main NGINX configuration file (typically <kbd>/etc/nginx/nginx.conf</kbd>) or at least included from there and outside the <kbd>http</kbd> block directive. Here's our configuration:</p>
<pre>stream { 
    upstream tcpapppool { 
        hash $remote_addr consistent; 
        server 127.0.0.1:8101; 
        server 127.0.0.1:8102; 
        server 127.0.0.1:8103; 
    } 
 
    server { 
        listen 7400; 
        proxy_pass tcpapppool; 
    } 
} </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In our recipe, we define an <kbd>upstream</kbd> block, which is virtually identical to our HTTP load balancer configurations. We also specified a hash against the client's IP <kbd>($remote_addr</kbd>), as any application which only has internal session tracking for authentication or similar would require re-authentication against each upstream server if new connections are made.</p>
<p>We have three upstream servers specified for this recipe, which again are against the loopback interface on the local server. Each instance of your TCP applications would need to be listening on <kbd>127.0.0.1</kbd> on ports <kbd>8101</kbd>, <kbd>8102</kbd>, and <kbd>8103</kbd> individually.</p>
<p>Our <kbd>server</kbd> block directive then tells NGINX to listen on port <kbd>7400</kbd>. As we haven't specified a protocol, it will default to TCP. If you require UDP, you'll need to specify the protocol as a parameter. Consider this example:</p>
<pre>listen 7400 udp; </pre>
<p>Lastly, we then configure a reverse proxy (<kbd>proxy_pass</kbd>) to our named upstream configuration <kbd>tcpapppool</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Easy testing</h1>
                </header>
            
            <article>
                
<p>If you need an application to help test connections, I'd recommend you try Packet Sender. This application is free, cross-platform, and allows you to send and receive UDP and TCP data both through a GUI and command-line interface.</p>
<p>This makes it perfect when testing new configurations, especially if you need either targets for your load balancer, or to test the connections through NGINX. Here's what the GUI application looks like:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="385" src="assets/befbe629-434d-4519-ac4d-b9542863f521.png" width="644"/></div>
<p>As shown in the screenshot, we sent a quick <kbd>ABC123</kbd> packet to the NGINX server and received <kbd>I received: ABC123</kbd> back from our upstream application (which is a simple echo app).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Just like traditional HTTP based configurations, we can also enable access logs for TCP load balancing. We can tailor the logs to suit specific applications, which may include fields such as the client IP address, bytes sent, and bytes received.</p>
<div class="packt_tip packt_infobox">This requires NGINX version 1.11.4 or higher.</div>
<p>To use the stream logging, we firstly need to define a log format. Here's a basic configuration:</p>
<pre>log_format basic '$remote_addr [$time_local] ' 
                 '$bytes_sent $bytes_received $session_time'; </pre>
<p>Then, we add the <kbd>access_log</kbd> directive to our <kbd>server</kbd> block:</p>
<pre>server { 
    listen     7400; 
    access_log  /var/log/nginx/tcp-access.log  basic; 
    proxy_pass tcpapppool; 
} </pre>
<p>This basic log format then gives us an output like this:</p>
<pre>1.2.3.4 [28/Dec/2016:22:17:08 +1000] 18 6 0.001 
1.2.3.4 [28/Dec/216:22:17:08 +1000] 18 6 0.001 
1.2.3.4 [28/Dec/216:22:17:09 +1000] 18 6 0.000 
1.2.3.4 [28/Dec/216:22:17:09 +1000] 18 6 0.001 </pre>
<p>We can see the originating IP (<kbd>1.2.3.4</kbd>), the time of the connection (28 December), the bytes sent and received, and then the total session time. If more detailed information is required (such as logging the upstream server used), the log format can be tailored to your specific needs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The NGINX stream module documentation: <a href="https://nginx.org/en/docs/stream/ngx_stream_core_module.html"><span class="URLPACKT">https://nginx.org/en/docs/stream/ngx_stream_core_module.html</span></a></li>
<li>The NGINX stream log module documentation: <a href="https://nginx.org/en/docs/stream/ngx_stream_log_module.html"><span class="URLPACKT">https://nginx.org/en/docs/stream/ngx_stream_log_module.html</span></a></li>
<li>Packet Sender: <a href="https://packetsender.com/"><span class="URLPACKT">https://packetsender.com/</span></a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">NGINX as an SMTP load balancer</h1>
                </header>
            
            <article>
                
<p>As demonstrated in our previous recipe, NGINX can do pure TCP load balancing. Further to this, there are some protocol specific implementations, which have some specific items to enhance the implementation.</p>
<p><strong>Simple Mail Transport Protocol</strong> (<strong>SMTP</strong>) is the standard protocol used to send and receive email at a server level. The most popular SMTP servers for a Linux platform include Postfix, Exim, and Sendmail, with Exchange being the most popular for Windows.</p>
<p>Load balancing SMTP can help to distribute the sending and receiving of email, especially if it's a high-volume environment such as an <strong>Internet Service Provider</strong> (<strong>ISP</strong>). By running multiple servers, you can distribute the sending aspect as well as provide some fault tolerance when systems have issues.</p>
<p>While NGINX has a specific mail module, unfortunately, this does not have load balancing capabilities. The good news is, the stream module is flexible enough that it works seamlessly with SMTP.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>We're going to configure three local SMTP applications, which will be used to help distribute the load. This is what our configuration looks like:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="321" src="assets/451c73fd-2d30-4559-93f2-89c9a6e68422.png" width="502"/></div>
<p>As this needs the stream module, we need to confirm that we have the right NGINX version first. To do this, we run this command:</p>
<pre><strong>nginx -V</strong>  </pre>
<p>If you see <kbd>--with-stream</kbd> in the output, you have the required module. Otherwise, first start with the instructions in <a href="69685f00-24c3-428c-b607-01a4e9a2784d.xhtml"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Let's Get Started</em>, to install an updated version.</p>
<p>Then, we define our <kbd>stream</kbd> block directive, which must be at the root level and not within the <kbd>http</kbd> block, like most of the other recipes in this book. You'll need to add it to the main NGINX configuration file (typically <kbd>/etc/nginx/nginx.conf</kbd>) or at least include the configuration from here.</p>
<p>Here's the <kbd>stream</kbd> block directive:</p>
<pre>stream { 
  upstream smtppool { 
      server 127.0.0.1:2501; 
      server 127.0.0.1:2502; 
      server 127.0.0.1:2503; 
  } 
 
log_format smtplog '$remote_addr $remote_port -&gt; $server_port ' 
                 '[$time_local] $bytes_sent $bytes_received ' 
                 '$session_time ==&gt; $upstream_addr'; 
 
  server { 
      listen 25; 
      proxy_pass smtppool; 
      access_log  /var/log/nginx/smtp-access.log smtplog; 
  } 
} </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Firstly, we define our <kbd>upstream</kbd> block directive and call it <kbd>smtppool</kbd>. There are three servers within this directive, which run on the same server as the NGINX and therefore listen on <kbd>127.0.0.1</kbd>. A real-world scenario will have these running on external servers to help distribute the load. As there's no explicit load balancing method set, this will default to round robin.</p>
<p>Next, we defined a custom log format, which we named <kbd>smtplog</kbd>. Compared to the previous recipe's more basic format, this time we added logging for the port numbers, as well as the upstream server used.</p>
<p>Here's an example of what the logs produce:</p>
<pre>1.2.3.4 64811 -&gt; 26 [04/Jan/2017:23:43:00 +1000] 282 122 0.041 ==&gt; 127.0.0.1:2501 
1.2.3.4 64812 -&gt; 26 [04/Jan/2017:23:43:00 +1000] 282 122 0.039 ==&gt; 127.0.0.1:2502 
1.2.3.4 64813 -&gt; 26 [04/Jan/2017:23:43:00 +1000] 282 122 0.040 ==&gt; 127.0.0.1:2503 
1.2.3.4 64814 -&gt; 26 [04/Jan/2017:23:43:01 +1000] 282 122 0.037 ==&gt; 127.0.0.1:2501 
1.2.3.4 64815 -&gt; 26 [04/Jan/2017:23:43:01 +1000] 282 122 0.038 ==&gt; 127.0.0.1:2502 
1.2.3.4 64816 -&gt; 26 [04/Jan/2017:23:43:01 +1000] 282 122 0.040 ==&gt; 127.0.0.1:2503 
1.2.3.4 64817 -&gt; 26 [04/Jan/2017:23:43:01 +1000] 282 122 0.110 ==&gt; 127.0.0.1:2501 </pre>
<p>While the upstream SMTP servers themselves should also have detailed logs, these logs can help when there are issues occurring and help diagnose if it's a particular upstream server at fault. We can also see that the upstream server used is different every time and in sequential order. This shows that the round robin load balancing is working as expected.</p>
<p>Lastly, we have our <kbd>server</kbd> block directive. We tell NGINX to listen on port <kbd>25</kbd> (the default for SMTP) and proxy connections to our <kbd>smtppool</kbd> upstream servers. We also then log the access using the log format (named <kbd>smtplog</kbd>) defined earlier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>As SMTP can run on multiple ports, we need to tell NGINX to load balance these ports as well. To do this, we simply define multiple listen lines within the <kbd>server</kbd> block:</p>
<pre>server { 
    listen 25; 
    listen 485; 
    listen 581; 
    proxy_pass smtppool; 
    access_log  /var/log/nginx/smtp-access.log smtplog; 
} </pre>
<p>With the server port (named <kbd>$server_port</kbd>) logged in our custom log format, we're still able to trace issues down to a specific port.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The NGINX stream module documentation: <a href="https://nginx.org/en/docs/stream/ngx_stream_core_module.html"><span class="URLPACKT">https://nginx.org/en/docs/stream/ngx_stream_core_module.html</span></a></li>
<li>The NGINX stream log module documentation: <a href="https://nginx.org/en/docs/stream/ngx_stream_log_module.html"><span class="URLPACKT">https://nginx.org/en/docs/stream/ngx_stream_log_module.html</span></a></li>
</ul>


            </article>

            
        </section>
    </body></html>