<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Performance Tuning</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Gzipping content in NGINX</li>
<li>Enhancing NGINX with keep alive</li>
<li>Tuning worker processes and connections</li>
<li>Fine tuning basic Linux system limits</li>
<li>Integrating <kbd>ngx_pagespeed</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>Once you have your NGINX configuration working, you can turn your focus to fine tuning to enhance performance. A few sections of this chapter will focus on delivering increased performance for users, while others will focus on delivering performance enhancements at a server level to allow greater concurrency.</p>
<p>As with any tuning, you need to ensure that you understand the limits first.</p>
<div class="packt_quote">Premature optimization is the root of all evil.<br/>
                                                                – Donald Knuth, 1974</div>
<p>In the context of NGINX, you need to ensure that you know what the limits are before changing them. Not all changes will necessarily result in performance increases if they don't suit your system or if they're not a current limitation.</p>
<p>On the flip side, optimization of your NGINX server is also critical to ensure that your website or application can handle increased traffic and to ensure fast responses. Especially when it comes to e-commerce platforms, keeping user engagement through low response times is paramount. Studies conducted by Amazon found that an increase in the page load time by one second would result in a loss of over $1.6 billion in revenue each year. Even without e-commerce, the last thing that you would want a user to be doing is waiting unnecessarily, as they will disengage very quickly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gzipping content in NGINX</h1>
                </header>
            
            <article>
                
<p>Gzip is a compression format, which is based on the DEFLATE algorithm and commonly found in most Unix environments. Compressing your HTML files is an easy way to reduce the amount of data transferred from NGINX to the browser. This in turn means that pages also load quicker as the file can be transferred in a shorter time due to the reduced size.</p>
<p>While it usually shows the most gain, HTML-based content isn't the only thing, which can compress easily. Any text-based file (for example, JavaScript or CSS) will generally compress by 70 percent or more, which can be quite significant with modern websites.</p>
<p>Of course, enabling compression isn't free. There is a performance hit in server load, as the server needs to use CPU cycles to compress the data. While this used to be a large consideration, with modern CPU's, the performance hit is far outweighed by the benefit of the compression.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The NGINX <kbd>gzip</kbd> module is part of the core modules, so no additional installation is required.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In order to enable <kbd>gzip</kbd> within NGINX, we need to enable the <kbd>gzip</kbd> module and explicitly tell it what files to compress. The easiest way to do this server-wide is to create a <kbd>gzip.conf</kbd> file within the <kbd>/etc/nginx/conf.d</kbd> directory directly, alongside your server directive files. This could also be set per site or even per location if required; the <kbd>gzip</kbd> directives can be nested within an existing block directive.</p>
<p>Here's what is required:</p>
<pre>gzip         on; 
gzip_disable "MSIE [1-6]\.(?!.*SV1)"; 
gzip_proxied any; 
gzip_types   text/plain text/css application/x-javascript application/javascript text/xml application/xml application/xml+rss text/javascript image/x-icon image/bmp image/svg+xml; 
gzip_vary    on; </pre>
<p>If you want to measure how much difference <em>gzipping</em> your files may make, tools such as GTmetrix can outline the reduction in file transmission size. For example, if we look at the <a href="https://www.packtpub.com/" target="_blank"><span class="URLPACKT">https://www.packtpub.com/</span></a> website, we see the following in the Gzip section:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/5189f612-1300-42f4-b646-a30c1c192f6f.png"/></div>
<p>While the savings in this example aren't massive, the 82 percent reduction can show you what's possible for other sites. If there were other files, such as JS or CSS, which weren't already compressed, the decrease becomes much more significant.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The first part of our configuration explicitly turns the <kbd>gzip</kbd> module <kbd>on</kbd>. Then, to maintain compatibility with really old versions of Internet Explorer (which hopefully nobody still uses), we disable <kbd>gzip</kbd> using the <kbd>MSIE [1-6]\.(?!.*SV1)</kbd> regex.</p>
<p>Then, <kbd>gzip_proxied</kbd> sets which proxied connections will use <kbd>gzip</kbd>, which we set to <kbd>any</kbd> to cover all requests. <kbd>gzip_types</kbd> is then used to set what file types are to be compressed. This is matched with the MIME type, for example, <kbd>text/plain</kbd>. We explicitly set types, as not every file type can be compressed further (for example, JPEG images).</p>
<p>Lastly, we set <kbd>gzip_vary</kbd> to <kbd>on</kbd>. This sets the <kbd>Vary: Accept-Encoding</kbd> header, which specifies that both <strong>Content Distribution Networks</strong> (<strong>CDN</strong>) and upstream proxies store a copy of the file as both compressed and uncompressed. While every modern browser supports Gzip compression, there are still some minor browsers and script-based HTTP tools which don't. Instructing the upstream CDN or proxy to store both shows that they're still able to support these older systems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>If the performance hit from gzipping the files on the fly is too much, NGINX also allows the ability to precompress the files to serve. While this means that there's a bit of extra maintenance work required, this can be incorporated into an existing build process (such as <strong>Grunt</strong> or <strong>Gulp</strong>) to reduce the steps required.</p>
<p>To enable in NGINX, we modify our <kbd>gzip.conf</kbd> file to look like the following code:</p>
<pre>gzip         on; 
gzip_static on; 
gzip_disable "MSIE [1-6]\.(?!.*SV1)"; 
gzip_proxied any; 
gzip_types   text/plain text/css application/x-javascript application/javascript text/xml application/xml application/xml+rss text/javascript image/x-icon image/bmp image/svg+xml; 
gzip_vary    on; </pre>
<p>With <kbd>gzip_static</kbd> set to <kbd>on</kbd>, NGINX will serve the precompressed files if they exist.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The Gzip NGINX module can be found at <a href="http://nginx.org/en/docs/http/ngx_http_gzip_module.html" target="_blank"><span class="URLPACKT">http://nginx.org/en/docs/http/ngx_http_gzip_module.html</span></a></li>
<li>Refer to GTmetrix's official website at <a href="https://gtmetrix.com/" target="_blank">https://gtm<span class="URLPACKT">etrix.com/</span></a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Enhancing NGINX with keep alive</h1>
                </header>
            
            <article>
                
<p>Using a persistent HTTP connection between the server and the browser speeds up additional requests, as there's no extra handshaking required. Especially over more latent connections, this can increase the overall performance. If NGINX is being used as a reverse proxy (as detailed in <a href="bc04362e-995f-4550-92b7-183754306d34.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 7</span></a>, <em>Reverse Proxy</em>), it's also important to ensure that these connections have <kbd>keepalive</kbd> enabled to ensure high throughput while minimizing latency. The following diagram highlights both areas where the <kbd>keepalive</kbd> packets are important to maintain high performance:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="154" src="assets/d68c0f3a-c1d6-442f-8d4a-38ff97850155.png" width="578"/></div>
<p>This persistent connection remains established using <strong>Keep Alive</strong> packets, so that the connections remain open for minutes rather than closing once they are complete. This reuse can be immediate for additional CSS/JS files or as further pages and resources are requested.</p>
<p>While some of the client-side gains are negated using HTTP/2 (which multiplexes connections as well as uses <kbd>keepalive</kbd>), it's still necessary for HTTP (non-SSL) connections and upstream connections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The NGINX <kbd>keepalive</kbd> module is part of the core modules, so no additional installation is required.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>By default, NGINX enables keep alive connections for non-proxied connections. This means that the connection between NGINX and the browser has already been optimized. However, as <kbd>keepalive</kbd> packets require HTTP/1.1 support, it's not enabled by default for reverse proxy connections. Using our Express example from <a href="db163fa8-2a5d-40bc-b83e-61e72ec67237.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Common Frameworks</em>, we can add the additional directives:</p>
<pre>server { 
    listen       80; 
    server_name  express.nginxcookbook.com; 
 
    access_log  /var/log/nginx/express-access.log  combined; 
 
    location / { 
        proxy_pass http://127.0.0.1:3000; 
        proxy_http_version 1.1; 
        proxy_set_header Upgrade $http_upgrade; 
        proxy_set_header Connection "upgrade"; 
        keepalive 8; 
    } 
} </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>By adding the <kbd>keepalive</kbd> directive, we define the maximum number of idle connections to keep open using keep alives. In our recipe, we specified a maximum of eight idle connections. It's important to note that this isn't the maximum number of connections in total, this only defines the number of idle connections to keep open.</p>
<p>We can confirm that <kbd>keepalive</kbd> is working by checking the connections from the NGINX server. To do this, we use the <kbd>ss</kbd> command with the <kbd>-o</kbd> flag to display timer information in relation to the socket. For example, we can run the following command:</p>
<pre><strong>ss -tpno</strong>  </pre>
<p>With our Express-based demo, you should see something like the following:</p>
<pre><strong>State Recv-Q Send-Q Local Address:Port Peer Address:Port 
ESTAB 0      0      127.0.0.1:3000     127.0.0.1:33396 
users:(("nodejs",pid=4669,fd=11))  
ESTAB 0      0      xx.xx.xx.xx:80     yy.yy.yy.yy:51239 
users:(("nginx",pid=4705,fd=29))  timer:(keepalive,3min44sec,0) 
ESTAB 0      0      127.0.0.1:33396    127.0.0.1:3000 users:(("nginx",pid=4705,fd=34))  timer:(keepalive,3min51sec,0)</strong> </pre>
<p>We can see that sockets which have a <kbd>keepalive</kbd> packet have been flagged with a timer output to show the expiry.</p>
<p>If you need to test if the browser is seeing the <kbd>keepalive</kbd> response from the server, you can do this with browser developer tools such as Chrome <strong>Developer Tools</strong> (<strong>DevTools</strong>). In your browser of preference, open the developer tools and look for the response headers:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="120" src="assets/801e506e-3bb0-4b83-bf46-7ec05000d7a9.png" width="406"/></div>
<p>In the preceding screenshot, we can see that the server responded with <kbd>Connection: keep-alive</kbd>. This means that the keepalive packets are supported and working.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>The NGINX <kbd>keepalive</kbd> documentation can be found at <a href="http://nginx.org/en/docs/http/ngx_http_upstream_module.html#keepalive" target="_blank"><span class="URLPACKT">http://nginx.org/en/docs/http/ngx_http_upstream_module.html#keepalive</span></a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tuning worker processes and connections</h1>
                </header>
            
            <article>
                
<p>One of the first limits you'll find with running NGINX at scale is the defaults for the worker processes and connections. At a low level, an NGINX worker process is the dedicated event handler for processing all requests.</p>
<p>The defaults for most NGINX installations are 512 worker connections and 1 worker process. While these defaults work in most scenarios, a very busy server can benefit from adjusting these levels to suit your environment. There is no one-size-fits-all scenario when it comes to the correct values, so it's important to know where you're hitting limits and therefore, how to adjust to overcome them.</p>
<p>Setting the limits too high can result in increased memory and CPU overhead, which would have the overall effect of reduced performance rather than increasing it. Thankfully, NGINX will log when it hits certain limits, which is why the logging (as covered in <a href="3aa7298c-9fc0-4f41-9dfa-6db2e4e5e345.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Logging</em>) and metrics of your systems are paramount to maintaining high performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>No special requirements exist for modifying worker process or connection directives.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">As the worker processes and worker connections can be independently adjusted, we can adjust either or both of them; depending on the limits which you're hitting.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Worker processes</h1>
                </header>
            
            <article>
                
<p>To adjust the number of worker processes, we need to edit the main NGINX configuration file. For most installations, this will be located at <kbd>/etc/nginx/nginx.conf</kbd>, and the directive is generally the first line in the file. Here's what you may see for a default value:</p>
<pre>worker_processes 1; </pre>
<p>If the server is dedicated to just running NGINX (for example, it doesn't have the database and other services also running on it), a good rule of thumb is to set it to the number of CPU's available. Consider this example when you have four CPU's:</p>
<pre>worker_processes 4; </pre>
<p>If you have a high number of connections and they're not CPU bound (for example, heavy disk I/O), having more processes than CPU's may assist with increasing the overall throughput of the server.</p>
<p>Lastly, NGINX can attempt to autodetect the number of CPU's in your system by setting the value to auto:</p>
<pre>worker_processes auto; </pre>
<p>If you're not sure what value to use, auto is the best option to use.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Worker connections</h1>
                </header>
            
            <article>
                
<p>Each process can handle up to a maximum number of connections, as specified by the <kbd>worker_connections</kbd> directive. This defaults to 512, but for systems with high numbers of connections, we can increase this further. To do this, we need to edit the main nginx configuration file (<kbd>/etc/nginx/nginx.conf</kbd>) and adjust the following:</p>
<pre>events { 
    worker_connections 4096; 
} </pre>
<p>As increasing the maximum number of connections means that additional system resources are required, caution should be exercised when making a change. If the server hits the limit for <kbd>worker_connections</kbd>, this will be logged in the NGINX error log. By ensuring that this is monitored alongside server resources, you can ensure that it has been set to the correct limit.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>The worker connections can be further enhanced on modern systems by a few extra directives. Here's what our updated block directive looks like:</p>
<pre>events { 
    worker_connections  4096; 
    multi_accept        on; 
    use                 epoll; 
} </pre>
<p>We have added the additional <kbd>multi_accept</kbd> directive and set it to <kbd>on</kbd>. This tells the NGINX worker to accept more than one connection at once when there are a high number of new, incoming connections.</p>
<p>Then, we set the <kbd>use</kbd> directive to <kbd>epoll</kbd>. This is the method NGINX uses to process the connections. While on every modern system this should be automatically set to <kbd>epoll</kbd> by default, we can explicitly set this to ensure that it's used.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>For more information on worker processes, refer to <a href="http://nginx.org/en/docs/ngx_core_module.html#worker_processes" target="_blank"><span class="URLPACKT">http://nginx.org/en/docs/ngx_core_module.html#worker_processes</span></a></li>
<li>For more information on worker connections, refer to <a href="http://nginx.org/en/docs/ngx_core_module.html#worker_connections" target="_blank"><span class="URLPACKT">http://nginx.org/en/docs/ngx_core_module.html#worker_connections</span></a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fine tuning basic Linux system limits</h1>
                </header>
            
            <article>
                
<p>The most popular hosting <strong>Operating System</strong> (<strong>OS</strong>) for NGINX is Linux, which is why we have focused on it in this book. Like NGINX, the out-of-the-box parameters are a good balance between resource usage and performance.</p>
<p>One of the neat features of Linux is the fact that most of the kernel (the <em>engine</em> of the OS) can be tweaked and tuned as required. In fact, nearly every underlying aspect can be adjusted quite easily to ensure that you can perfectly tune it to your needs. With over 1,000 configurable parameters in the kernel, there's virtually infinite tuning available.</p>
<p>Like our warning at the beginning of this chapter however, larger numbers don't necessarily reflect greater performance. It's important to ensure that you understand what parameters you're changing to understand the impact they'll have.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Make sure that you have a backup of your existing server before making any changes. As the changing of the kernel parameters could result in adverse performance or even an unworkable system, it's advisable to perform this on a development or staging system first.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Each kernel parameter can be set in real time, which allows you to test and refine the changes on the fly. To do this, you can use the <kbd>sysctl</kbd> program to change these parameters. For starters, we can ensure that TCP syncookies (a method of resisting low level denial of service attacks) are enabled:</p>
<pre>sysctl -w net.ipv4.tcp_syncookies=1  </pre>
<p>If you want the change to be persistent between boots, we can add this to the <kbd>sysctl</kbd> configuration file, generally located at <kbd>/etc/sysctl.conf</kbd>. Here's what the configuration line should look like:</p>
<pre>net.ipv4.tcp_syncookies = 1 </pre>
<p>You can test and retrieve what value any of the kernel parameters are set to using <kbd>sysctl</kbd> again in read-only mode:</p>
<pre><strong>sysctl net.ipv4.tcp_syncookies</strong>  </pre>
<p>If you' set it correctly, you should see the following result:</p>
<pre><strong>net.ipv4.tcp_syncookies = 1</strong>  </pre>
<p>When you're trying to determine if limits have been hit, ensure that you check the kernel ring buffer for errors by running the <kbd>dmesg</kbd> utility. This logs the output from any kernel module, which generally occurs when they either encounter a limit or error and usually the first port of call to determine what limits you've hit.</p>
<p>If you have a busy server, one of the first Linux kernel limits you may find yourself hitting is when there are some delays in processing and you still have a large number of incoming connections which haven't yet been accepted. While there is a buffer, once you hit this buffer, the server will simply drop any further incoming connections which will cause disruption. To increase this limit, we can adjust the limit by increasing the value for <kbd>net.core.somaxconn</kbd>. On many systems, this defaults to 128 connections, but we can increase this by running the following command:</p>
<pre><strong>sysctl-w net.core.somaxconn=1024</strong>  </pre>
<p>With a large amount of incoming connections, you may also find yourself running out of ephemeral ports for newer connections. As one of the final stages of a TCP connection is the <kbd>TIME_WAIT</kbd> stage, here the connection has been requested to be closed but i's held open just in case there are any further packets. On a busy server, this can result in thousands of connections being held in a <kbd>TIME_WAIT</kbd> state and, by default, these need to be completely closed before they can be reused. We can see the state of the TCP ports on a server by running the following command:</p>
<pre><strong>ss -ant | awk '{print $1}' | sort | uniq -c | sort -n</strong>  </pre>
<p>Here's the output from a moderately-low used server:</p>
<pre><strong>1 CLOSING</strong>
<strong>1 State</strong><br/><strong>2 CLOSE-WAIT</strong>
<strong>3 LAST-ACK</strong>
<strong>3 SYN-RECV</strong>
<strong>5 FIN-WAIT-1<br/></strong><strong>59 FIN-WAIT-2</strong><br/><strong>1311 LISTEN</strong>
<strong>1516 ESTAB</strong>
<strong>4210 TIME-WAIT </strong>  </pre>
<p>If the server becomes very busy, it's possible that all the ports available will be locked in the <kbd>TIME_WAIT</kbd> state. There are two approaches to overcoming this limitation. The first is to reduce the time we hold a connection in the <kbd>TIME_WAIT</kbd> stage. This can be done by lowering the default of <kbd>60</kbd> seconds to <kbd>10</kbd> seconds:</p>
<pre><strong>sysctl-w net.ipv4.tcp_fin_timeout=10</strong>  </pre>
<p>Secondly, we could simply tell the Linux kernel to reuse ports still in the <kbd>TIME_WAIT</kbd> stage for new connections, if required:</p>
<pre><strong>sysctl -w net.ipv4.tcp_tw_reuse=1</strong>  </pre>
<p>This generally isn't enabled by default due to possible conflicts and issues with old, legacy applications, but should be safe to enable for an NGINX server.</p>
<p>You may also find many blogs and articles advising you to increase the buffer sizes for TCP, but these generally focus on increasing the buffer sizes for file serving. Unless you're using NGINX to serve large files, the default values are generally high enough for low latency connections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The NGINX tuning blog can be found at <a href="https://www.nginx.com/blog/tuning-nginx/" target="_blank"><span class="URLPACKT">https://www.nginx.com/blog/tuning-nginx/</span></a></li>
<li>For more information on kernel tuning, refer to <a href="https://www.linux.com/news/kernel-tuning-sysctl" target="_blank"><span class="URLPACKT">https://www.linux.com/news/kernel-tuning-sysctl</span></a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integrating ngx_pagespeed</h1>
                </header>
            
            <article>
                
<p>As the kings of high performance, Google has given us many tools and enhancements that have benefited the web world enormously. Google Chrome (as of 2017) has over 60 percent of the browser market share and its drive for performance has forced other browsers to play catch-up.</p>
<p>Not to be outdone at the server level, Google also has vested interest in ensuring that websites are highly performant as well. This is because faster sites offer a better user experience, which is important when you're trying to offer highly relevant search results. To expedite this, Google released <kbd>ngx_pagespeed</kbd>, which is a module for NGINX that tries to apply Google's best practices to reduce both latency and bandwidth for websites.</p>
<p>While many (if not all) of these optimizations can be manually applied, or should be part of any highly-performant development workflow, not everyone has the time to put the same amount of focus on overall website performance. This is especially common with smaller business websites, where the development has been outsourced to a third party, but don't have the budget to fully optimize.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The <kbd>ngx_pagespeed</kbd> module requires you to compile the module and NGINX from source, which can be achieved in two ways. The first is to use the automated script provided by Google:</p>
<pre><strong>bash &lt;(curl -f -L -sS https://ngxpagespeed.com/install) \</strong>
    <strong>     --nginx-version latest</strong>  </pre>
<div class="packt_tip">Manually inspecting any script before running it is advisable, especially when downloading from new or unknown sources.</div>
<p>This script will install any required dependencies, then download the latest mainline NGINX edition, and then add the <kbd>ngx_pagespeed</kbd> module. This script isn't completely headless and may ask you to confirm some basic parameters such as additional modules to compile (if required). If you intend to use this on a production server, you will need to adjust some of the installation paths to suit your environment.</p>
<p>The second method is via a manual installation, which can also be used if you need to modify the standard build. Details of the manual installation steps are located on the <kbd>ngx_pagespeed</kbd> website. With the advent of dynamic modules within NGINX, there should hopefully be a compiled binary version available very soon as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To test the differences with and without the module enabled, I've used a typical Bootstrap-based website which incorporates several JavaScript and <strong>Cascading Style Sheets</strong> (<strong>CSS</strong>) scripts:</p>
<p>To serve these files, we have a basic NGINX configuration:</p>
<pre>server { 
    listen       80; 
    server_name  pagespeed.nginxcookbook.com; 
 
    access_log  /var/log/nginx/test-access.log combined; 
 
    location / { 
        root   /var/www/test; 
        index  index.html; 
    } 
} </pre>
<p>With a basic site set up, we can now enable the <kbd>nginx_pagespeed</kbd> module. Before we enable the module, we first need to create a directory for cache file storage. We can do this using the following command:</p>
<pre><strong>mkdir /var/ngx_pagespeed-cache</strong>  </pre>
<p>We also need to ensure that it's writable by NGINX, so we simply set the ownership to be the <kbd>nginx</kbd> user, using the following command:</p>
<pre><strong>chown nginx:nginx /var/ngx_pagespeed-cache</strong>  </pre>
<p>With the cache directory ready, we can now load the module by adding the following lines into the server block directive:</p>
<pre>pagespeed on; 
pagespeed FileCachePath /var/ngx_pagespeed-cache; </pre>
<p>While it may seem overly simple, there's an enormous amount of complexity and work which goes on in the module. Since some of the options may cause issues with a small number of sites, there's also the ability to disable certain sub-modules. For instance, if we wanted to disable combining CSS files, we can disable the filter by adding the following directive:</p>
<pre class="CodePACKT">pagespeed DisableFilters rewrite_images,combine_css;</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Using Google's <strong>PageSpeed Insights</strong>, we can see the out-of-the-box score for the website without any optimization enabled:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="177" src="assets/584442ec-2238-448f-8e4f-85756a1df79c.png" width="411"/></div>
<p>Obviously, 51/100 isn't a great score; this is due to multiple CSS and JS files that haven't been minified, are not compressed, and use no explicit browser caching. With <kbd>ngx_pagespeed</kbd> enabled, we get a much better result:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="171" src="assets/01f109cb-7dd0-4578-bbc6-eaf724ea22d7.png" width="518"/></div>
<p>This has instantly given the website a boost in performance, but the simple score doesn't tell the whole story. When comparing the differences in the number of requests, the total has nearly halved.</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td/>
<td>
<p><strong>Without ngx_pagespeed</strong></p>
</td>
<td>
<p><strong>With ngx_pagespeed</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>Files Loaded</strong></p>
</td>
<td>
<p>18</p>
</td>
<td>
<p>11</p>
</td>
</tr>
<tr>
<td>
<p><strong>Total Transferred</strong></p>
</td>
<td>
<p>540 kB</p>
</td>
<td>
<p>214 kB</p>
</td>
</tr>
</tbody>
</table>
<p>While this is only for a very basic site, as the improvements show, there are significant performance gains for nearly zero effort.</p>
<p>Like many of the systems that work, Google's optimizations have quite a number of neat features. As compressing and minifying CSS/JS can be CPU intensive, on the first page load (without the cache being warmed), NGINX will simply serve the site in the original format. In the background, the module queues these tasks and, once available, they will be served directly from the cache.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>If you want to see what's going on, we can enable the admin area for <kbd>modpagespeed</kbd>. To do this, we need to add configuration items outside of the main server directive. Here's the code to add:</p>
<pre>pagespeed on; 
pagespeed FileCachePath /var/ngx_pagespeed-cache; 
pagespeed statistics on; 
pagespeed StatisticsLogging on; 
pagespeed LogDir /var/log/pagespeed; 
pagespeed MessageBufferSize 100000; </pre>
<p>This allows us to see what's going on within the module, including details such as hits and misses from the cache, image compression, CSS, and JavaScript minification, and more:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="322" src="assets/19685a0a-8b8a-4240-9d73-61f111a2b769.png" width="776"/></div>
<p>If this is a production environment, ensure that you restrict access so that it can't be used for malicious reasons.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>To learn more about NGINX PageSpeed, refer to <a href="http://ngxpagespeed.com/" target="_blank"><span class="URLPACKT">http://ngxpagespeed.com/</span></a></li>
<li>More information about installation reference is available at <a href="https://modpagespeed.com/doc/build_ngx_pagespeed_from_source" target="_blank"><span class="URLPACKT">https://modpagespeed.com/doc/build_ngx_pagespeed_from_source</span></a></li>
<li>More information about PageSpeed Insights can be found at <a href="https://developers.google.com/speed/pagespeed/insights/" target="_blank"><span class="URLPACKT">https://developers.google.com/speed/pagespeed/insights/﻿</span></a></li>
</ul>


            </article>

            
        </section>
    </body></html>